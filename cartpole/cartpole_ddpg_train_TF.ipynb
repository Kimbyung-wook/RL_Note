{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50c13cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, concatenate\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "740f86af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc = Dense(64, activation='relu')\n",
    "        self.out= Dense(action_size, activation='softmax', kernel_initializer=tf.keras.initializers.RandomUniform(-1e-3,1e-3))\n",
    "        # self.build(input_shape=[(None,) + state_size])\n",
    "        # self.summary()\n",
    "\n",
    "    def call(self, x):\n",
    "        x      = self.fc(x)\n",
    "        policy = self.out(x)\n",
    "        return policy\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        # self.s1 = Dense(16, activation='relu')\n",
    "        # self.a1 = Dense(16, activation='relu')\n",
    "        self.fc1= Dense(32, activation='relu')\n",
    "        self.fc2= Dense(16, activation='relu')\n",
    "        self.out= Dense(1, kernel_initializer=tf.keras.initializers.RandomUniform(-1e-3,1e-3))\n",
    "\n",
    "        # self.build(input_shape=[(None,) + state_size, (None,) + action_size])\n",
    "        # self.summary()\n",
    "\n",
    "    def call(self,x):\n",
    "        states = x[0]\n",
    "        actions = np.transpose([x[1]])\n",
    "        x = tf.concat([states,actions],axis=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        q = self.out(x)\n",
    "        # x      = tf.keras.layers.concatenate([states, actions], axis=1)\n",
    "        # x      = self.fc(x)\n",
    "        # q_value= self.out(x)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a44d11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size= action_size\n",
    "        \n",
    "        # Hyper params for learning\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.tau = 0.001\n",
    "        \n",
    "        # Experience Replay\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        \n",
    "        self.actor         = Actor(self.state_size, self.action_size)\n",
    "        self.critic        = Critic(self.state_size, self.action_size)\n",
    "        self.target_actor  = Actor(self.state_size, self.action_size)\n",
    "        self.target_critic = Critic(self.state_size, self.action_size)\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr=self.learning_rate)\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def update_target_model(self):\n",
    "        # new_target_actor_weight  = self.tau * self.array(self.actor.get_weight())  + (1 - self.tau) * self.array(self.target_actor.get_weight())\n",
    "        # new_target_critic_weight = self.tau * self.array(self.critic.get_weight()) + (1 - self.tau) * self.array(self.target_critic.get_weight())\n",
    "\n",
    "        # self.target_actor.set_weight( new_target_actor_weight )\n",
    "        # self.target_critic.set_weight(new_target_critic_weight)\n",
    "        \n",
    "    # def update_target(self,tau=1.0):\n",
    "        # tau = max(0.0, min(tau, 1.0))\n",
    "        tau = self.tau\n",
    "        for (target_net, net) in zip(   self.actor.trainable_variables,\n",
    "                                        self.target_actor.trainable_variables):\n",
    "            target_net.assign(tau * target_net + (1.0 - tau) * net)\n",
    "        for (target_net, net) in zip(   self.critic.trainable_variables,\n",
    "                                        self.target_critic.trainable_variables):\n",
    "            target_net.assign(tau * target_net + (1.0 - tau) * net)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        policy = self.actor(state)\n",
    "        policy = np.array(policy[0])\n",
    "        return np.random.choice(self.action_size, 1, p=policy)[0]\n",
    "        \n",
    "    def train_model(self, state, action, reward, next_state, done):\n",
    "        # Train from Experience Replay\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return 0.0\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        states      = np.array([sample[0][0] for sample in mini_batch])\n",
    "        actions     = np.array([sample[1] for sample in mini_batch])\n",
    "        rewards     = np.array([sample[2] for sample in mini_batch])\n",
    "        next_states = np.array([sample[3][0] for sample in mini_batch])\n",
    "        dones       = np.array([sample[4] for sample in mini_batch])\n",
    "        \n",
    "        critic_params = self.critic.trainable_variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            # critic network\n",
    "            print(states)\n",
    "            print(actions)\n",
    "            # print(np.transpose([actions]))\n",
    "            # print(tf.concat([states,actions])],axis=1))\n",
    "            critic_values  = self.critic([states, actions])\n",
    "            # Target actor network\n",
    "            target_actions = self.target_actor(next_states)\n",
    "            print(target_actions)\n",
    "            # Target critic network\n",
    "            target_critic_values  = self.target_critic([next_states, target_actions])\n",
    "            # Target Value\n",
    "            target_value = rewards + (1 - dones) * target_critic_values[0]\n",
    "            # TD-error and Critic Loss\n",
    "            critic_loss = tf.reduce_mean(tf.square(critic_values - target_value))\n",
    "            \n",
    "        critic_grads = tape.gradient(critic_loss, critic_params)\n",
    "        self.optimizer.apply_gradients(zip(critic_grads, critic_params))\n",
    "\n",
    "        actor_params  = self.actor.trainable_variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            # actor network\n",
    "            actions_new = self.actor(states)\n",
    "            # critic network\n",
    "            critic_values  = self.critic([states, actions_new])\n",
    "            # actor network\n",
    "            actor_loss = -tf.reduce_mean(critic_values)\n",
    "            \n",
    "        actor_grads = tape.gradient(actor_loss, actor_params)\n",
    "        self.optimizer.apply_gradients(zip(actor_grads, actor_params))\n",
    "\n",
    "        return critic_loss\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f3571b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epi:   0 | score avg 19.00 | mem length:   19 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:   1 | score avg 21.30 | mem length:   61 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:   2 | score avg 20.67 | mem length:   76 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:   3 | score avg 20.10 | mem length:   91 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:   4 | score avg 20.49 | mem length:  115 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:   5 | score avg 21.14 | mem length:  142 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:   6 | score avg 21.13 | mem length:  163 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:   7 | score avg 20.62 | mem length:  179 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:   8 | score avg 20.35 | mem length:  197 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:   9 | score avg 20.62 | mem length:  220 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  10 | score avg 20.76 | mem length:  242 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  11 | score avg 20.28 | mem length:  258 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  12 | score avg 19.15 | mem length:  267 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  13 | score avg 18.64 | mem length:  281 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  14 | score avg 19.07 | mem length:  304 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  15 | score avg 25.67 | mem length:  389 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  16 | score avg 25.60 | mem length:  414 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  17 | score avg 24.44 | mem length:  428 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  18 | score avg 24.20 | mem length:  450 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  19 | score avg 22.98 | mem length:  462 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  20 | score avg 22.48 | mem length:  480 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  21 | score avg 22.53 | mem length:  503 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  22 | score avg 22.28 | mem length:  523 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  23 | score avg 21.25 | mem length:  535 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  24 | score avg 20.43 | mem length:  548 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  25 | score avg 21.38 | mem length:  578 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  26 | score avg 21.34 | mem length:  599 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  27 | score avg 22.61 | mem length:  633 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  28 | score avg 21.65 | mem length:  646 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  29 | score avg 20.58 | mem length:  657 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  30 | score avg 19.93 | mem length:  671 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  31 | score avg 20.93 | mem length:  701 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  32 | score avg 20.14 | mem length:  714 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  33 | score avg 21.23 | mem length:  745 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  34 | score avg 21.40 | mem length:  768 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  35 | score avg 20.56 | mem length:  781 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  36 | score avg 20.41 | mem length:  800 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  37 | score avg 21.77 | mem length:  834 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  38 | score avg 21.49 | mem length:  853 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  39 | score avg 23.24 | mem length:  892 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  40 | score avg 24.12 | mem length:  924 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  41 | score avg 22.70 | mem length:  934 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  42 | score avg 21.63 | mem length:  946 | epsilon: 1.0000 | loss: 0.0000\n",
      "epi:  43 | score avg 21.47 | mem length:  966 | epsilon: 1.0000 | loss: 0.0000\n",
      "[[-1.99086395e-02  2.28465934e-01  7.46502721e-04 -2.83251941e-01]\n",
      " [-1.22202450e-01  1.70982463e-01  1.69922409e-01  4.77479584e-02]\n",
      " [ 4.11940858e-02  3.09959941e-02 -1.74899418e-02  1.55603613e-01]\n",
      " [ 1.73163583e-01  3.77181402e-01 -2.00826967e-01 -8.09581157e-01]\n",
      " [-8.15931493e-03  2.09695088e-01 -4.92424422e-02 -3.22769697e-01]\n",
      " [-2.21187250e-02 -7.48979822e-01  1.03968703e-02  1.16332193e+00]\n",
      " [ 1.88068754e-02  3.88668201e-01  5.15290534e-02 -4.41973743e-01]\n",
      " [-5.49330171e-02  5.39323923e-01 -3.14279187e-02 -9.72221944e-01]\n",
      " [-5.02714601e-02 -4.37304815e-01 -2.33036945e-02  5.14121216e-01]\n",
      " [-5.97548855e-03 -1.93503859e-01 -4.44074898e-02  2.56159933e-01]\n",
      " [-4.05730053e-02 -4.91161225e-02 -2.38654687e-02 -2.61258929e-02]\n",
      " [-2.50424486e-02 -1.61668151e-01  1.31303039e-02  2.99706218e-01]\n",
      " [ 1.98618428e-02  3.64601756e-01 -5.30765895e-02 -5.67853233e-01]\n",
      " [ 1.52184588e-02 -3.90462529e-02 -2.34853978e-04 -9.00631895e-02]\n",
      " [-1.47973984e-01 -9.92678226e-01  9.41680327e-02  1.46040623e+00]\n",
      " [ 3.71367285e-02  1.56286143e-01  5.31243544e-02 -1.75533044e-01]\n",
      " [-5.78363410e-02 -4.44197888e-01  1.38069584e-01  8.54844587e-01]\n",
      " [-4.41817047e-03 -5.79554244e-01  4.70118960e-02  8.71123859e-01]\n",
      " [ 7.96204649e-02  2.14476933e-01 -6.24361232e-02 -2.86807427e-01]\n",
      " [-2.47945635e-01 -1.08716136e+00 -6.39869904e-02  3.43206586e-01]\n",
      " [-3.66471775e-01 -1.08305890e+00 -4.53974042e-02  2.52218285e-01]\n",
      " [ 1.90530563e-02 -6.19323218e-01  3.25273268e-03  8.20636790e-01]\n",
      " [ 1.46729394e-01  5.40833350e-01 -1.62393483e-01 -9.24437143e-01]\n",
      " [-4.29379714e-02 -8.16996302e-01  8.66683884e-02  1.17291782e+00]\n",
      " [-2.37112984e-02 -5.81064046e-01  7.63060609e-02  9.05847848e-01]\n",
      " [ 1.77441664e-02  5.79267304e-01 -1.16404608e-01 -9.61886777e-01]\n",
      " [ 6.82292812e-02  3.94492768e-01 -2.09339486e-01 -9.10155558e-01]\n",
      " [-2.95370123e-02  4.25604034e-01  6.85393756e-02 -3.81277909e-01]\n",
      " [ 4.52177208e-02 -1.70865164e-01  7.76232760e-02  5.04963230e-01]\n",
      " [-5.70245327e-02 -3.01594499e-02  9.19038661e-03  6.32159817e-02]\n",
      " [ 1.48059219e-02 -1.57281737e-01  1.00004087e-02  3.29762168e-01]\n",
      " [ 1.16602871e-02 -3.52544609e-01  1.65956520e-02  6.25581902e-01]\n",
      " [ 1.47217725e-02 -5.90348556e-01  5.24947305e-02  9.61648284e-01]\n",
      " [ 1.47662967e-01 -1.82710523e-01 -1.97105508e-01 -2.84930936e-01]\n",
      " [ 4.30154693e-02  3.74967905e-01  1.51585381e-02 -5.47188938e-01]\n",
      " [ 1.64277937e-02  3.80432468e-02  2.14638015e-03  3.25939577e-02]\n",
      " [ 4.70038895e-03 -1.46321681e-01 -5.37931382e-03  2.05151277e-01]\n",
      " [ 8.31412546e-02  6.07105813e-01 -6.38516399e-02 -9.29533230e-01]\n",
      " [-8.43744821e-03  1.39066641e-02 -4.89413280e-02 -1.50557065e-02]\n",
      " [ 9.22398660e-02  1.94433053e-01 -1.22206334e-01 -6.11009076e-01]\n",
      " [ 3.78980773e-02 -3.80674417e-02  5.11124325e-02  1.00596094e-01]\n",
      " [ 3.29638251e-02 -3.39724900e-02 -8.43402519e-03 -3.72619554e-02]\n",
      " [ 4.95491540e-02  2.01944454e-01  3.66654550e-02 -2.38115103e-01]\n",
      " [ 8.57293869e-03  2.49543035e-01 -1.16228713e-01 -5.64655089e-01]\n",
      " [ 2.36542074e-02 -2.40626356e-01  2.99473491e-02  2.78662144e-01]\n",
      " [-2.93071617e-02  4.33765178e-01  2.83030539e-02 -5.60605590e-01]\n",
      " [-5.59830334e-02 -7.49542280e-01  6.28482369e-02  1.17726702e+00]\n",
      " [-1.28669033e-01 -7.04912890e-01 -9.92354049e-02 -6.69327714e-02]\n",
      " [ 4.69703010e-02  2.26815212e-01 -1.43174300e-02 -1.52500859e-01]\n",
      " [-4.34305839e-03  1.69375731e-01  5.38234547e-02 -2.22447644e-01]\n",
      " [-2.36880194e-02 -1.66701012e-01  1.05741775e-01  5.40263514e-01]\n",
      " [ 3.86655463e-02 -2.34902899e-01 -2.11682601e-02  2.18884930e-01]\n",
      " [ 2.28803445e-02 -2.55171754e-03  3.57000104e-02  2.90869408e-02]\n",
      " [-7.95383813e-02 -4.48096259e-01  1.78917668e-01  9.47258578e-01]\n",
      " [ 5.28731627e-03 -4.71362288e-02  5.31204099e-02  2.18995856e-02]\n",
      " [-2.92551115e-02 -7.77231662e-01 -1.23028050e-02  1.09811590e+00]\n",
      " [-1.00475979e-01 -1.20261911e+00 -3.72554224e-02  1.35180966e+00]\n",
      " [-4.54196082e-02 -2.42592593e-01 -2.79111235e-02  2.30371447e-01]\n",
      " [-4.29338876e-02 -4.16670281e-01 -4.65285100e-03  5.66260970e-01]\n",
      " [-3.75683084e-02 -3.74935492e-01  5.99725423e-02  6.79811743e-01]\n",
      " [-4.94299018e-02 -3.46326814e-01  1.15977177e-01  6.59784478e-01]\n",
      " [ 3.04244717e-02  1.81833988e-03  2.28162734e-02  6.92472861e-02]\n",
      " [ 6.31956484e-02 -5.33750891e-01 -1.28349097e-01  1.70628619e-01]\n",
      " [-8.40055972e-02 -4.05039137e-01  5.12966634e-03  5.27516465e-01]]\n",
      "[0 0 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 1 1 0 0 1 1 1 1 0 0\n",
      " 1 1 1 1 1 1 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 1 0 0 0 0]\n",
      "tf.Tensor(\n",
      "[[0.49997476 0.5000253 ]\n",
      " [0.5001561  0.49984393]\n",
      " [0.49981216 0.5001879 ]\n",
      " [0.49939084 0.5006091 ]\n",
      " [0.4999642  0.5000358 ]\n",
      " [0.50071734 0.4992827 ]\n",
      " [0.49949777 0.50050217]\n",
      " [0.49966016 0.50033987]\n",
      " [0.5001307  0.49986932]\n",
      " [0.5002506  0.49974942]\n",
      " [0.5001384  0.4998616 ]\n",
      " [0.49998322 0.5000168 ]\n",
      " [0.4994749  0.5005251 ]\n",
      " [0.49984    0.50016   ]\n",
      " [0.50063473 0.49936527]\n",
      " [0.4996991  0.5003009 ]\n",
      " [0.50060046 0.49939957]\n",
      " [0.500324   0.499676  ]\n",
      " [0.49961254 0.50038743]\n",
      " [0.5005834  0.49941656]\n",
      " [0.5003183  0.49968165]\n",
      " [0.5005921  0.49940792]\n",
      " [0.49961683 0.5003832 ]\n",
      " [0.5007987  0.4992013 ]\n",
      " [0.50063545 0.49936458]\n",
      " [0.49960944 0.5003906 ]\n",
      " [0.49937406 0.50062597]\n",
      " [0.4994876  0.5005124 ]\n",
      " [0.50007683 0.49992314]\n",
      " [0.50016624 0.49983373]\n",
      " [0.50029016 0.4997098 ]\n",
      " [0.50015444 0.49984556]\n",
      " [0.50035435 0.49964568]\n",
      " [0.4998457  0.50015426]\n",
      " [0.49948296 0.500517  ]\n",
      " [0.5001384  0.49986163]\n",
      " [0.50024754 0.4997525 ]\n",
      " [0.49924734 0.5007527 ]\n",
      " [0.49980134 0.50019866]\n",
      " [0.4995723  0.5004277 ]\n",
      " [0.49987507 0.50012493]\n",
      " [0.49983874 0.50016123]\n",
      " [0.49965253 0.5003475 ]\n",
      " [0.4995456  0.50045437]\n",
      " [0.50033236 0.49966764]\n",
      " [0.49980026 0.5001997 ]\n",
      " [0.5004641  0.49953592]\n",
      " [0.5003237  0.49967635]\n",
      " [0.50001323 0.49998677]\n",
      " [0.50005263 0.4999474 ]\n",
      " [0.50039655 0.49960345]\n",
      " [0.5002849  0.49971515]\n",
      " [0.49983677 0.5001632 ]\n",
      " [0.500645   0.499355  ]\n",
      " [0.49987417 0.5001259 ]\n",
      " [0.50069964 0.4993003 ]\n",
      " [0.50091887 0.49908108]\n",
      " [0.50027436 0.49972564]\n",
      " [0.5001478  0.49985224]\n",
      " [0.50019616 0.49980378]\n",
      " [0.5005035  0.4994965 ]\n",
      " [0.5001779  0.4998221 ]\n",
      " [0.5003089  0.49969113]\n",
      " [0.50042033 0.49957967]], shape=(64, 2), dtype=float32)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "InvalidArgumentError",
     "evalue": "ConcatOp : Ranks of all input tensors should match: shape[0] = [64,4] vs. shape[1] = [2,64,1] [Op:ConcatV2] name: concat",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5eb335d7fc0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m             \u001b[0mloss_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-aaee430ff8d8>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;31m# Target critic network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0mtarget_critic_values\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_critic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_actions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m             \u001b[1;31m# Target Value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[0mtarget_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtarget_critic_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1012\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-c7261f1636cf>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(values, axis, name)\u001b[0m\n\u001b[0;32m   1675\u001b[0m           dtype=dtypes.int32).get_shape().assert_has_rank(0)\n\u001b[0;32m   1676\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1677\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1678\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mconcat_v2\u001b[1;34m(values, axis, name)\u001b[0m\n\u001b[0;32m   1190\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1192\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1193\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1194\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6861\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6862\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6863\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: ConcatOp : Ranks of all input tensors should match: shape[0] = [64,4] vs. shape[1] = [2,64,1] [Op:ConcatV2] name: concat"
     ]
    }
   ],
   "source": [
    "%matplotlib tk\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DDPGAgent(state_size, action_size)\n",
    "\n",
    "    scores, episodes, losses = [], [], []\n",
    "    score_avg = 0\n",
    "    \n",
    "    end = False\n",
    "    \n",
    "    fig = plt.figure(1)\n",
    "    fig.clf()\n",
    "    \n",
    "    num_episode = 2000\n",
    "    for e in range(num_episode):\n",
    "        done = False\n",
    "        score = 0\n",
    "        loss_list = []\n",
    "        \n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        \n",
    "        while not done:\n",
    "            # env.render()\n",
    "\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "            score += reward\n",
    "            reward = 0.1 if not done or score == 500 else -1\n",
    "\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "            loss = agent.train_model(state, action, reward, next_state, done)\n",
    "            loss_list.append(loss)\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                agent.update_target_model()\n",
    "                \n",
    "                score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\n",
    "                print('epi: {:3d} | score avg {:3.2f} | mem length: {:4d} | epsilon: {:.4f} | loss: {:.4f}'\n",
    "                      .format(e, score_avg, len(agent.memory), agent.epsilon, np.mean(loss_list)))\n",
    "\n",
    "                scores.append(score_avg)\n",
    "                episodes.append(e)\n",
    "                losses.append(np.mean(loss_list))\n",
    "                plt.subplot(211)\n",
    "                plt.plot(episodes, scores, 'b')\n",
    "                plt.xlabel('episode')\n",
    "                plt.ylabel('average score')\n",
    "                plt.title('cartpole DDPG')\n",
    "                \n",
    "                plt.subplot(212)\n",
    "                plt.plot(episodes, losses, 'b')\n",
    "                plt.xlabel('episode')\n",
    "                plt.ylabel('loss')\n",
    "                \n",
    "                plt.savefig('./save_model/cartpole_ddpg.png')\n",
    "\n",
    "                if score_avg > 400:\n",
    "                    agent.model.save_weights('./save_model/cartpole_ddpg', save_format='tf')\n",
    "                    end = True\n",
    "                    break\n",
    "        if end == True:\n",
    "            np.save('./save_model/cartpole_ddpg_epi',episodes)\n",
    "            np.save('./save_model/cartpole_ddpg_score',scores)\n",
    "            np.save('./save_model/cartpole_ddpg_loss',losses)\n",
    "            env.close()\n",
    "            print(\"End\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4608448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # num_episode = 1000\n",
    "    # for e in range(num_episode,2*num_episode):\n",
    "    #     done = False\n",
    "    #     score = 0\n",
    "    #     loss_list = []\n",
    "        \n",
    "    #     state = env.reset()\n",
    "    #     state = np.reshape(state, [1, state_size])\n",
    "        \n",
    "    #     while not done:\n",
    "    #         env.render()\n",
    "\n",
    "    #         action = agent.get_action(state)\n",
    "\n",
    "    #         next_state, reward, done, info = env.step(action)\n",
    "    #         next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "    #         score += reward\n",
    "    #         reward = 0.1 if not done or score == 500 else -1\n",
    "\n",
    "    #         loss = agent.train_model(state, action, reward, next_state, done)\n",
    "    #         loss_list.append(loss)\n",
    "\n",
    "    #         state = next_state\n",
    "    #         if done:\n",
    "                \n",
    "    #             score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\n",
    "    #             print('epi: {:3d} | score avg {:3.2f} | loss: {:.4f}'.format(e, score_avg, np.mean(loss_list)))\n",
    "\n",
    "    #             scores.append(score_avg)\n",
    "    #             episodes.append(e)\n",
    "    #             losses.append(np.mean(loss_list))\n",
    "    #             plt.subplot(211)\n",
    "    #             plt.plot(episodes, scores, 'b')\n",
    "    #             plt.xlabel('episode')\n",
    "    #             plt.ylabel('average score')\n",
    "    #             plt.title('cartpole A2C')\n",
    "                \n",
    "    #             plt.subplot(212)\n",
    "    #             plt.plot(episodes, losses, 'b')\n",
    "    #             plt.xlabel('episode')\n",
    "    #             plt.ylabel('loss')\n",
    "                \n",
    "    #             plt.savefig('./save_model/cartpole_a2c.png')\n",
    "\n",
    "    #             if score_avg > 400:\n",
    "    #                 agent.model.save_weights('./save_model/cartpole_a2c', save_format='tf')\n",
    "    #                 end = True\n",
    "    #                 break\n",
    "    #     if end == True:\n",
    "    #         np.save('./save_model/cartpole_a2c_epi',episodes)\n",
    "    #         np.save('./save_model/cartpole_a2c_score',scores)\n",
    "    #         np.save('./save_model/cartpole_a2c_loss',losses)\n",
    "    #         env.close()\n",
    "    #         print(\"End\")\n",
    "    #         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e642caf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('TF240': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8f237aa33f5a133d3a67a1e00bf3cb9d47b6c38bcc6ab493273f5d4df41b8866"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}