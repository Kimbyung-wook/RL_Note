{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('TORCH190': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a62d03de4abdbf02f2c70aa26fad76bfe5b246d84e1d7929e4e42191e53d635f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.device = device\n",
    "        # Neural Net Layers\n",
    "        self.fc1 = nn.Linear(state_size, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.out = nn.Linear(24,action_size)\n",
    "        # Random Uniform\n",
    "        torch.nn.init.uniform_(self.out.weight,-1e-3,1e-3)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        q = self.out(x)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, device):\n",
    "        self.state_size = state_size\n",
    "        self.action_size= action_size\n",
    "        self.device = device\n",
    "        \n",
    "        # Hyper-parameters for learning\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        \n",
    "        # Experience Replay\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        self.buffer_length = 2000\n",
    "        self.memory = deque(maxlen=self.buffer_length)\n",
    "\n",
    "        # Neural Network Architecture\n",
    "        self.model        = DQN(self.state_size, self.action_size).to(self.device)\n",
    "        self.target_model = DQN(self.state_size, self.action_size).to(self.device)\n",
    "        self.optimizer    = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        self.update_target_model()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state,\\\n",
    "                            action.to(self.device),\\\n",
    "                            torch.FloatTensor([reward]).to(self.device),\\\n",
    "                            torch.FloatTensor([next_state]).to(self.device),\\\n",
    "                            torch.LongTensor([done]).to(self.device)))\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        # self.target_model.load_dict(self.model.state_dict())\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        # Exploration and Exploitation\n",
    "        if (np.random.rand() <= self.epsilon):\n",
    "            return torch.LongTensor([[random.randrange(self.action_size)]])\n",
    "        else:\n",
    "            return self.model.forward(state).max(1)[1].view(1, 1)\n",
    "\n",
    "    def train_model(self):\n",
    "        # Train from Experience Replay\n",
    "        # Training Condition - Memory Size\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return 0.0\n",
    "        # Decaying Exploration Ratio\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        # Sampling from the memory\n",
    "        mini_batch  = random.sample(self.memory, self.batch_size)\n",
    "        batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = zip(*mini_batch)\n",
    "\n",
    "        states      = torch.cat(batch_states)\n",
    "        actions     = torch.cat(batch_actions)\n",
    "        rewards     = torch.cat(batch_rewards)\n",
    "        next_states = torch.cat(batch_next_states)\n",
    "        dones       = torch.cat(batch_dones)\n",
    "\n",
    "        q           = self.model.forward(states).gather(1,actions).squeeze()\n",
    "        max_q       = self.target_model.forward(next_states).detach().max(1)[0]\n",
    "        target      = rewards + (1 - dones) * self.discount_factor * max_q\n",
    "        loss        = F.mse_loss(q,target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DEVICE :  cuda\n",
      "Env Name :  CartPole-v1\n",
      "States 4, Actions 2\n",
      "epi:   0 | score avg 23.00 | mem length:   23 | epsilon: 1.0000\n",
      "epi:   1 | score avg 22.00 | mem length:   36 | epsilon: 1.0000\n",
      "epi:   2 | score avg 23.20 | mem length:   70 | epsilon: 1.0000\n",
      "epi:   3 | score avg 24.18 | mem length:  103 | epsilon: 1.0000\n",
      "epi:   4 | score avg 22.96 | mem length:  115 | epsilon: 1.0000\n",
      "epi:   5 | score avg 22.97 | mem length:  138 | epsilon: 1.0000\n",
      "epi:   6 | score avg 23.07 | mem length:  162 | epsilon: 1.0000\n",
      "epi:   7 | score avg 23.36 | mem length:  188 | epsilon: 1.0000\n",
      "epi:   8 | score avg 22.73 | mem length:  205 | epsilon: 1.0000\n",
      "epi:   9 | score avg 21.55 | mem length:  216 | epsilon: 1.0000\n",
      "epi:  10 | score avg 22.20 | mem length:  244 | epsilon: 1.0000\n",
      "epi:  11 | score avg 24.38 | mem length:  288 | epsilon: 1.0000\n",
      "epi:  12 | score avg 24.04 | mem length:  309 | epsilon: 1.0000\n",
      "epi:  13 | score avg 22.64 | mem length:  319 | epsilon: 1.0000\n",
      "epi:  14 | score avg 22.07 | mem length:  336 | epsilon: 1.0000\n",
      "epi:  15 | score avg 24.07 | mem length:  378 | epsilon: 1.0000\n",
      "epi:  16 | score avg 24.06 | mem length:  402 | epsilon: 1.0000\n",
      "epi:  17 | score avg 23.45 | mem length:  420 | epsilon: 1.0000\n",
      "epi:  18 | score avg 25.51 | mem length:  464 | epsilon: 1.0000\n",
      "epi:  19 | score avg 28.76 | mem length:  522 | epsilon: 1.0000\n",
      "epi:  20 | score avg 27.28 | mem length:  536 | epsilon: 1.0000\n",
      "epi:  21 | score avg 26.65 | mem length:  557 | epsilon: 1.0000\n",
      "epi:  22 | score avg 27.99 | mem length:  597 | epsilon: 1.0000\n",
      "epi:  23 | score avg 26.39 | mem length:  609 | epsilon: 1.0000\n",
      "epi:  24 | score avg 25.25 | mem length:  624 | epsilon: 1.0000\n",
      "epi:  25 | score avg 24.63 | mem length:  643 | epsilon: 1.0000\n",
      "epi:  26 | score avg 24.56 | mem length:  667 | epsilon: 1.0000\n",
      "epi:  27 | score avg 23.61 | mem length:  682 | epsilon: 1.0000\n",
      "epi:  28 | score avg 24.55 | mem length:  715 | epsilon: 1.0000\n",
      "epi:  29 | score avg 23.49 | mem length:  729 | epsilon: 1.0000\n",
      "epi:  30 | score avg 22.94 | mem length:  747 | epsilon: 1.0000\n",
      "epi:  31 | score avg 21.85 | mem length:  759 | epsilon: 1.0000\n",
      "epi:  32 | score avg 20.66 | mem length:  769 | epsilon: 1.0000\n",
      "epi:  33 | score avg 23.30 | mem length:  816 | epsilon: 1.0000\n",
      "epi:  34 | score avg 22.47 | mem length:  831 | epsilon: 1.0000\n",
      "epi:  35 | score avg 23.82 | mem length:  867 | epsilon: 1.0000\n",
      "epi:  36 | score avg 24.64 | mem length:  899 | epsilon: 1.0000\n",
      "epi:  37 | score avg 24.47 | mem length:  922 | epsilon: 1.0000\n",
      "epi:  38 | score avg 23.73 | mem length:  939 | epsilon: 1.0000\n",
      "epi:  39 | score avg 23.25 | mem length:  958 | epsilon: 1.0000\n",
      "epi:  40 | score avg 21.73 | mem length:  966 | epsilon: 1.0000\n",
      "epi:  41 | score avg 20.96 | mem length:  980 | epsilon: 1.0000\n",
      "epi:  42 | score avg 21.46 | mem length: 1006 | epsilon: 0.9930\n",
      "epi:  43 | score avg 21.01 | mem length: 1023 | epsilon: 0.9763\n",
      "epi:  44 | score avg 20.61 | mem length: 1040 | epsilon: 0.9598\n",
      "epi:  45 | score avg 20.75 | mem length: 1062 | epsilon: 0.9389\n",
      "epi:  46 | score avg 19.68 | mem length: 1072 | epsilon: 0.9296\n",
      "epi:  47 | score avg 21.81 | mem length: 1113 | epsilon: 0.8922\n",
      "epi:  48 | score avg 21.03 | mem length: 1127 | epsilon: 0.8798\n",
      "epi:  49 | score avg 20.73 | mem length: 1145 | epsilon: 0.8641\n",
      "epi:  50 | score avg 28.85 | mem length: 1247 | epsilon: 0.7803\n",
      "epi:  51 | score avg 28.07 | mem length: 1268 | epsilon: 0.7640\n",
      "epi:  52 | score avg 26.76 | mem length: 1283 | epsilon: 0.7527\n",
      "epi:  53 | score avg 26.08 | mem length: 1303 | epsilon: 0.7377\n",
      "epi:  54 | score avg 28.58 | mem length: 1354 | epsilon: 0.7010\n",
      "epi:  55 | score avg 33.22 | mem length: 1429 | epsilon: 0.6504\n",
      "epi:  56 | score avg 34.80 | mem length: 1478 | epsilon: 0.6193\n",
      "epi:  57 | score avg 35.82 | mem length: 1523 | epsilon: 0.5920\n",
      "epi:  58 | score avg 36.04 | mem length: 1561 | epsilon: 0.5699\n",
      "epi:  59 | score avg 33.43 | mem length: 1571 | epsilon: 0.5642\n",
      "epi:  60 | score avg 36.49 | mem length: 1635 | epsilon: 0.5292\n",
      "epi:  61 | score avg 53.04 | mem length: 1837 | epsilon: 0.4324\n",
      "epi:  62 | score avg 56.54 | mem length: 1925 | epsilon: 0.3960\n",
      "epi:  63 | score avg 66.58 | mem length: 2000 | epsilon: 0.3384\n",
      "epi:  64 | score avg 88.72 | mem length: 2000 | epsilon: 0.2537\n",
      "epi:  65 | score avg 104.65 | mem length: 2000 | epsilon: 0.1979\n",
      "epi:  66 | score avg 126.79 | mem length: 2000 | epsilon: 0.1428\n",
      "epi:  67 | score avg 146.11 | mem length: 2000 | epsilon: 0.1037\n",
      "epi:  68 | score avg 162.20 | mem length: 2000 | epsilon: 0.0763\n",
      "epi:  69 | score avg 177.98 | mem length: 2000 | epsilon: 0.0554\n",
      "epi:  70 | score avg 191.28 | mem length: 2000 | epsilon: 0.0406\n",
      "epi:  71 | score avg 203.75 | mem length: 2000 | epsilon: 0.0296\n",
      "epi:  72 | score avg 213.38 | mem length: 2000 | epsilon: 0.0219\n",
      "epi:  73 | score avg 223.34 | mem length: 2000 | epsilon: 0.0160\n",
      "epi:  74 | score avg 232.00 | mem length: 2000 | epsilon: 0.0117\n",
      "epi:  75 | score avg 248.20 | mem length: 2000 | epsilon: 0.0100\n",
      "epi:  76 | score avg 252.18 | mem length: 2000 | epsilon: 0.0100\n",
      "epi:  77 | score avg 259.07 | mem length: 2000 | epsilon: 0.0100\n",
      "epi:  78 | score avg 277.96 | mem length: 2000 | epsilon: 0.0100\n",
      "epi:  79 | score avg 279.06 | mem length: 2000 | epsilon: 0.0100\n",
      "epi:  80 | score avg 300.16 | mem length: 2000 | epsilon: 0.0100\n",
      "epi:  81 | score avg 307.44 | mem length: 2000 | epsilon: 0.0100\n",
      "epi:  82 | score avg 303.10 | mem length: 2000 | epsilon: 0.0100\n",
      "epi:  83 | score avg 296.09 | mem length: 2000 | epsilon: 0.0100\n",
      "epi:  84 | score avg 288.68 | mem length: 2000 | epsilon: 0.0100\n",
      "epi:  85 | score avg 282.51 | mem length: 2000 | epsilon: 0.0100\n",
      "epi:  86 | score avg 277.56 | mem length: 2000 | epsilon: 0.0100\n",
      "epi:  87 | score avg 272.90 | mem length: 2000 | epsilon: 0.0100\n",
      "epi:  88 | score avg 266.11 | mem length: 2000 | epsilon: 0.0100\n",
      "epi:  89 | score avg 262.00 | mem length: 2000 | epsilon: 0.0100\n",
      "epi:  90 | score avg 261.70 | mem length: 2000 | epsilon: 0.0100\n",
      "epi:  91 | score avg 257.03 | mem length: 2000 | epsilon: 0.0100\n",
      "epi:  92 | score avg 251.53 | mem length: 2000 | epsilon: 0.0100\n",
      "epi:  93 | score avg 248.08 | mem length: 2000 | epsilon: 0.0100\n",
      "epi:  94 | score avg 249.07 | mem length: 2000 | epsilon: 0.0100\n",
      "epi:  95 | score avg 247.46 | mem length: 2000 | epsilon: 0.0100\n",
      "epi:  96 | score avg 242.22 | mem length: 2000 | epsilon: 0.0100\n",
      "epi:  97 | score avg 237.09 | mem length: 2000 | epsilon: 0.0100\n",
      "epi:  98 | score avg 234.18 | mem length: 2000 | epsilon: 0.0100\n",
      "epi:  99 | score avg 230.17 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 100 | score avg 229.85 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 101 | score avg 230.86 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 102 | score avg 226.48 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 103 | score avg 224.03 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 104 | score avg 222.23 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 105 | score avg 221.10 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 106 | score avg 221.09 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 107 | score avg 222.68 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 108 | score avg 222.62 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 109 | score avg 226.65 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 110 | score avg 229.29 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 111 | score avg 231.76 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 112 | score avg 231.78 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 113 | score avg 235.11 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 114 | score avg 240.30 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 115 | score avg 238.37 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 116 | score avg 239.33 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 117 | score avg 237.90 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 118 | score avg 241.21 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 119 | score avg 239.39 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 120 | score avg 240.75 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 121 | score avg 240.27 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 122 | score avg 238.35 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 123 | score avg 238.81 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 124 | score avg 226.73 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 125 | score avg 231.16 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 126 | score avg 231.34 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 127 | score avg 233.71 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 128 | score avg 236.84 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 129 | score avg 239.15 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 130 | score avg 243.14 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 131 | score avg 247.22 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 132 | score avg 257.00 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 133 | score avg 256.30 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 134 | score avg 257.87 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 135 | score avg 268.28 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 136 | score avg 291.46 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 137 | score avg 312.31 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 138 | score avg 331.08 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 139 | score avg 347.97 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 140 | score avg 363.17 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 141 | score avg 376.86 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 142 | score avg 385.27 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 143 | score avg 396.74 | mem length: 2000 | epsilon: 0.0100\n",
      "epi: 144 | score avg 407.07 | mem length: 2000 | epsilon: 0.0100\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "%matplotlib tk\n",
    "\n",
    "ENV_NAME = 'CartPole-v1'\n",
    "EPISODES = 1000\n",
    "# if gpu is to be used\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"DEVICE : \", device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(ENV_NAME)\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    print('Env Name : ',ENV_NAME)\n",
    "    print('States {}, Actions {}'\n",
    "            .format(state_size, action_size))\n",
    "\n",
    "    agent = DQNAgent(state_size, action_size, device)\n",
    "\n",
    "    scores, episodes, epsilons, losses = [], [], [], []\n",
    "    score_avg = 0\n",
    "    \n",
    "    end = False\n",
    "    \n",
    "    # fig = plt.figure(1)\n",
    "    # fig.clf()\n",
    "    \n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        loss_list = []\n",
    "\n",
    "        state = env.reset()\n",
    "        \n",
    "        while not done:\n",
    "            #env.render()\n",
    "\n",
    "            # Interact with env.\n",
    "            state = torch.FloatTensor([state]).to(device)\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, info = env.step(action.item())\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            loss = agent.train_model()\n",
    "            state = next_state\n",
    "            \n",
    "            # \n",
    "            score += reward\n",
    "            loss_list.append(loss)\n",
    "            if done:\n",
    "                agent.update_target_model()\n",
    "\n",
    "                score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\n",
    "                print('epi: {:3d} | score avg {:3.2f} | mem length: {:4d} | epsilon: {:.4f}'\n",
    "                      .format(e, score_avg, len(agent.memory), agent.epsilon))\n",
    "\n",
    "                episodes.append(e)\n",
    "                scores.append(score_avg)\n",
    "                epsilons.append(agent.epsilon)\n",
    "                losses.append(np.mean(loss_list))\n",
    "                plt.subplot(311)\n",
    "                plt.plot(episodes, scores, 'b')\n",
    "                plt.xlabel('episode')\n",
    "                plt.ylabel('average score')\n",
    "                plt.title('cartpole DQN TORCH')\n",
    "                plt.grid()\n",
    "                \n",
    "                plt.subplot(312)\n",
    "                plt.plot(episodes, epsilons, 'b')\n",
    "                plt.xlabel('episode')\n",
    "                plt.ylabel('epsilon')\n",
    "                plt.grid()\n",
    "                \n",
    "                plt.subplot(313)\n",
    "                plt.plot(episodes, losses, 'b')\n",
    "                plt.xlabel('episode')\n",
    "                plt.ylabel('losses')\n",
    "                plt.grid()\n",
    "                \n",
    "                plt.savefig('./save_model/cartpole_Tdqn.png')\n",
    "\n",
    "                if score_avg > 400:\n",
    "                    torch.save(agent.model.state_dict(),'./save_model/cartpole_Tdqn')\n",
    "                    end = True\n",
    "                    break\n",
    "        if end == True:\n",
    "            env.close()\n",
    "            np.save('./save_model/cartpole_Tdqn_epi',  episodes)\n",
    "            np.save('./save_model/cartpole_Tdqn_score',scores)\n",
    "            np.save('./save_model/cartpole_Tdqn_loss', losses)\n",
    "            print(\"End\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import gym\n",
    "# import collections\n",
    "# import random\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "\n",
    "# #Hyperparameters\n",
    "# learning_rate = 0.0001\n",
    "# gamma         = 0.999\n",
    "# buffer_limit  = 2000\n",
    "# batch_size    = 64\n",
    "\n",
    "# class ReplayBuffer():\n",
    "#     def __init__(self):\n",
    "#         self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "    \n",
    "#     def put(self, transition):\n",
    "#         self.buffer.append(transition)\n",
    "    \n",
    "#     def sample(self, n):\n",
    "#         mini_batch = random.sample(self.buffer, n)\n",
    "#         s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "        \n",
    "#         for transition in mini_batch:\n",
    "#             s, a, r, s_prime, done_mask = transition\n",
    "#             s_lst.append(s)\n",
    "#             a_lst.append([a])\n",
    "#             r_lst.append([r])\n",
    "#             s_prime_lst.append(s_prime)\n",
    "#             done_mask_lst.append([done_mask])\n",
    "\n",
    "#         return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "#                torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "#                torch.tensor(done_mask_lst)\n",
    "    \n",
    "#     def size(self):\n",
    "#         return len(self.buffer)\n",
    "\n",
    "# class Qnet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Qnet, self).__init__()\n",
    "#         self.fc1 = nn.Linear(4, 24)\n",
    "#         self.fc2 = nn.Linear(24, 24)\n",
    "#         self.fc3 = nn.Linear(24, 2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "      \n",
    "#     def sample_action(self, obs, epsilon):\n",
    "#         out = self.forward(obs)\n",
    "#         coin = random.random()\n",
    "#         if coin < epsilon:\n",
    "#             return random.randint(0,1)\n",
    "#         else : \n",
    "#             return out.argmax().item()\n",
    "            \n",
    "# def train(q, q_target, memory, optimizer):\n",
    "#     for i in range(10):\n",
    "#         s,a,r,s_prime,done_mask = memory.sample(batch_size)\n",
    "\n",
    "#         q_out = q(s)\n",
    "#         q_a = q_out.gather(1,a)\n",
    "#         max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "#         target = r + gamma * max_q_prime * done_mask\n",
    "#         loss = F.mse_loss(q_a, target)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# def main():\n",
    "#     env = gym.make('CartPole-v1')\n",
    "#     q = Qnet()\n",
    "#     q_target = Qnet()\n",
    "#     q_target.load_state_dict(q.state_dict())\n",
    "#     memory = ReplayBuffer()\n",
    "\n",
    "#     print_interval = 1\n",
    "#     score = 0.0  \n",
    "#     optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
    "#     score_avg = 0.0\n",
    "#     for n_epi in range(10000):\n",
    "#         epsilon = max(0.01, 0.08 - 0.01*(n_epi/200)) #Linear annealing from 8% to 1%\n",
    "#         s = env.reset()\n",
    "#         done = False\n",
    "\n",
    "#         while not done:\n",
    "#             a = q.sample_action(torch.from_numpy(s).float(), epsilon)      \n",
    "#             s_prime, r, done, info = env.step(a)\n",
    "#             done_mask = 0.0 if done else 1.0\n",
    "#             memory.put((s,a,r/100.0,s_prime, done_mask))\n",
    "#             s = s_prime\n",
    "\n",
    "#             score += r\n",
    "#             score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\n",
    "#             if done:\n",
    "#                 break\n",
    "            \n",
    "#         if memory.size()>1000:\n",
    "#             train(q, q_target, memory, optimizer)\n",
    "#         if n_epi%print_interval==0 and n_epi!=0:\n",
    "#             q_target.load_state_dict(q.state_dict())\n",
    "#             print(\"n_episode :{}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(\n",
    "#                                                             n_epi, score/print_interval, memory.size(), epsilon*100))\n",
    "#             score = 0.0\n",
    "            \n",
    "#         if score_avg > 400:\n",
    "#             torch.save(q_target.state_dict(),'./save_model/cartpole_Tdqn')\n",
    "#             break\n",
    "#     env.close()\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}