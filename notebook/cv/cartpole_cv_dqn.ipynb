{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('TF240': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8f237aa33f5a133d3a67a1e00bf3cb9d47b6c38bcc6ab493273f5d4df41b8866"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "e:\\MyNote\\RL_Note\n"
     ]
    }
   ],
   "source": [
    "# Find RL_Note path and append sys path\n",
    "import os, sys\n",
    "cwd = os.getcwd()\n",
    "pos = cwd.find('RL_Note')\n",
    "root_path = cwd[0:pos] + 'RL_Note'\n",
    "sys.path.append(root_path)\n",
    "print(root_path)\n",
    "workspace_path = root_path + \"\\\\pys\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten\n",
    "from tensorflow.keras.layers import Max\n",
    "from pys.utils.memory import ReplayMemory\n",
    "from pys.utils.prioritized_memory import ProportionalPrioritizedMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refer from..\n",
    "# https://pylessons.com/CartPole-PER-CNN/\n",
    "class QNetwork(tf.keras.Model):\n",
    "    def __init__(self, state_size, action_size,cfg):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.stacked_layer = []\n",
    "        layer = Conv2D(filter=4,kernel_size=(),strides=(1,1),padding='valid',kernel_initializer='he_uniform', activation='relu')\n",
    "        self.stacked_layer.append(layer)\n",
    "        layer = MaxPool2D(pool_size=(2,2), strides=None)\n",
    "        self.stacked_layer.append(layer)\n",
    "        layer = Conv2D(filter=32,kernel_size=(),strides=(1,1),padding='valid',kernel_initializer='he_uniform', activation='relu')\n",
    "        self.stacked_layer.append(layer)\n",
    "        layer = MaxPool2D(pool_size=(2,2), strides=None)\n",
    "        self.stacked_layer.append(layer)\n",
    "\n",
    "        layer = Dense(units=128,kernel_initializer='he_uniform', activation='relu')\n",
    "        self.stacked_layer.append(layer)\n",
    "        layer = Dense(units=128,kernel_initializer='he_uniform', activation='relu')\n",
    "        self.stacked_layer.append(layer)\n",
    "\n",
    "        self.out = Dense(action_size,kernel_initializer=tf.keras.initializers.RandomUniform(-1e-3,1e-3))\n",
    "\n",
    "    def call(self,x):\n",
    "        for layer in self.stacked_layer:\n",
    "            x = layer(x)\n",
    "        q = self.out(x)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, env:object, cfg:dict):\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size= env.action_space.n\n",
    "        self.env_name   = cfg[\"ENV\"]\n",
    "        self.rl_type    = \"DQN\"\n",
    "        self.er_type    = cfg[\"ER\"][\"ALGORITHM\"].upper()\n",
    "        self.filename   = cfg[\"ENV\"] + '_' + cfg[\"RL\"][\"ALGORITHM\"] + '_' + cfg[\"ER\"][\"ALGORITHM\"]\n",
    "\n",
    "        # Experience Replay\n",
    "        self.batch_size = cfg[\"BATCH_SIZE\"]\n",
    "        self.train_start = cfg[\"TRAIN_START\"]\n",
    "        self.buffer_size = cfg[\"MEMORY_SIZE\"]\n",
    "        if self.er_type == \"ER\":\n",
    "            self.memory = ReplayMemory(capacity=self.buffer_size)\n",
    "        elif self.er_type == \"PER\":\n",
    "            self.memory = ProportionalPrioritizedMemory(capacity=self.buffer_size)\n",
    "        elif self.er_type == \"HER\":\n",
    "            self.memory = HindsightMemory(\\\n",
    "                capacity            = self.buffer_size,\\\n",
    "                replay_n            = cfg[\"ER\"][\"REPLAY_N\"],\\\n",
    "                replay_strategy     = cfg[\"ER\"][\"STRATEGY\"],\\\n",
    "                reward_func         = cfg[\"ER\"][\"REWARD_FUNC\"],\\\n",
    "                done_func           = cfg[\"ER\"][\"DONE_FUNC\"])\n",
    "            self.filename = cfg[\"ENV\"] + '_' + cfg[\"RL\"][\"ALGORITHM\"] + '_' + cfg[\"ER\"][\"ALGORITHM\"] + '_' + cfg[\"ER\"][\"STRATEGY\"]\n",
    "\n",
    "        # Hyper-parameters for learning\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.epsilon_min = 0.01\n",
    "        self.tau = 0.005\n",
    "        \n",
    "        # Neural Network Architecture\n",
    "        self.model        = QNetwork(self.state_size, self.action_size, cfg[\"RL\"][\"NETWORK\"])\n",
    "        self.target_model = QNetwork(self.state_size, self.action_size, cfg[\"RL\"][\"NETWORK\"])\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr=self.learning_rate)\n",
    "        self.hard_update_target_model()\n",
    "        \n",
    "        # Miscellaneous\n",
    "        self.show_media_info = False\n",
    "        self.steps = 0\n",
    "        self.update_period = 100\n",
    "        # self.interaction_period = 1\n",
    "\n",
    "        print(self.filename)\n",
    "        print('States {0}, Actions {1}'.format(self.state_size, self.action_size))\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done, goal=None):\n",
    "        state       = np.array(state,       dtype=np.float32)\n",
    "        action      = np.array([action])\n",
    "        reward      = np.array([reward],    dtype=np.float32)\n",
    "        done        = np.array([done],      dtype=np.float32)\n",
    "        next_state  = np.array(next_state,  dtype=np.float32)\n",
    "        if self.er_type == \"HER\":\n",
    "            goal        = np.array(goal,        dtype=np.float32)\n",
    "            transition  = (state, action, reward, next_state, done, goal)\n",
    "        else:\n",
    "            transition  = (state, action, reward, next_state, done)\n",
    "        self.memory.append(transition)\n",
    "        return\n",
    "        \n",
    "    def hard_update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def soft_update_target_model(self):\n",
    "        tau = self.tau\n",
    "        for (net, target_net) in zip(   self.model.trainable_variables,\n",
    "                                        self.target_model.trainable_variables):\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\n",
    "\n",
    "    def get_action(self,state):\n",
    "        self.steps += 1\n",
    "        # Exploration and Exploitation\n",
    "        if (np.random.rand() <= self.epsilon):\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "            return np.argmax(self.model(state))\n",
    "        \n",
    "    def train_model(self):\n",
    "        # Train from Experience Replay\n",
    "        # Training Condition - Memory Size\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return 0.0\n",
    "        # Decaying Exploration Ratio\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        # Sampling from the memory\n",
    "        if self.er_type == \"ER\" or self.er_type == \"HER\":\n",
    "            mini_batch = self.memory.sample(self.batch_size)\n",
    "        elif self.er_type == \"PER\":\n",
    "            mini_batch, idxs, is_weights = self.memory.sample(self.batch_size)\n",
    "\n",
    "        states      = tf.convert_to_tensor(np.array([sample[0] for sample in mini_batch]))\n",
    "        actions     = tf.convert_to_tensor(np.array([sample[1][0] for sample in mini_batch]))\n",
    "        rewards     = tf.convert_to_tensor(np.array([sample[2] for sample in mini_batch]))\n",
    "        next_states = tf.convert_to_tensor(np.array([sample[3] for sample in mini_batch]))\n",
    "        dones       = tf.convert_to_tensor(np.array([sample[4] for sample in mini_batch]))\n",
    "        \n",
    "        if self.show_media_info == False:\n",
    "            self.show_media_info = True\n",
    "            print('Start to train, check batch shapes')\n",
    "            print('**** shape of mini_batch', np.shape(mini_batch),type(mini_batch))\n",
    "            print('**** shape of states', np.shape(states),type(states))\n",
    "            print('**** shape of actions', np.shape(actions),type(actions))\n",
    "            print('**** shape of rewards', np.shape(rewards),type(rewards))\n",
    "            print('**** shape of next_states', np.shape(next_states),type(next_states))\n",
    "            print('**** shape of dones', np.shape(dones),type(dones))\n",
    "            if self.er_type == \"HER\":\n",
    "                goals = tf.convert_to_tensor(np.array([sample[5] for sample in mini_batch]))\n",
    "                print('**** shape of goals', np.shape(goals),type(goals))\n",
    "\n",
    "        model_params = self.model.trainable_variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            # get q value\n",
    "            q = self.model(states)\n",
    "            one_hot_action = tf.one_hot(actions, self.action_size)\n",
    "            q = tf.reduce_sum(one_hot_action * q, axis=1)\n",
    "            q = tf.expand_dims(q,axis=1)\n",
    "            # Target q and maximum target q\n",
    "            target_q = tf.stop_gradient(self.target_model(next_states))\n",
    "            max_q = tf.reduce_max(target_q,axis=1)\n",
    "            max_q = tf.expand_dims(max_q,axis=1)\n",
    "            \n",
    "            targets = rewards + (1 - dones) * self.discount_factor * max_q\n",
    "            td_error = targets - q\n",
    "            loss = tf.reduce_mean(tf.square(targets - q))\n",
    "            \n",
    "        grads = tape.gradient(loss, model_params)\n",
    "        self.optimizer.apply_gradients(zip(grads, model_params))\n",
    "\n",
    "        if self.er_type == \"PER\":\n",
    "            sample_importance = td_error.numpy()\n",
    "            for i in range(self.batch_size):\n",
    "                self.memory.update(idxs[i], sample_importance[i])\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def update_network(self):\n",
    "        if self.steps % self.update_period != 0:\n",
    "            self.soft_update_target_model()\n",
    "        return\n",
    "\n",
    "    def load_model(self,at):\n",
    "        self.model.load_weights( at + self.filename + \"_TF\")\n",
    "        self.target_model.load_weights(at + self.filename + \"_TF\")\n",
    "        return\n",
    "\n",
    "    def save_model(self,at):\n",
    "        self.model.save_weights( at + self.filename + \"_TF\", save_format=\"tf\")\n",
    "        self.target_model.save_weights(at + self.filename + \"_TF\", save_format=\"tf\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    cfg = {\\\n",
    "            # \"ENV\":\"Pong-v0\",\\\n",
    "            \"ENV\":\"CartPole-v1\",\\\n",
    "            \"RL\":{\n",
    "                \"ALGORITHM\":\"DQN\",\\\n",
    "                \"NETWORK\":{\n",
    "                    \"LAYER\":[255,255],\\\n",
    "\n",
    "                }\n",
    "            },\\\n",
    "            \"ER\":\n",
    "                {\n",
    "                    \"ALGORITHM\":\"ER\",\\\n",
    "                    \"REPLAY_N\":8,\\\n",
    "                    \"STRATEGY\":\"EPISODE\",\\\n",
    "                    # \"REWARD_FUNC\":reward_function,\\\n",
    "                    # \"DONE_FUNC\":done_function,\\\n",
    "                },\\\n",
    "            \"BATCH_SIZE\":8,\\\n",
    "            \"TRAIN_START\":500,\\\n",
    "            \"MEMORY_SIZE\":20000,\\\n",
    "            }\n",
    "    env_config = env_configs[cfg[\"ENV\"]]\n",
    "    if cfg[\"ER\"][\"ALGORITHM\"] == \"HER\":\n",
    "        FILENAME = cfg[\"ENV\"] + '_' + cfg[\"RL\"][\"ALGORITHM\"] + '_' + cfg[\"ER\"][\"ALGORITHM\"] + '_' + cfg[\"HER\"][\"STRATEGY\"]\n",
    "    else:\n",
    "        FILENAME = cfg[\"ENV\"] + '_' + cfg[\"RL\"][\"ALGORITHM\"] + '_' + cfg[\"ER\"][\"ALGORITHM\"]\n",
    "    EPISODES = env_config[\"EPISODES\"]\n",
    "    END_SCORE = env_config[\"END_SCORE\"]\n",
    "\n",
    "    env = gym.make(cfg[\"ENV\"])\n",
    "    agent = DQNAgent(env, cfg)\n",
    "\n",
    "    plt.clf()\n",
    "    figure = plt.gcf()\n",
    "    figure.set_size_inches(8,6)\n",
    "\n",
    "    scores_avg, scores_raw, episodes, losses = [], [], [], []\n",
    "    epsilons = []\n",
    "    score_avg = 0\n",
    "    end = False\n",
    "    show_media_info = True\n",
    "    goal = np.array([1.0,0.0,0.0])\n",
    "    \n",
    "    for e in range(EPISODES):\n",
    "        # Episode initialization\n",
    "        done = False\n",
    "        score = 0\n",
    "        loss_list = []\n",
    "        state = env.reset()\n",
    "        while not done:\n",
    "            # env.render()\n",
    "            # Interact with env.\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done, goal)\n",
    "            loss = agent.train_model()\n",
    "            agent.update_network()\n",
    "            state = next_state\n",
    "            # \n",
    "            score += reward\n",
    "            loss_list.append(loss)\n",
    "            if show_media_info:\n",
    "                print(\"-------------- Variable shapes --------------\")\n",
    "                print(\"State Shape : \", np.shape(state))\n",
    "                print(\"Action Shape : \", np.shape(action))\n",
    "                print(\"Reward Shape : \", np.shape(reward))\n",
    "                print(\"done Shape : \", np.shape(done))\n",
    "                print(\"---------------------------------------------\")\n",
    "                show_media_info = False\n",
    "            if done:\n",
    "                score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\n",
    "                print(\"episode: {0:3d} | score avg: {1:3.2f} | mem size {2:6d} |\"\n",
    "                    .format(e, score_avg, len(agent.memory)))\n",
    "\n",
    "                episodes.append(e)\n",
    "                scores_avg.append(score_avg)\n",
    "                scores_raw.append(score)\n",
    "                losses.append(np.mean(loss_list))\n",
    "                epsilons.append(agent.epsilon)\n",
    "                # View data\n",
    "                plt.clf()\n",
    "                plt.subplot(311)\n",
    "                plt.plot(episodes, scores_avg, 'b')\n",
    "                plt.plot(episodes, scores_raw, 'b', alpha=0.8, linewidth=0.5)\n",
    "                plt.xlabel('episode'); plt.ylabel('average score'); plt.grid()\n",
    "                plt.title(FILENAME)\n",
    "                plt.subplot(312)\n",
    "                plt.plot(episodes, epsilons, 'b')\n",
    "                plt.xlabel('episode'); plt.ylabel('epsilon'); plt.grid()\n",
    "                plt.subplot(313)\n",
    "                plt.plot(episodes, losses, 'b')\n",
    "                plt.xlabel('episode'); plt.ylabel('losses') ;plt.grid()\n",
    "                # plt.savefig(workspace_path + \"\\\\result\\\\img\\\\\" + FILENAME + \"_TF.jpg\", dpi=100)\n",
    "\n",
    "                # 이동 평균이 0 이상일 때 종료\n",
    "                if score_avg > END_SCORE:\n",
    "                    # agent.save_model(workspace_path + \"\\\\result\\\\save_model\\\\\")\n",
    "                    end = True\n",
    "                    break\n",
    "        if end == True:\n",
    "            env.close()\n",
    "            # np.save(workspace_path + \"\\\\result\\\\data\\\\\" + FILENAME + \"_TF_epi\",  episodes)\n",
    "            # np.save(workspace_path + \"\\\\result\\\\data\\\\\" + FILENAME + \"_TF_scores_avg\",scores_avg)\n",
    "            # np.save(workspace_path + \"\\\\result\\\\data\\\\\" + FILENAME + \"_TF_scores_raw\",scores_raw)\n",
    "            # np.save(workspace_path + \"\\\\result\\\\data\\\\\" + FILENAME + \"_TF_losses\",losses)\n",
    "            print(\"End\")\n",
    "            break"
   ]
  }
 ]
}