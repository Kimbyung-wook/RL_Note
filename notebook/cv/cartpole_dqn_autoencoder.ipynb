{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('TF241': conda)"
  },
  "interpreter": {
   "hash": "7550fa3b4b9bd0b158a72075e6f5a855890dfd3a1674a25ba81ceef29fa1ff7d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Find RL_Note path and append sys path\r\n",
    "import os, sys\r\n",
    "cwd = os.getcwd()\r\n",
    "dir_name = 'RL_note'\r\n",
    "pos = cwd.find(dir_name)\r\n",
    "root_path = cwd[0:pos] + dir_name\r\n",
    "sys.path.append(root_path)\r\n",
    "print(root_path)\r\n",
    "workspace_path = root_path + \"\\\\pys\""
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "c:\\Users\\User\\RL_note\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import gym\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras.models import Model\r\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Conv2DTranspose, Flatten, Dropout, BatchNormalization, Reshape, LeakyReLU, ReLU\r\n",
    "from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy\r\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\r\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\r\n",
    "\r\n",
    "from pys.utils.er import ReplayMemory\r\n",
    "from pys.utils.per import ProportionalPrioritizedMemory\r\n",
    "from pys.utils.her import HindsightMemory\r\n",
    "from pys.model.q_network import QNetwork"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_encoder(input_shape, compressed_shape):\r\n",
    "    X_input = Input(shape=input_shape, name='Input')\r\n",
    "    X = X_input\r\n",
    "    X = Conv2D(filters=32, kernel_size=8, strides=4, padding='same', \\\r\n",
    "                data_format=\"channels_last\", name='Encoder1')(X)\r\n",
    "    X = BatchNormalization(name=\"BN1\", axis=1)(X)\r\n",
    "    X = LeakyReLU(name='LReLU1')(X)\r\n",
    "\r\n",
    "    X = Conv2D(filters=64, kernel_size=8, strides=4, padding='same', \\\r\n",
    "                data_format=\"channels_last\", name='Encoder2')(X)\r\n",
    "    X = BatchNormalization(name=\"BN2\", axis=1)(X)\r\n",
    "    X = LeakyReLU(name='LReLU2')(X)\r\n",
    "\r\n",
    "    X = Conv2D(filters=64, kernel_size=3, strides=2, padding='same', \\\r\n",
    "                data_format=\"channels_last\", name='Encoder3')(X)\r\n",
    "    X = BatchNormalization(name=\"BN3\", axis=1)(X)\r\n",
    "    X = LeakyReLU(name='LReLU3')(X)\r\n",
    "\r\n",
    "    X = Conv2D(filters=64, kernel_size=3, strides=1, padding='same', \\\r\n",
    "                data_format=\"channels_last\", name='Encoder4')(X)\r\n",
    "    X = BatchNormalization(name=\"BN4\", axis=1)(X)\r\n",
    "    X = LeakyReLU(name='LReLU4')(X)\r\n",
    "    # X = MaxPool2D(filters-\r\n",
    "\r\n",
    "    X = Flatten(name='Flattening')(X)\r\n",
    "\r\n",
    "    encoder_output = Dense(compressed_shape[0], activation='linear', \\\r\n",
    "                name='EncoderOut')(X)\r\n",
    "    encoder_model = Model(inputs=X_input, outputs=encoder_output, name='Encoder')\r\n",
    "\r\n",
    "    return encoder_model\r\n",
    "\r\n",
    "def get_decoder(input_shape, compressed_shape):\r\n",
    "    X_input = Input(shape=compressed_shape, name='Input')\r\n",
    "    X = X_input\r\n",
    "\r\n",
    "    X = Dense(units=7*7*64, activation='linear', \\\r\n",
    "                name='DeFlattening')(X)\r\n",
    "    X = Reshape((7,7,64), name='Reshape')(X)\r\n",
    "\r\n",
    "    X = Conv2DTranspose(filters=64, kernel_size=3, strides=1, padding='same', \\\r\n",
    "                data_format=\"channels_last\", name='Decoder1')(X)\r\n",
    "    X = BatchNormalization(name=\"BN1\", axis=1)(X)\r\n",
    "    X = LeakyReLU(name='LReLU1')(X)\r\n",
    "\r\n",
    "    X = Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', \\\r\n",
    "                data_format=\"channels_last\", name='Decoder2')(X)\r\n",
    "    X = BatchNormalization(name=\"BN2\", axis=1)(X)\r\n",
    "    X = LeakyReLU(name='LReLU2')(X)\r\n",
    "\r\n",
    "    X = Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', \\\r\n",
    "                data_format=\"channels_last\", name='Decoder3')(X)\r\n",
    "    X = BatchNormalization(name=\"BN3\", axis=1)(X)\r\n",
    "    X = LeakyReLU(name='LReLU3')(X)\r\n",
    "\r\n",
    "    X = Conv2DTranspose(filters=32, kernel_size=3, strides=1, padding='same', \\\r\n",
    "                data_format=\"channels_last\", name='Decoder4')(X)\r\n",
    "    X = BatchNormalization(name=\"BN4\", axis=1)(X)\r\n",
    "    X = LeakyReLU(name='LReLU4')(X)\r\n",
    "\r\n",
    "    decoder_output = Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding='same', \\\r\n",
    "                data_format=\"channels_last\", name='Decoder_out', activation='tanh')(X)\r\n",
    "    decoder_model = Model(inputs=X_input, outputs=decoder_output, name='Decoder')\r\n",
    "    \r\n",
    "    return decoder_model\r\n",
    "\r\n",
    "def AutoEncoder(input_shape, compressed_shape):\r\n",
    "        input_shape = input_shape\r\n",
    "        compressed_shape = compressed_shape\r\n",
    "\r\n",
    "        # Hyper-parameter\r\n",
    "        learning_rate = 0.001\r\n",
    "        batch_size = 32\r\n",
    "\r\n",
    "        # Define auto-encoder\r\n",
    "        encoder = get_encoder(input_shape=input_shape, compressed_shape=compressed_shape)\r\n",
    "        decoder = get_decoder(input_shape=input_shape, compressed_shape=compressed_shape)\r\n",
    "        # encoder.compile(optimizer=Adam(learning_rate=learning_rate), loss=MeanSquaredError())\r\n",
    "        # decoder.compile(optimizer=Adam(learning_rate=learning_rate), loss=MeanSquaredError())\r\n",
    "        # encoder.summary()\r\n",
    "        # decoder.summary()\r\n",
    "\r\n",
    "        # Connect encoder with decoder\r\n",
    "        encoder_in = Input(shape=input_shape)\r\n",
    "        decoder_out= decoder(encoder(encoder_in))\r\n",
    "        \r\n",
    "        auto_encoder = Model(inputs=encoder_in, outputs=decoder_out)\r\n",
    "        auto_encoder.compile(optimizer=Adam(learning_rate=learning_rate), loss=MeanSquaredError())\r\n",
    "        # auto_encoder.compile(optimizer=Adam(learning_rate=learning_rate), loss=BinaryCrossentropy())\r\n",
    "        auto_encoder.summary()\r\n",
    "        return auto_encoder, encoder, decoder"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DQNAgent:\r\n",
    "    def __init__(self, env:object, cfg:dict):\r\n",
    "        self.state_size = (160,240)\r\n",
    "        self.action_size= env.action_space.n\r\n",
    "        self.env_name   = cfg[\"ENV\"]\r\n",
    "        self.rl_type    = \"DQN\"\r\n",
    "        self.er_type    = cfg[\"ER\"][\"ALGORITHM\"].upper()\r\n",
    "        self.filename   = cfg[\"ENV\"] + '_' + cfg[\"RL\"][\"ALGORITHM\"] + '_' + cfg[\"ER\"][\"ALGORITHM\"]\r\n",
    "\r\n",
    "        # Experience Replay\r\n",
    "        self.batch_size = cfg[\"BATCH_SIZE\"]\r\n",
    "        self.train_start = cfg[\"TRAIN_START\"]\r\n",
    "        self.buffer_size = cfg[\"MEMORY_SIZE\"]\r\n",
    "        if self.er_type == \"ER\":\r\n",
    "            self.memory = ReplayMemory(capacity=self.buffer_size)\r\n",
    "        elif self.er_type == \"PER\":\r\n",
    "            self.memory = ProportionalPrioritizedMemory(capacity=self.buffer_size)\r\n",
    "        elif self.er_type == \"HER\":\r\n",
    "            self.memory = HindsightMemory(\\\r\n",
    "                capacity            = self.buffer_size,\\\r\n",
    "                replay_n            = cfg[\"ER\"][\"REPLAY_N\"],\\\r\n",
    "                replay_strategy     = cfg[\"ER\"][\"STRATEGY\"],\\\r\n",
    "                reward_func         = cfg[\"ER\"][\"REWARD_FUNC\"],\\\r\n",
    "                done_func           = cfg[\"ER\"][\"DONE_FUNC\"])\r\n",
    "            self.filename = cfg[\"ENV\"] + '_' + cfg[\"RL\"][\"ALGORITHM\"] + '_' + cfg[\"ER\"][\"ALGORITHM\"] + '_' + cfg[\"ER\"][\"STRATEGY\"]\r\n",
    "\r\n",
    "        # Hyper-parameters for learning\r\n",
    "        self.discount_factor = 0.99\r\n",
    "        self.learning_rate = 0.001\r\n",
    "        self.epsilon = 1.0\r\n",
    "        self.epsilon_decay = 0.999\r\n",
    "        self.epsilon_min = 0.01\r\n",
    "        self.tau = 0.005\r\n",
    "        \r\n",
    "        # Neural Network Architecture\r\n",
    "        self.compressed_shape = (4,)\r\n",
    "        self.auto_encoder, self.encoder, self.decoder = AutoEncoder(self.state_size, self.compressed_shape)\r\n",
    "        self.model        = QNetwork(self.state_size, self.action_size, cfg[\"RL\"][\"NETWORK\"])\r\n",
    "        self.target_model = QNetwork(self.state_size, self.action_size, cfg[\"RL\"][\"NETWORK\"])\r\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr=self.learning_rate)\r\n",
    "        self.hard_update_target_model()\r\n",
    "        \r\n",
    "        # Miscellaneous\r\n",
    "        self.show_media_info = False\r\n",
    "        self.steps = 0\r\n",
    "        self.update_period = 100\r\n",
    "        # self.interaction_period = 1\r\n",
    "\r\n",
    "        print(self.filename)\r\n",
    "        print('States {0}, Actions {1}'.format(self.state_size, self.action_size))\r\n",
    "        \r\n",
    "    def remember(self, state, action, reward, next_state, done, goal=None):\r\n",
    "        state       = np.array(state,       dtype=np.float32)\r\n",
    "        action      = np.array([action])\r\n",
    "        reward      = np.array([reward],    dtype=np.float32)\r\n",
    "        done        = np.array([done],      dtype=np.float32)\r\n",
    "        next_state  = np.array(next_state,  dtype=np.float32)\r\n",
    "        if self.er_type == \"HER\":\r\n",
    "            goal        = np.array(goal,        dtype=np.float32)\r\n",
    "            transition  = (state, action, reward, next_state, done, goal)\r\n",
    "        else:\r\n",
    "            transition  = (state, action, reward, next_state, done)\r\n",
    "        self.memory.append(transition)\r\n",
    "        return\r\n",
    "        \r\n",
    "    def hard_update_target_model(self):\r\n",
    "        self.target_model.set_weights(self.model.get_weights())\r\n",
    "\r\n",
    "    def soft_update_target_model(self):\r\n",
    "        tau = self.tau\r\n",
    "        for (net, target_net) in zip(   self.model.trainable_variables,\r\n",
    "                                        self.target_model.trainable_variables):\r\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\r\n",
    "\r\n",
    "    def get_action(self,state):\r\n",
    "        self.steps += 1\r\n",
    "        # Exploration and Exploitation\r\n",
    "        if (np.random.rand() <= self.epsilon):\r\n",
    "            return random.randrange(self.action_size)\r\n",
    "        else:\r\n",
    "            state = tf.convert_to_tensor([state], dtype=tf.float32)\r\n",
    "            return np.argmax(self.model(state))\r\n",
    "        \r\n",
    "    def train_model(self):\r\n",
    "        # Train from Experience Replay\r\n",
    "        # Training Condition - Memory Size\r\n",
    "        if len(self.memory) < self.train_start:\r\n",
    "            return 0.0\r\n",
    "        # Decaying Exploration Ratio\r\n",
    "        if self.epsilon > self.epsilon_min:\r\n",
    "            self.epsilon *= self.epsilon_decay\r\n",
    "        # Sampling from the memory\r\n",
    "        if self.er_type == \"ER\" or self.er_type == \"HER\":\r\n",
    "            mini_batch = self.memory.sample(self.batch_size)\r\n",
    "        elif self.er_type == \"PER\":\r\n",
    "            mini_batch, idxs, is_weights = self.memory.sample(self.batch_size)\r\n",
    "\r\n",
    "        states      = tf.convert_to_tensor(np.array([sample[0] for sample in mini_batch]))\r\n",
    "        actions     = tf.convert_to_tensor(np.array([sample[1][0] for sample in mini_batch]))\r\n",
    "        rewards     = tf.convert_to_tensor(np.array([sample[2] for sample in mini_batch]))\r\n",
    "        next_states = tf.convert_to_tensor(np.array([sample[3] for sample in mini_batch]))\r\n",
    "        dones       = tf.convert_to_tensor(np.array([sample[4] for sample in mini_batch]))\r\n",
    "        \r\n",
    "        if self.show_media_info == False:\r\n",
    "            self.show_media_info = True\r\n",
    "            print('Start to train, check batch shapes')\r\n",
    "            print('**** shape of mini_batch', np.shape(mini_batch),type(mini_batch))\r\n",
    "            print('**** shape of states', np.shape(states),type(states))\r\n",
    "            print('**** shape of actions', np.shape(actions),type(actions))\r\n",
    "            print('**** shape of rewards', np.shape(rewards),type(rewards))\r\n",
    "            print('**** shape of next_states', np.shape(next_states),type(next_states))\r\n",
    "            print('**** shape of dones', np.shape(dones),type(dones))\r\n",
    "            if self.er_type == \"HER\":\r\n",
    "                goals = tf.convert_to_tensor(np.array([sample[5] for sample in mini_batch]))\r\n",
    "                print('**** shape of goals', np.shape(goals),type(goals))\r\n",
    "\r\n",
    "        model_params = self.model.trainable_variables\r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            # get q value\r\n",
    "            q = self.model(states)\r\n",
    "            one_hot_action = tf.one_hot(actions, self.action_size)\r\n",
    "            q = tf.reduce_sum(one_hot_action * q, axis=1)\r\n",
    "            q = tf.expand_dims(q,axis=1)\r\n",
    "            # Target q and maximum target q\r\n",
    "            target_q = tf.stop_gradient(self.target_model(next_states))\r\n",
    "            max_q = tf.reduce_max(target_q,axis=1)\r\n",
    "            max_q = tf.expand_dims(max_q,axis=1)\r\n",
    "            \r\n",
    "            targets = rewards + (1 - dones) * self.discount_factor * max_q\r\n",
    "            td_error = targets - q\r\n",
    "            loss = tf.reduce_mean(tf.square(targets - q))\r\n",
    "            \r\n",
    "        grads = tape.gradient(loss, model_params)\r\n",
    "        self.optimizer.apply_gradients(zip(grads, model_params))\r\n",
    "\r\n",
    "        if self.er_type == \"PER\":\r\n",
    "            sample_importance = td_error.numpy()\r\n",
    "            for i in range(self.batch_size):\r\n",
    "                self.memory.update(idxs[i], sample_importance[i])\r\n",
    "\r\n",
    "        return loss\r\n",
    "\r\n",
    "    def step_update(self):\r\n",
    "        if self.steps % self.update_period != 0:\r\n",
    "            self.soft_update_target_model()\r\n",
    "        return\r\n",
    "\r\n",
    "    def episode_update(self):\r\n",
    "        checkpoint_path = '01_basic_auto_encoder_' + data + '.ckpt'\r\n",
    "        if (training==True):\r\n",
    "            # Train\r\n",
    "            checkpoint = ModelCheckpoint(checkpoint_path, \r\n",
    "                                        save_best_only=True, \r\n",
    "                                        save_weights_only=True, \r\n",
    "                                        monitor='loss', \r\n",
    "                                        verbose=1)\r\n",
    "            auto_encoder.fit(x_train, x_train, \r\n",
    "                            batch_size=batch_size, \r\n",
    "                            epochs=100, \r\n",
    "                            callbacks=[checkpoint], \r\n",
    "                            )\r\n",
    "            auto_encoder.save_weights(checkpoint_path)\r\n",
    "            print('Save model weights')\r\n",
    "        else:\r\n",
    "            auto_encoder.load_weights(checkpoint_path)\r\n",
    "            print('Load model weights')\r\n",
    "        pass\r\n",
    "\r\n",
    "    def load_model(self,at:str):\r\n",
    "        self.model.load_weights( at + self.filename + \"_TF\")\r\n",
    "        self.target_model.load_weights(at + self.filename + \"_TF\")\r\n",
    "        return\r\n",
    "\r\n",
    "    def save_model(self,at:str):\r\n",
    "        self.model.save_weights( at + self.filename + \"_TF\", save_format=\"tf\")\r\n",
    "        self.target_model.save_weights(at + self.filename + \"_TF\", save_format=\"tf\")\r\n",
    "        return"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\r\n",
    "    cfg = {\\\r\n",
    "            # \"ENV\":\"Pong-v0\",\\\r\n",
    "            \"ENV\":\"CartPole-v1\",\\\r\n",
    "            \"RL\":{\r\n",
    "                \"ALGORITHM\":\"DQN\",\\\r\n",
    "                \"NETWORK\":{\r\n",
    "                    \"LAYER\":[255,255],\\\r\n",
    "\r\n",
    "                }\r\n",
    "            },\\\r\n",
    "            \"ER\":\r\n",
    "                {\r\n",
    "                    \"ALGORITHM\":\"ER\",\\\r\n",
    "                    \"REPLAY_N\":8,\\\r\n",
    "                    \"STRATEGY\":\"EPISODE\",\\\r\n",
    "                    # \"REWARD_FUNC\":reward_function,\\\r\n",
    "                    # \"DONE_FUNC\":done_function,\\\r\n",
    "                },\\\r\n",
    "            \"BATCH_SIZE\":8,\\\r\n",
    "            \"TRAIN_START\":1000,\\\r\n",
    "            \"MEMORY_SIZE\":20000,\\\r\n",
    "            }\r\n",
    "    env_config = env_configs[cfg[\"ENV\"]]\r\n",
    "    if cfg[\"ER\"][\"ALGORITHM\"] == \"HER\":\r\n",
    "        FILENAME = cfg[\"ENV\"] + '_' + cfg[\"RL\"][\"ALGORITHM\"] + '_' + cfg[\"ER\"][\"ALGORITHM\"] + '_' + cfg[\"HER\"][\"STRATEGY\"]\r\n",
    "    else:\r\n",
    "        FILENAME = cfg[\"ENV\"] + '_' + cfg[\"RL\"][\"ALGORITHM\"] + '_' + cfg[\"ER\"][\"ALGORITHM\"]\r\n",
    "    EPISODES = env_config[\"EPISODES\"]\r\n",
    "    END_SCORE = env_config[\"END_SCORE\"]\r\n",
    "\r\n",
    "    env = gym.make(cfg[\"ENV\"])\r\n",
    "    agent = DQNAgent(env, cfg)\r\n",
    "\r\n",
    "    plt.clf()\r\n",
    "    figure = plt.gcf()\r\n",
    "    figure.set_size_inches(8,6)\r\n",
    "\r\n",
    "    scores_avg, scores_raw, episodes, losses = [], [], [], []\r\n",
    "    epsilons = []\r\n",
    "    score_avg = 0\r\n",
    "    end = False\r\n",
    "    show_media_info = True\r\n",
    "    goal = np.array([1.0,0.0,0.0])\r\n",
    "    \r\n",
    "    for e in range(EPISODES):\r\n",
    "        # Episode initialization\r\n",
    "        done = False\r\n",
    "        score = 0\r\n",
    "        loss_list = []\r\n",
    "        state = env.reset()\r\n",
    "        while not done:\r\n",
    "            # env.render()\r\n",
    "            # Interact with env.\r\n",
    "            action = agent.get_action(state)\r\n",
    "            next_state, reward, done, info = env.step(action)\r\n",
    "            agent.remember(state, action, reward, next_state, done, goal)\r\n",
    "            loss = agent.train_model()\r\n",
    "            agent.step_update()\r\n",
    "            state = next_state\r\n",
    "            # \r\n",
    "            score += reward\r\n",
    "            loss_list.append(loss)\r\n",
    "            if show_media_info:\r\n",
    "                print(\"-------------- Variable shapes --------------\")\r\n",
    "                print(\"State Shape : \", np.shape(state))\r\n",
    "                print(\"Action Shape : \", np.shape(action))\r\n",
    "                print(\"Reward Shape : \", np.shape(reward))\r\n",
    "                print(\"done Shape : \", np.shape(done))\r\n",
    "                print(\"---------------------------------------------\")\r\n",
    "                show_media_info = False\r\n",
    "            if done:\r\n",
    "                agent.episode_update()\r\n",
    "                score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\r\n",
    "                print(\"episode: {0:3d} | score avg: {1:3.2f} | mem size {2:6d} |\"\r\n",
    "                    .format(e, score_avg, len(agent.memory)))\r\n",
    "\r\n",
    "                episodes.append(e)\r\n",
    "                scores_avg.append(score_avg)\r\n",
    "                scores_raw.append(score)\r\n",
    "                losses.append(np.mean(loss_list))\r\n",
    "                epsilons.append(agent.epsilon)\r\n",
    "                # View data\r\n",
    "                plt.clf()\r\n",
    "                plt.subplot(311)\r\n",
    "                plt.plot(episodes, scores_avg, 'b')\r\n",
    "                plt.plot(episodes, scores_raw, 'b', alpha=0.8, linewidth=0.5)\r\n",
    "                plt.xlabel('episode'); plt.ylabel('average score'); plt.grid()\r\n",
    "                plt.title(FILENAME)\r\n",
    "                plt.subplot(312)\r\n",
    "                plt.plot(episodes, epsilons, 'b')\r\n",
    "                plt.xlabel('episode'); plt.ylabel('epsilon'); plt.grid()\r\n",
    "                plt.subplot(313)\r\n",
    "                plt.plot(episodes, losses, 'b')\r\n",
    "                plt.xlabel('episode'); plt.ylabel('losses') ;plt.grid()\r\n",
    "                # plt.savefig(workspace_path + \"\\\\result\\\\img\\\\\" + FILENAME + \"_TF.jpg\", dpi=100)\r\n",
    "\r\n",
    "                # 이동 평균이 0 이상일 때 종료\r\n",
    "                if score_avg > END_SCORE:\r\n",
    "                    # agent.save_model(workspace_path + \"\\\\result\\\\save_model\\\\\")\r\n",
    "                    end = True\r\n",
    "                    break\r\n",
    "        if end == True:\r\n",
    "            env.close()\r\n",
    "            # np.save(workspace_path + \"\\\\result\\\\data\\\\\" + FILENAME + \"_TF_epi\",  episodes)\r\n",
    "            # np.save(workspace_path + \"\\\\result\\\\data\\\\\" + FILENAME + \"_TF_scores_avg\",scores_avg)\r\n",
    "            # np.save(workspace_path + \"\\\\result\\\\data\\\\\" + FILENAME + \"_TF_scores_raw\",scores_raw)\r\n",
    "            # np.save(workspace_path + \"\\\\result\\\\data\\\\\" + FILENAME + \"_TF_losses\",losses)\r\n",
    "            print(\"End\")\r\n",
    "            break"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}