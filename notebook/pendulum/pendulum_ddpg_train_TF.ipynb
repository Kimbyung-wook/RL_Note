{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('TF240': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8f237aa33f5a133d3a67a1e00bf3cb9d47b6c38bcc6ab493273f5d4df41b8866"
    }
   }
  },
  "interpreter": {
   "hash": "f7f3f20f7907c4d59130059bf0ac4ad20e77f96568ae424e1dedda78aa67e631"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Find RL_Note path and append sys path\r\n",
    "import os, sys\r\n",
    "cwd = os.getcwd()\r\n",
    "pos = cwd.find('RL_Note')\r\n",
    "root_path = cwd[0:pos] + 'RL_Note'\r\n",
    "sys.path.append(root_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Refer from\n",
    "#  https://pasus.tistory.com/138\n",
    "#  https://horomary.hatenablog.com/entry/2020/06/26/003806\n",
    "#  https://keras.io/examples/rl/ddpg_pendulum/\n",
    "#\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, concatenate, Lambda\n",
    "import matplotlib.pyplot as plt\n",
    "from pys.utils.ou_noise import OUActionNoise\n",
    "from pys.utils.ER import ReplayMemory\n",
    "from pys.utils.PER import ProportionalPrioritizedMemory\n",
    "from pys.utils.HER import HindsightMemory\n",
    "from pys.config.env_config import env_configs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class Actor(tf.keras.Model):\r\n",
    "    def __init__(self, state_size, action_size, action_min, action_max):\r\n",
    "        super(Actor, self).__init__()\r\n",
    "        self.action_min = action_min\r\n",
    "        self.action_max = action_max\r\n",
    "\r\n",
    "        self.fc1 = Dense(64, activation='relu')\r\n",
    "        self.fc2 = Dense(64, activation='relu')\r\n",
    "        # self.fc3 = Dense(16, activation='relu')\r\n",
    "        self.out= Dense(action_size, activation='tanh',kernel_initializer = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)) # -1 ~ +1\r\n",
    "\r\n",
    "    def call(self, x):\r\n",
    "        x       = self.fc1(x)\r\n",
    "        x       = self.fc2(x)\r\n",
    "        # x       = self.fc3(x)\r\n",
    "        action  = self.out(x)\r\n",
    "        # return self.projected_to_action_space(action)\r\n",
    "        a = Lambda(lambda x: x*self.action_max)(action)\r\n",
    "        return a\r\n",
    "\r\n",
    "class Critic(tf.keras.Model):\r\n",
    "    def __init__(self, state_size, action_size):\r\n",
    "        super(Critic, self).__init__()\r\n",
    "        self.s1 = Dense(16, activation='relu')\r\n",
    "        self.s2 = Dense(32, activation='relu')\r\n",
    "        self.a1 = Dense(32, activation='relu')\r\n",
    "        self.a2 = Dense(32, activation='relu')\r\n",
    "        self.fc1= Dense(64, activation='relu')\r\n",
    "        self.fc2= Dense(64, activation='relu')\r\n",
    "        self.out= Dense(1,  activation='linear')\r\n",
    "\r\n",
    "    def call(self,state_action):\r\n",
    "        state  = state_action[0]\r\n",
    "        action = state_action[1]\r\n",
    "        s = self.s1(state)\r\n",
    "        s = self.s2(s)\r\n",
    "        a = self.a1(action)\r\n",
    "        a = self.a2(a)\r\n",
    "        c = concatenate([s,a],axis=-1)\r\n",
    "        x = self.fc1(c)\r\n",
    "        x = self.fc2(x)\r\n",
    "        q = self.out(x)\r\n",
    "        return q\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self, env:object, cfg:dict):\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size= env.action_space.shape[0]\n",
    "        self.action_min = env.action_space.low[0]\n",
    "        self.action_max = env.action_space.high[0]\n",
    "        self.env_name   = cfg[\"ENV\"]\n",
    "        self.rl_type    = \"DDPG\"\n",
    "        self.er_type    = cfg[\"ER\"].upper()\n",
    "        print('Env Name : ',cfg[\"ENV\"])\n",
    "        print('States {0}, Actions {1}'.format(self.state_size, self.action_size))\n",
    "        for i in range(self.action_size):\n",
    "            print(i+1,'th Action space {0:.2f} ~ {1:.2f}'.format(env.action_space.low[i], env.action_space.high[i]))\n",
    "        self.filename = cfg[\"ENV\"] + '_' + cfg[\"RL\"] + '_' + cfg[\"ER\"]\n",
    "\n",
    "        # Experience Replay\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 2000\n",
    "        self.buffer_size = 50000\n",
    "        if self.er_type == \"ER\":\n",
    "            self.memory = ReplayMemory(capacity=self.buffer_size)\n",
    "        elif self.er_type == \"PER\":\n",
    "            self.memory = ProportionalPrioritizedMemory(capacity=self.buffer_size)\n",
    "        elif self.er_type == \"HER\":\n",
    "            self.memory = HindsightMemory(\\\n",
    "                capacity            = self.buffer_size,\\\n",
    "                replay_n            = cfg[\"HER\"][\"REPLAY_N\"],\\\n",
    "                replay_strategy     = cfg[\"HER\"][\"STRATEGY\"],\\\n",
    "                reward_func         = cfg[\"HER\"][\"REWARD_FUNC\"],\\\n",
    "                done_func           = cfg[\"HER\"][\"DONE_FUNC\"])\n",
    "            self.filename = cfg[\"ENV\"] + '_' + cfg[\"RL\"] + '_' + cfg[\"ER\"] + '_' + cfg[\"HER\"][\"STRATEGY\"]\n",
    "\n",
    "        # Hyper params for learning\n",
    "        self.discount_factor = 0.99\n",
    "        self.actor_learning_rate  = 0.001\n",
    "        self.critic_learning_rate = 0.002\n",
    "        self.tau = 0.005\n",
    "\n",
    "        # Networks\n",
    "        self.critic         = Critic(self.state_size, self.action_size)\n",
    "        self.target_critic  = Critic(self.state_size, self.action_size)\n",
    "        self.actor          = Actor(self.state_size, self.action_size, self.action_min, self.action_max)\n",
    "        self.target_actor   = Actor(self.state_size, self.action_size, self.action_min, self.action_max)\n",
    "        self.critic_optimizer   = tf.keras.optimizers.Adam(lr=self.critic_learning_rate)\n",
    "        self.actor_optimizer    = tf.keras.optimizers.Adam(lr=self.actor_learning_rate)\n",
    "\n",
    "        self.actor.build(input_shape=(None, self.state_size))\n",
    "        self.target_actor.build(input_shape=(None, self.state_size))\n",
    "        state_in = Input((self.state_size,))\n",
    "        action_in = Input((self.action_size,))\n",
    "        self.actor(state_in)\n",
    "        self.target_actor(state_in)\n",
    "        self.critic([state_in, action_in])\n",
    "        self.target_critic([state_in, action_in])\n",
    "        # self.actor.summary()\n",
    "        # self.critic.summary()\n",
    "        \n",
    "        self.hard_update_target_model()\n",
    "\n",
    "        # Noise\n",
    "        self.noise_std_dev = 0.2\n",
    "        self.ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(self.noise_std_dev) * np.ones(1))\n",
    "\n",
    "        # Miscellaneous\n",
    "        self.show_media_info = False\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done, goal=None):\n",
    "        state       = np.array(state,       dtype=np.float32)\n",
    "        action      = np.array(action,      dtype=np.float32)\n",
    "        reward      = np.array([reward],    dtype=np.float32)\n",
    "        done        = np.array([done],      dtype=np.float32)\n",
    "        next_state  = np.array(next_state,  dtype=np.float32)\n",
    "        if self.er_type == \"HER\":\n",
    "            goal        = np.array(goal,        dtype=np.float32)\n",
    "            transition  = (state, action, reward, next_state, done, goal)\n",
    "        else:\n",
    "            transition  = (state, action, reward, next_state, done)\n",
    "        self.memory.append(transition)\n",
    "        return\n",
    "\n",
    "    def hard_update_target_model(self):\n",
    "        self.target_actor.set_weights(self.actor.get_weights())\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\n",
    "\n",
    "    def soft_update_target_model(self):\n",
    "        tau = self.tau\n",
    "        for (net, target_net) in zip(   self.actor.trainable_variables,\n",
    "                                        self.target_actor.trainable_variables):\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\n",
    "        for (net, target_net) in zip(   self.critic.trainable_variables,\n",
    "                                        self.target_critic.trainable_variables):\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\n",
    "\n",
    "    def get_action(self,state):\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        action = self.actor(state)\n",
    "        # Exploration and Exploitation\n",
    "        action_from_net = action.numpy()[0]\n",
    "        action_from_noise = self.ou_noise()\n",
    "        return np.clip(action_from_net+action_from_noise,self.action_min,self.action_max)\n",
    "\n",
    "    def train_model(self):\n",
    "        # Train from Experience Replay\n",
    "        # Training Condition - Memory Size\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return 0.0,0.0\n",
    "        # Sampling from the memory\n",
    "        if self.er_type == \"ER\" or self.er_type == \"HER\":\n",
    "            mini_batch = self.memory.sample(self.batch_size)\n",
    "        elif self.er_type == \"PER\":\n",
    "            mini_batch, idxs, is_weights = self.memory.sample(self.batch_size)\n",
    "\n",
    "        states      = tf.convert_to_tensor(np.array([sample[0] for sample in mini_batch]))\n",
    "        actions     = tf.convert_to_tensor(np.array([sample[1] for sample in mini_batch]))\n",
    "        rewards     = tf.convert_to_tensor(np.array([sample[2] for sample in mini_batch]))\n",
    "        next_states = tf.convert_to_tensor(np.array([sample[3] for sample in mini_batch]))\n",
    "        dones       = tf.convert_to_tensor(np.array([sample[4] for sample in mini_batch]))\n",
    "        \n",
    "        if self.show_media_info == False:\n",
    "            self.show_media_info = True\n",
    "            print('Start to train, check batch shapes')\n",
    "            print('shape of states', np.shape(states),type(states))\n",
    "            print('shape of actions', np.shape(actions),type(actions))\n",
    "            print('shape of rewards', np.shape(rewards),type(rewards))\n",
    "            print('shape of next_states', np.shape(next_states),type(next_states))\n",
    "            print('shape of dones', np.shape(dones),type(dones))\n",
    "            if self.er_type == \"HER\":\n",
    "                goals = tf.convert_to_tensor(np.array([sample[5] for sample in mini_batch]))\n",
    "                print('shape of goals', np.shape(goals),type(goals))\n",
    "\n",
    "        # Update critic\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor(next_states,training=True)\n",
    "            target_q = self.target_critic([next_states,target_actions],training=True)\n",
    "            target_value = rewards + (1 - dones) * self.discount_factor * target_q\n",
    "            q = self.critic([states, actions],training=True)\n",
    "            td_error = target_value - q\n",
    "            if self.er_type == \"ER\" or self.er_type == \"HER\":\n",
    "                critic_loss = tf.math.reduce_mean(tf.math.square(target_value - q))\n",
    "            elif self.er_type == \"PER\":\n",
    "                critic_loss = tf.math.reduce_mean(is_weights * tf.math.square(target_value - q))\n",
    "        critic_loss_out = critic_loss.numpy()\n",
    "        critic_params = self.critic.trainable_variables\n",
    "        critic_grads = tape.gradient(critic_loss, critic_params)\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grads, critic_params))\n",
    "\n",
    "        # Update critic\n",
    "        with tf.GradientTape() as tape:\n",
    "            new_actions = self.actor(states,training=True)\n",
    "            new_q = self.critic([states, new_actions],training=True)\n",
    "            actor_loss = -tf.reduce_mean(new_q)\n",
    "        actor_loss_out = actor_loss.numpy()\n",
    "        actor_params = self.actor.trainable_variables\n",
    "        actor_grads = tape.gradient(actor_loss, actor_params)\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, actor_params))\n",
    "        \n",
    "        if self.er_type == \"PER\":\n",
    "            sample_importance = td_error.numpy()\n",
    "            for i in range(self.batch_size):\n",
    "                self.memory.update(idxs[i], sample_importance[i])\n",
    "\n",
    "        self.soft_update_target_model()\n",
    "        return critic_loss_out, actor_loss_out\n",
    "\n",
    "    def load_model(self):\n",
    "        # self.actor.load_weights( \"./save_model/\" + self.filename + \"_TF_actor\")\n",
    "        # self.critic.load_weights(\"./save_model/\" + self.filename + \"_TF_critic\")\n",
    "        return\n",
    "\n",
    "    def save_model(self):\n",
    "        # self.actor.save_weights( \"./save_model/\" + self.filename + \"_TF_actor\", save_format=\"tf\")\n",
    "        # self.critic.save_weights(\"./save_model/\" + self.filename + \"_TF_critic\", save_format=\"tf\")\n",
    "        return"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def done_function(state):\r\n",
    "    return False\r\n",
    "\r\n",
    "def reward_function(state, action, next_state, done):\r\n",
    "    costh   = state[0]\r\n",
    "    sinth   = state[1]\r\n",
    "    th      = np.arctan2(sinth,costh)\r\n",
    "    thdot   = state[2]\r\n",
    "    u       = action[0]\r\n",
    "    costs   = th ** 2 + 0.1 * thdot + 0.001 * (u ** 2)\r\n",
    "    return -costs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = { \"ENV\":\"Pendulum-v0\",\\\n",
    "        \"RL\":\"DDPG\",\\\n",
    "        \"ER\":\"ER\",\\\n",
    "        # \"HER\":\\\n",
    "        #     {\n",
    "        #         \"REPLAY_N\":8,\\\n",
    "        #         \"STRATEGY\":\"FINAL\",\\\n",
    "        #         \"REWARD_FUNC\":reward_function,\\\n",
    "        #         \"DONE_FUNC\":done_function,\\\n",
    "        #     }\n",
    "        # }\n",
    "env_config = env_configs[cfg[\"ENV\"]]\n",
    "FILENAME = cfg[\"ENV\"] + '_' + cfg[\"RL\"] + '_' + cfg[\"ER\"]\n",
    "EPISODES = env_config[\"EPISODES\"]\n",
    "END_SCORE = env_config[\"END_SCORE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "%matplotlib tk\n",
    "\n",
    "figure = plt.gcf()\n",
    "figure.set_size_inches(8,6)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(cfg[\"ENV\"])\n",
    "    agent = DDPGAgent(env, cfg)\n",
    "    scores_avg, scores_raw, episodes, losses = [], [], [], []\n",
    "    critic_mean, actor_mean = [], []\n",
    "    score_avg = 0\n",
    "    end = False\n",
    "    show_media_info = True\n",
    "    goal = np.array([1.0,0.0,0.0])\n",
    "    \n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        critic_losses = []\n",
    "        actor_losses = []\n",
    "        while not done:\n",
    "            # env.render()\n",
    "\n",
    "            # Interact with env.\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done, goal)\n",
    "            critic_loss, actor_loss = agent.train_model()\n",
    "            state = next_state\n",
    "\n",
    "            score += reward\n",
    "            critic_losses.append(critic_loss)\n",
    "            actor_losses.append(actor_loss)\n",
    "            if show_media_info:\n",
    "                print(\"State Shape : \", np.shape(state),    type(state),    state)\n",
    "                print(\"Action Shape : \",np.shape(action),   type(action),   action)\n",
    "                print(\"Reward Shape : \",np.shape(reward),   type(reward),   reward)\n",
    "                print(\"Done Shape : \",  np.shape(done),     type(done),     done)\n",
    "                print(\"Goal Shape : \",  np.shape(goal),     type(goal),     goal)\n",
    "                show_media_info = False\n",
    "            if done:\n",
    "                score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\n",
    "                print(\"episode: {0:3d} | score avg: {1:3.2f} | mem size {2:6d} |\"\n",
    "                    .format(e, score_avg, len(agent.memory)))\n",
    "\n",
    "                episodes.append(e)\n",
    "                scores_avg.append(score_avg)\n",
    "                scores_raw.append(score)\n",
    "                critic_mean.append(np.mean(critic_losses))\n",
    "                actor_mean.append(np.mean(actor_losses))\n",
    "                # View data\n",
    "                plt.clf()\n",
    "                plt.subplot(311)\n",
    "                plt.plot(episodes, scores_avg, 'b')\n",
    "                plt.plot(episodes, scores_raw, 'b', alpha=0.8, linewidth=0.5)\n",
    "                plt.xlabel('episode'); plt.ylabel('average score'); plt.grid()\n",
    "                plt.title(cfg[\"ENV\"] +'_' + cfg[\"RL\"] +'_' + cfg[\"ER\"])\n",
    "                plt.subplot(312)\n",
    "                plt.plot(episodes, critic_mean, 'b.',markersize=3)\n",
    "                plt.xlabel('episode'); plt.ylabel('critic loss'); plt.grid()\n",
    "                plt.subplot(313)\n",
    "                plt.plot(episodes, actor_mean, 'b.',markersize=3)\n",
    "                plt.xlabel('episode'); plt.ylabel('actor loss'); plt.grid()\n",
    "                # plt.savefig(\"./result/\" + FILENAME + \"_TF.jpg\", dpi=100)\n",
    "\n",
    "                # 이동 평균이 0 이상일 때 종료\n",
    "                if score_avg > END_SCORE:\n",
    "                    agent.save_model()\n",
    "                    end = True\n",
    "                    break\n",
    "        if end == True:\n",
    "            env.close()\n",
    "            # np.save(\"./save_model/data/\" + FILENAME + \"_TF_epi\",  episodes)\n",
    "            # np.save(\"./save_model/data/\" + FILENAME + \"_TF_scores_avg\",scores_avg)\n",
    "            # np.save(\"./save_model/data/\" + FILENAME + \"_TF_scores_raw\",scores_raw)\n",
    "            # np.save(\"./save_model/data/\" + FILENAME + \"_TF_critic_mean\",critic_mean)\n",
    "            # np.save(\"./save_model/data/\" + FILENAME + \"_TF_actor_mean\",actor_mean)\n",
    "            print(\"End\")\n",
    "            break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Env Name :  Pendulum-v0\n",
      "States 3, Actions 1\n",
      "1 th Action space -2.00 ~ 2.00\n",
      "State Shape :  (3,) <class 'numpy.ndarray'> [ 0.9938495  -0.11073921  0.76229894]\n",
      "Action Shape :  (1,) <class 'numpy.ndarray'> [-0.0018271]\n",
      "Reward Shape :  () <class 'numpy.float64'> -0.09860783483182112\n",
      "Done Shape :  () <class 'bool'> False\n",
      "Goal Shape :  (3,) <class 'numpy.ndarray'> [1. 0. 0.]\n",
      "episode:   0 | score avg: -762.92 | mem size    400 |\n",
      "episode:   1 | score avg: -823.49 | mem size    800 |\n",
      "episode:   2 | score avg: -815.68 | mem size   1200 |\n",
      "episode:   3 | score avg: -919.77 | mem size   1600 |\n",
      "Start to train, check batch shapes\n",
      "shape of mini_batch (64, 6) <class 'list'>\n",
      "shape of states (64, 3) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "shape of actions (64, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "shape of rewards (64, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "shape of next_states (64, 3) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "shape of dones (64, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "shape of goals (64, 3) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "episode:   4 | score avg: -1009.25 | mem size   2000 |\n",
      "episode:   5 | score avg: -1047.77 | mem size   2400 |\n",
      "episode:   6 | score avg: -1122.40 | mem size   2800 |\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-a4e50b73341f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mcritic_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-2892ca01a527>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mactor_loss_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactor_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mactor_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m         \u001b[0mactor_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1085\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1086\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1087\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1088\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m_ReluGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRegisterGradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_ReluGrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_nn_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mrelu_grad\u001b[1;34m(gradients, features, name)\u001b[0m\n\u001b[0;32m  10647\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10648\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m> 10649\u001b[1;33m         _ctx, \"ReluGrad\", name, gradients, features)\n\u001b[0m\u001b[0;32m  10650\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10651\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env.close()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\tf241\\lib\\site-packages\\ipykernel\\eventloops.py:256: RuntimeWarning: coroutine 'Kernel.do_one_iteration' was never awaited\n",
      "  self.func()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}