{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('TF240': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8f237aa33f5a133d3a67a1e00bf3cb9d47b6c38bcc6ab493273f5d4df41b8866"
    }
   }
  },
  "interpreter": {
   "hash": "61683dc6b2a2d3d4f2fca4fc9c31d7600238da1c31c9bb494e8f77b62993b62b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Find RL_Note path and append sys path\r\n",
    "import os, sys\r\n",
    "cwd = os.getcwd()\r\n",
    "pos = cwd.find('RL_Note')\r\n",
    "root_path = cwd[0:pos] + 'RL_Note'\r\n",
    "sys.path.append(root_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Refer from\r\n",
    "#  https://pasus.tistory.com/138\r\n",
    "#  https://horomary.hatenablog.com/entry/2020/06/26/003806\r\n",
    "#  https://keras.io/examples/rl/ddpg_pendulum/\r\n",
    "#\r\n",
    "import gym\r\n",
    "import sys\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, concatenate, Lambda\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from pys.utils.memory import ReplayMemory"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class Actor(tf.keras.Model):\r\n",
    "    def __init__(self, state_size, action_size, action_min, action_max):\r\n",
    "        super(Actor, self).__init__()\r\n",
    "        self.action_min = action_min\r\n",
    "        self.action_max = action_max\r\n",
    "\r\n",
    "        self.fc1 = Dense(64, activation='relu')\r\n",
    "        self.fc2 = Dense(64, activation='relu')\r\n",
    "        # self.fc3 = Dense(16, activation='relu')\r\n",
    "        self.out= Dense(action_size, activation='tanh',kernel_initializer = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)) # -1 ~ +1\r\n",
    "\r\n",
    "    def call(self, x):\r\n",
    "        x       = self.fc1(x)\r\n",
    "        x       = self.fc2(x)\r\n",
    "        # x       = self.fc3(x)\r\n",
    "        action  = self.out(x)\r\n",
    "        # return self.projected_to_action_space(action)\r\n",
    "        a = Lambda(lambda x: x*self.action_max)(action)\r\n",
    "        return a\r\n",
    "\r\n",
    "class Critic(tf.keras.Model):\r\n",
    "    def __init__(self, state_size, action_size):\r\n",
    "        super(Critic, self).__init__()\r\n",
    "        self.s1 = Dense(16, activation='relu')\r\n",
    "        self.s2 = Dense(32, activation='relu')\r\n",
    "        self.a1 = Dense(32, activation='relu')\r\n",
    "        self.a2 = Dense(32, activation='relu')\r\n",
    "        self.fc1= Dense(64, activation='relu')\r\n",
    "        self.fc2= Dense(64, activation='relu')\r\n",
    "        self.out= Dense(1,  activation='linear')\r\n",
    "\r\n",
    "    def call(self,state_action):\r\n",
    "        state  = state_action[0]\r\n",
    "        action = state_action[1]\r\n",
    "        s = self.s1(state)\r\n",
    "        s = self.s2(s)\r\n",
    "        a = self.a1(action)\r\n",
    "        a = self.a2(a)\r\n",
    "        c = concatenate([s,a],axis=-1)\r\n",
    "        x = self.fc1(c)\r\n",
    "        x = self.fc2(x)\r\n",
    "        q = self.out(x)\r\n",
    "        return q"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class TD3Agent:\r\n",
    "    def __init__(self, state_size, action_size, action_min, action_max):\r\n",
    "        self.state_size = state_size\r\n",
    "        self.action_size= action_size\r\n",
    "        self.action_min = action_min\r\n",
    "        self.action_max = action_max\r\n",
    "\r\n",
    "        # Hyper params for learning\r\n",
    "        self.discount_factor = 0.99\r\n",
    "        self.actor_learning_rate  = 0.001\r\n",
    "        self.critic_learning_rate = 0.002\r\n",
    "        self.tau = 0.005\r\n",
    "\r\n",
    "        # Experience Replay\r\n",
    "        self.batch_size = 64\r\n",
    "        self.train_start = 2000\r\n",
    "        self.buffer_size = 50000\r\n",
    "        self.memory = ReplayMemory(capacity=self.buffer_size)\r\n",
    "        \r\n",
    "        self.critic1_optimizer   = tf.keras.optimizers.Adam(lr=self.critic_learning_rate)\r\n",
    "        self.critic2_optimizer   = tf.keras.optimizers.Adam(lr=self.critic_learning_rate)\r\n",
    "        self.actor_optimizer    = tf.keras.optimizers.Adam(lr=self.actor_learning_rate)\r\n",
    "\r\n",
    "        self.critic1        = Critic(self.state_size, self.action_size)\r\n",
    "        self.critic2        = Critic(self.state_size, self.action_size)\r\n",
    "        self.target_critic1 = Critic(self.state_size, self.action_size)\r\n",
    "        self.target_critic2 = Critic(self.state_size, self.action_size)\r\n",
    "        self.actor          = Actor(self.state_size, self.action_size, self.action_min, self.action_max)\r\n",
    "        self.target_actor   = Actor(self.state_size, self.action_size, self.action_min, self.action_max)\r\n",
    "\r\n",
    "        self.actor.build(input_shape=(None, self.state_size))\r\n",
    "        self.target_actor.build(input_shape=(None, self.state_size))\r\n",
    "        state_in = Input((self.state_size,))\r\n",
    "        action_in = Input((self.action_size,))\r\n",
    "        self.actor(state_in)\r\n",
    "        self.target_actor(state_in)\r\n",
    "        self.critic1([state_in, action_in])\r\n",
    "        self.critic2([state_in, action_in])\r\n",
    "        self.target_critic1([state_in, action_in])\r\n",
    "        self.target_critic2([state_in, action_in])\r\n",
    "\r\n",
    "        self.actor.summary()\r\n",
    "        self.critic1.summary()\r\n",
    "        self.critic2.summary()\r\n",
    "        \r\n",
    "        self.hard_update_target_model()\r\n",
    "\r\n",
    "        self.update_freq = 2\r\n",
    "        self.train_idx = 0\r\n",
    "        self.show_media_info = False\r\n",
    "        self.noise_std_dev = 0.2\r\n",
    "        self.noise_mean = 0.0\r\n",
    "\r\n",
    "    def remember(self, state, action, reward, next_state, done):\r\n",
    "        state       = np.array(state,dtype=np.float32)\r\n",
    "        action      = np.array(action,dtype=np.float32)\r\n",
    "        reward      = np.array([reward],dtype=np.float32)\r\n",
    "        done        = np.array([done],dtype=np.float32)\r\n",
    "        next_state  = np.array(next_state,dtype=np.float32)\r\n",
    "        transition  = (state, action, reward, next_state, done)\r\n",
    "        self.memory.append(transition)\r\n",
    "\r\n",
    "    def hard_update_target_model(self):\r\n",
    "        self.target_actor.set_weights(self.actor.get_weights())\r\n",
    "        self.target_critic1.set_weights(self.critic1.get_weights())\r\n",
    "        self.target_critic2.set_weights(self.critic2.get_weights())\r\n",
    "\r\n",
    "    def soft_update_target_model(self):\r\n",
    "        tau = self.tau\r\n",
    "        for (net, target_net) in zip(   self.actor.trainable_variables,\r\n",
    "                                        self.target_actor.trainable_variables):\r\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\r\n",
    "        for (net, target_net) in zip(   self.critic1.trainable_variables,\r\n",
    "                                        self.target_critic1.trainable_variables):\r\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\r\n",
    "        for (net, target_net) in zip(   self.critic2.trainable_variables,\r\n",
    "                                        self.target_critic2.trainable_variables):\r\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\r\n",
    "\r\n",
    "    def get_action(self,state):\r\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\r\n",
    "        action = self.actor(state)\r\n",
    "        noise = np.random.randn(self.action_size)*self.noise_std_dev + self.noise_mean\r\n",
    "        # Exploration and Exploitation\r\n",
    "        return np.clip(action.numpy()[0]+noise,self.action_min,self.action_max)\r\n",
    "\r\n",
    "    def train_model(self):\r\n",
    "        # Train from Experience Replay\r\n",
    "        # Training Condition - Memory Size\r\n",
    "        if len(self.memory) < self.train_start:\r\n",
    "            return 0.0, 0.0\r\n",
    "        # Sampling from the memory\r\n",
    "        # ER\r\n",
    "        # mini_batch = random.sample(self.memory, self.batch_size)\r\n",
    "        # PER\r\n",
    "        mini_batch, idxs, is_weights = self.memory.sample(self.batch_size)\r\n",
    "\r\n",
    "        states      = tf.convert_to_tensor(np.array([sample[0] for sample in mini_batch]))\r\n",
    "        actions     = tf.convert_to_tensor(np.array([sample[1] for sample in mini_batch]))\r\n",
    "        rewards     = tf.convert_to_tensor(np.array([sample[2] for sample in mini_batch]))\r\n",
    "        next_states = tf.convert_to_tensor(np.array([sample[3] for sample in mini_batch]))\r\n",
    "        dones       = tf.convert_to_tensor(np.array([sample[4] for sample in mini_batch]))\r\n",
    "        \r\n",
    "        if self.show_media_info == False:\r\n",
    "            self.show_media_info = True\r\n",
    "            print('Start to train, check batch shapes')\r\n",
    "            print('**** shape of states', np.shape(states),type(states))\r\n",
    "            print('**** shape of actions', np.shape(actions),type(actions))\r\n",
    "            print('**** shape of rewards', np.shape(rewards),type(rewards))\r\n",
    "            print('**** shape of next_states', np.shape(next_states),type(next_states))\r\n",
    "            print('**** shape of dones', np.shape(dones),type(dones))\r\n",
    "\r\n",
    "        target_actions = self.target_actor(next_states,training=True)\r\n",
    "        target_q1 = self.target_critic1([next_states,target_actions],training=True)\r\n",
    "        target_q2 = self.target_critic2([next_states,target_actions],training=True)\r\n",
    "        target_q_min = tf.minimum(target_q1,target_q2) # Clipping Double Q\r\n",
    "        target_value = rewards + (1.0 - dones) * self.discount_factor * target_q_min\r\n",
    "\r\n",
    "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\r\n",
    "            q1 = self.critic1([states, actions], training=True)\r\n",
    "            q2 = self.critic2([states, actions], training=True)\r\n",
    "            critic1_loss = tf.math.reduce_mean(tf.math.square(target_value - q1))\r\n",
    "            critic2_loss = tf.math.reduce_mean(tf.math.square(target_value - q2))\r\n",
    "        critic1_params = self.critic1.trainable_variables\r\n",
    "        critic2_params = self.critic2.trainable_variables\r\n",
    "        critic1_grads = tape1.gradient(critic1_loss, critic1_params)\r\n",
    "        critic2_grads = tape2.gradient(critic2_loss, critic2_params)\r\n",
    "        self.critic1_optimizer.apply_gradients(zip(critic1_grads, critic1_params))\r\n",
    "        self.critic2_optimizer.apply_gradients(zip(critic2_grads, critic2_params))\r\n",
    "\r\n",
    "        self.train_idx = self.train_idx + 1\r\n",
    "        if self.train_idx % self.update_freq == 0:\r\n",
    "            with tf.GradientTape() as tape:\r\n",
    "                new_actions = self.actor(states,training=True)\r\n",
    "                new_q = self.critic1([states, new_actions],training=True)\r\n",
    "                actor_loss = -tf.reduce_mean(new_q)\r\n",
    "            actor_params = self.actor.trainable_variables\r\n",
    "            actor_grads = tape.gradient(actor_loss, actor_params)\r\n",
    "            self.actor_optimizer.apply_gradients(zip(actor_grads, actor_params))\r\n",
    "            actor_loss_out = actor_loss.numpy()\r\n",
    "            self.soft_update_target_model()\r\n",
    "        else:\r\n",
    "            actor_loss_out = 0.0\r\n",
    "\r\n",
    "        critic_loss_out = 0.5 * (critic1_loss.numpy() + critic2_loss.numpy())\r\n",
    "        return critic_loss_out, actor_loss_out\r\n",
    "\r\n",
    "    def save_model(self):\r\n",
    "        self.actor.save_weights(\"./save_model/pendulum_td3_TF_actor\", save_format=\"tf\")\r\n",
    "        self.critic1.save_weights(\"./save_model/pendulum_td3_TF_critic1\", save_format=\"tf\")\r\n",
    "        self.critic2.save_weights(\"./save_model/pendulum_td3_TF_critic2\", save_format=\"tf\")\r\n",
    "        return\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "%matplotlib tk\n",
    "\n",
    "ENV_NAME = 'Pendulum-v0'\n",
    "EPISODES = 300\n",
    "END_SCORE = -200\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(ENV_NAME)\n",
    "    state_size  = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.shape[0]\n",
    "    action_min  = env.action_space.low[0]\n",
    "    action_max  = env.action_space.high[0]\n",
    "\n",
    "    agent = TD3Agent(state_size, action_size, action_min, action_max)\n",
    "    print('Env Name : ',ENV_NAME)\n",
    "    print('States {0}, Actions {1}'.format(state_size, action_size))\n",
    "    print('Action space {0:.2f} ~ {1:.2f}'.format(action_min, action_max))\n",
    "    scores_avg, scores_raw, episodes, losses = [], [], [], []\n",
    "    critic_mean, actor_mean = [], []\n",
    "    score_avg = 0\n",
    "\n",
    "    end = False\n",
    "    show_media_info = True\n",
    "    \n",
    "    fig = plt.figure(1)\n",
    "    fig.clf()\n",
    "    \n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        critic_losses = []\n",
    "        actor_losses = []\n",
    "        while not done:\n",
    "            # env.render()\n",
    "\n",
    "            # Interact with env.\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            critic_loss, actor_loss = agent.train_model()\n",
    "            state = next_state\n",
    "            \n",
    "            score += reward\n",
    "            critic_losses.append(critic_loss)\n",
    "            actor_losses.append(actor_loss)\n",
    "            if show_media_info:\n",
    "                print(\"State Shape : \", np.shape(state))\n",
    "                print(\"Action Shape : \", np.shape(action))\n",
    "                print(\"Reward Shape : \", np.shape(reward))\n",
    "                print(\"done Shape : \", np.shape(done))\n",
    "                show_media_info = False\n",
    "            if done:\n",
    "                score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\n",
    "                print(\"episode: {0:4d} | score avg: {1:8.2f} | mem size {2:6d} |\"\n",
    "                    .format(e, score_avg, len(agent.memory)))\n",
    "\n",
    "                episodes.append(e)\n",
    "                scores_avg.append(score_avg)\n",
    "                scores_raw.append(score)\n",
    "                critic_mean.append(np.mean(critic_losses))\n",
    "                actor_mean.append(np.mean(actor_losses))\n",
    "                # View data\n",
    "                plt.clf()\n",
    "                plt.subplot(311)\n",
    "                plt.plot(episodes, scores_avg, 'b')\n",
    "                plt.plot(episodes, scores_raw, 'b', alpha=0.8, linewidth=0.5)\n",
    "                plt.xlabel('episode'); plt.ylabel('average score'); plt.grid()\n",
    "                plt.title('pendulum TD3')\n",
    "                plt.subplot(312)\n",
    "                plt.plot(episodes, critic_mean, 'b.',markersize=3)\n",
    "                plt.xlabel('episode'); plt.ylabel('critic loss'); plt.grid()\n",
    "                plt.subplot(313)\n",
    "                plt.plot(episodes, actor_mean, 'b.',markersize=3)\n",
    "                plt.xlabel('episode'); plt.ylabel('actor loss'); plt.grid()\n",
    "                plt.savefig(\"./result/pendulum_td3_TF.png\")\n",
    "\n",
    "                # 이동 평균이 0 이상일 때 종료\n",
    "                if score_avg > END_SCORE:\n",
    "                    agent.save_model()\n",
    "                    end = True\n",
    "                    break\n",
    "        if end == True:\n",
    "            env.close()\n",
    "            np.save('./save_model/data/pendulum_td3_TF_epi',  episodes)\n",
    "            np.save('./save_model/data/pendulum_td3_TF_scores_avg',scores_avg)\n",
    "            np.save('./save_model/data/pendulum_td3_TF_scores_raw',scores_raw)\n",
    "            np.save('./save_model/data/pendulum_td3_TF_critic_mean',critic_mean)\n",
    "            np.save('./save_model/data/pendulum_td3_TF_actor_mean',actor_mean)\n",
    "            print(\"End\")\n",
    "            break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"actor\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             multiple                  256       \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             multiple                  65        \n",
      "=================================================================\n",
      "Total params: 4,481\n",
      "Trainable params: 4,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"critic\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  64        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  544       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  64        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  1056      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  65        \n",
      "=================================================================\n",
      "Total params: 10,113\n",
      "Trainable params: 10,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"critic_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              multiple                  64        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              multiple                  544       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              multiple                  64        \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             multiple                  1056      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             multiple                  65        \n",
      "=================================================================\n",
      "Total params: 10,113\n",
      "Trainable params: 10,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Env Name :  Pendulum-v0\n",
      "States 3, Actions 1\n",
      "Action space -2.00 ~ 2.00\n",
      "State Shape :  (3,)\n",
      "Action Shape :  (1,)\n",
      "Reward Shape :  ()\n",
      "done Shape :  ()\n",
      "episode:    0 | score avg: -1027.93 | mem size    200 |\n",
      "episode:    1 | score avg: -1030.33 | mem size    400 |\n",
      "episode:    2 | score avg:  -977.76 | mem size    600 |\n",
      "episode:    3 | score avg: -1048.20 | mem size    800 |\n",
      "episode:    4 | score avg: -1086.54 | mem size   1000 |\n",
      "episode:    5 | score avg: -1109.36 | mem size   1200 |\n",
      "episode:    6 | score avg: -1114.83 | mem size   1400 |\n",
      "episode:    7 | score avg: -1143.62 | mem size   1600 |\n",
      "episode:    8 | score avg: -1103.91 | mem size   1800 |\n",
      "Start to train, check batch shapes\n",
      "**** shape of states (64, 3) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of actions (64, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of rewards (64, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of next_states (64, 3) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of dones (64, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "episode:    9 | score avg: -1181.64 | mem size   2000 |\n",
      "episode:   10 | score avg: -1135.89 | mem size   2200 |\n",
      "episode:   11 | score avg: -1153.92 | mem size   2400 |\n",
      "episode:   12 | score avg: -1208.42 | mem size   2600 |\n",
      "episode:   13 | score avg: -1233.79 | mem size   2800 |\n",
      "episode:   14 | score avg: -1279.12 | mem size   3000 |\n",
      "episode:   15 | score avg: -1276.18 | mem size   3200 |\n",
      "episode:   16 | score avg: -1250.85 | mem size   3400 |\n",
      "episode:   17 | score avg: -1298.91 | mem size   3600 |\n",
      "episode:   18 | score avg: -1326.13 | mem size   3800 |\n",
      "episode:   19 | score avg: -1345.08 | mem size   4000 |\n",
      "episode:   20 | score avg: -1387.26 | mem size   4200 |\n",
      "episode:   21 | score avg: -1409.11 | mem size   4400 |\n",
      "episode:   22 | score avg: -1435.15 | mem size   4600 |\n",
      "episode:   23 | score avg: -1422.01 | mem size   4800 |\n",
      "episode:   24 | score avg: -1446.91 | mem size   5000 |\n",
      "episode:   25 | score avg: -1455.68 | mem size   5200 |\n",
      "episode:   26 | score avg: -1480.71 | mem size   5400 |\n",
      "episode:   27 | score avg: -1516.78 | mem size   5600 |\n",
      "episode:   28 | score avg: -1369.88 | mem size   5800 |\n",
      "episode:   29 | score avg: -1361.86 | mem size   6000 |\n",
      "episode:   30 | score avg: -1341.77 | mem size   6200 |\n",
      "episode:   31 | score avg: -1339.73 | mem size   6400 |\n",
      "episode:   32 | score avg: -1257.53 | mem size   6600 |\n",
      "episode:   33 | score avg: -1309.34 | mem size   6800 |\n",
      "episode:   34 | score avg: -1362.66 | mem size   7000 |\n",
      "episode:   35 | score avg: -1338.75 | mem size   7200 |\n",
      "episode:   36 | score avg: -1299.89 | mem size   7400 |\n",
      "episode:   37 | score avg: -1305.89 | mem size   7600 |\n",
      "episode:   38 | score avg: -1287.99 | mem size   7800 |\n",
      "episode:   39 | score avg: -1303.94 | mem size   8000 |\n",
      "episode:   40 | score avg: -1334.59 | mem size   8200 |\n",
      "episode:   41 | score avg: -1215.68 | mem size   8400 |\n",
      "episode:   42 | score avg: -1175.88 | mem size   8600 |\n",
      "episode:   43 | score avg: -1158.15 | mem size   8800 |\n",
      "episode:   44 | score avg: -1094.84 | mem size   9000 |\n",
      "episode:   45 | score avg:  -986.43 | mem size   9200 |\n",
      "episode:   46 | score avg:  -888.72 | mem size   9400 |\n",
      "episode:   47 | score avg:  -813.08 | mem size   9600 |\n",
      "episode:   48 | score avg:  -744.83 | mem size   9800 |\n",
      "episode:   49 | score avg:  -727.67 | mem size  10000 |\n",
      "episode:   50 | score avg:  -680.56 | mem size  10200 |\n",
      "episode:   51 | score avg:  -638.47 | mem size  10400 |\n",
      "episode:   52 | score avg:  -587.49 | mem size  10600 |\n",
      "episode:   53 | score avg:  -541.67 | mem size  10800 |\n",
      "episode:   54 | score avg:  -539.05 | mem size  11000 |\n",
      "episode:   55 | score avg:  -588.05 | mem size  11200 |\n",
      "episode:   56 | score avg:  -585.34 | mem size  11400 |\n",
      "episode:   57 | score avg:  -602.51 | mem size  11600 |\n",
      "episode:   58 | score avg:  -554.89 | mem size  11800 |\n",
      "episode:   59 | score avg:  -524.37 | mem size  12000 |\n",
      "episode:   60 | score avg:  -484.77 | mem size  12200 |\n",
      "episode:   61 | score avg:  -461.87 | mem size  12400 |\n",
      "episode:   62 | score avg:  -480.52 | mem size  12600 |\n",
      "episode:   63 | score avg:  -445.23 | mem size  12800 |\n",
      "episode:   64 | score avg:  -426.55 | mem size  13000 |\n",
      "episode:   65 | score avg:  -396.49 | mem size  13200 |\n",
      "episode:   66 | score avg:  -369.53 | mem size  13400 |\n",
      "episode:   67 | score avg:  -357.55 | mem size  13600 |\n",
      "episode:   68 | score avg:  -334.42 | mem size  13800 |\n",
      "episode:   69 | score avg:  -325.93 | mem size  14000 |\n",
      "episode:   70 | score avg:  -305.92 | mem size  14200 |\n",
      "episode:   71 | score avg:  -275.56 | mem size  14400 |\n",
      "episode:   72 | score avg:  -261.09 | mem size  14600 |\n",
      "episode:   73 | score avg:  -247.83 | mem size  14800 |\n",
      "episode:   74 | score avg:  -223.46 | mem size  15000 |\n",
      "episode:   75 | score avg:  -213.78 | mem size  15200 |\n",
      "episode:   76 | score avg:  -205.10 | mem size  15400 |\n",
      "episode:   77 | score avg:  -210.70 | mem size  15600 |\n",
      "episode:   78 | score avg:  -213.26 | mem size  15800 |\n",
      "episode:   79 | score avg:  -228.42 | mem size  16000 |\n",
      "episode:   80 | score avg:  -218.09 | mem size  16200 |\n",
      "episode:   81 | score avg:  -301.74 | mem size  16400 |\n",
      "episode:   82 | score avg:  -296.85 | mem size  16600 |\n",
      "episode:   83 | score avg:  -305.45 | mem size  16800 |\n",
      "episode:   84 | score avg:  -288.07 | mem size  17000 |\n",
      "episode:   85 | score avg:  -272.16 | mem size  17200 |\n",
      "episode:   86 | score avg:  -257.72 | mem size  17400 |\n",
      "episode:   87 | score avg:  -257.55 | mem size  17600 |\n",
      "episode:   88 | score avg:  -269.33 | mem size  17800 |\n",
      "episode:   89 | score avg:  -280.34 | mem size  18000 |\n",
      "episode:   90 | score avg:  -404.02 | mem size  18200 |\n",
      "episode:   91 | score avg:  -376.60 | mem size  18400 |\n",
      "episode:   92 | score avg:  -351.77 | mem size  18600 |\n",
      "episode:   93 | score avg:  -329.51 | mem size  18800 |\n",
      "episode:   94 | score avg:  -309.25 | mem size  19000 |\n",
      "episode:   95 | score avg:  -332.12 | mem size  19200 |\n",
      "episode:   96 | score avg:  -310.88 | mem size  19400 |\n",
      "episode:   97 | score avg:  -304.77 | mem size  19600 |\n",
      "episode:   98 | score avg:  -287.04 | mem size  19800 |\n",
      "episode:   99 | score avg:  -295.79 | mem size  20000 |\n",
      "episode:  100 | score avg:  -278.51 | mem size  20200 |\n",
      "episode:  101 | score avg:  -262.90 | mem size  20400 |\n",
      "episode:  102 | score avg:  -249.56 | mem size  20600 |\n",
      "episode:  103 | score avg:  -237.39 | mem size  20800 |\n",
      "episode:  104 | score avg:  -213.67 | mem size  21000 |\n",
      "episode:  105 | score avg:  -204.61 | mem size  21200 |\n",
      "episode:  106 | score avg:  -208.96 | mem size  21400 |\n",
      "episode:  107 | score avg:  -212.39 | mem size  21600 |\n",
      "episode:  108 | score avg:  -203.44 | mem size  21800 |\n",
      "episode:  109 | score avg:  -195.44 | mem size  22000 |\n",
      "End\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "env.close()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}