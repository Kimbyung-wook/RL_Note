{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('tf240': conda)"
  },
  "interpreter": {
   "hash": "61683dc6b2a2d3d4f2fca4fc9c31d7600238da1c31c9bb494e8f77b62993b62b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Refer from\n",
    "#  https://pasus.tistory.com/138\n",
    "#  https://horomary.hatenablog.com/entry/2020/06/26/003806\n",
    "#  https://keras.io/examples/rl/ddpg_pendulum/\n",
    "#  https://github.com/dongminlee94/Samsung-DRL-Code/blob/master/5_SAC/sac/model.py\n",
    "# ! pip \n",
    "import gym\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, concatenate, Lambda\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, state_size, action_size, log_std_min, log_std_max):\n",
    "        super(Actor, self).__init__()\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "\n",
    "        self.fc1= Dense(64, activation='relu')\n",
    "        self.fc2= Dense(64, activation='relu')\n",
    "        # self.fc3 = Dense(16, activation='relu')\n",
    "        self.mu = Dense(action_size)\n",
    "        self.log_std= Dense(action_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        x       = self.fc1(x)\n",
    "        x       = self.fc2(x)\n",
    "        # x       = self.fc3(x)\n",
    "        mu = self.mu(x)\n",
    "        log_std = self.log_std(x)\n",
    "        log_std = tf.clip_by_value(log_std, self.log_std_min, self.log_std_max)\n",
    "        std = tf.math.exp(log_std)\n",
    "        return mu, std\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.s1 = Dense(16, activation='relu')\n",
    "        self.s2 = Dense(32, activation='relu')\n",
    "        self.a1 = Dense(32, activation='relu')\n",
    "        self.a2 = Dense(32, activation='relu')\n",
    "        self.fc1= Dense(64, activation='relu')\n",
    "        self.fc2= Dense(64, activation='relu')\n",
    "        self.out= Dense(1,  activation='linear')\n",
    "\n",
    "    def call(self,state,action):\n",
    "        # state  = state_action[0]\n",
    "        # action = state_action[1]\n",
    "        s = self.s1(state)\n",
    "        s = self.s2(s)\n",
    "        a = self.a1(action)\n",
    "        a = self.a2(a)\n",
    "        c = concatenate([s,a],axis=-1)\n",
    "        x = self.fc1(c)\n",
    "        x = self.fc2(x)\n",
    "        q = self.out(x)\n",
    "        return q"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# https://github.com/dongminlee94/Samsung-DRL-Code/blob/master/5_SAC/sac/utils.py\n",
    "# https://github.com/RickyMexx/SAC-tf2/blob/master/SAC/SAC_rla.py\n",
    "# https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch/blob/b338c87bebb672e39304e47e0eed55aeb462b243/agents/Base_Agent.py#L278\n",
    "# \n",
    "class SACAgent:\n",
    "    def __init__(self, state_size, action_size, action_min, action_max):\n",
    "        self.state_size = state_size\n",
    "        self.action_size= action_size\n",
    "        self.action_min = action_min\n",
    "        self.action_max = action_max\n",
    "\n",
    "        self.actor          = Actor(self.state_size, self.action_size, self.action_min, self.action_max)\n",
    "        self.actor.build(input_shape=(None, self.state_size))\n",
    "        state_in = Input(shape=(self.state_size,),dtype=tf.float32)\n",
    "        self.actor(state_in)\n",
    "        self.actor.summary()\n",
    "        self.load_model()\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        mu, std = self.actor(state)\n",
    "        action = mu\n",
    "        return action[0]\n",
    "\n",
    "    def eval_action(self, mu, std, epsilon=1e-6):\n",
    "        action_prob = tfd.Normal(loc=mu, scale=std)\n",
    "        z = action_prob.sample()\n",
    "        action = tf.math.tanh(z)\n",
    "        # action = tf.stop_gradient(action)\n",
    "        log_prob = action_prob.log_prob(z) - tf.math.log(1.0 - tf.pow(action,2) + epsilon)\n",
    "        log_prob = tf.reduce_sum(log_prob, axis=-1, keepdims=True)\n",
    "        return action, log_prob\n",
    "\n",
    "    def load_model(self):\n",
    "        self.actor.load_weights(\"./save_model/pendulum_sac_TF_actor\")\n",
    "        return\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib tk\n",
    "\n",
    "ENV_NAME = 'Pendulum-v0'\n",
    "EPISODES = 5\n",
    "# END_SCORE = -200\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(ENV_NAME)\n",
    "    state_size      = env.observation_space.shape[0]\n",
    "    action_size     = env.action_space.shape[0]\n",
    "    log_std_min     = -20.0\n",
    "    log_std_max     = 5.0\n",
    "\n",
    "    agent = SACAgent(state_size, action_size, log_std_min, log_std_max)\n",
    "    print('Env Name : ',ENV_NAME)\n",
    "    print('States {0}, Actions {1}'.format(state_size, action_size))\n",
    "    print('Action scale exp({0:.2f} ~ {1:.2f})'.format(log_std_min, log_std_max))\n",
    "    \n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        critic_losses = []\n",
    "        actor_losses = []\n",
    "        while not done:\n",
    "            env.render()\n",
    "            # Interact with env.\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            state = next_state\n",
    "            # \n",
    "            score += reward\n",
    "            if done:\n",
    "                # score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\n",
    "                print(\"episode: {0:3d} | score: {1:3.2f} |\".format(e, score))"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env.close()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}