{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('tf240': conda)"
  },
  "interpreter": {
   "hash": "61683dc6b2a2d3d4f2fca4fc9c31d7600238da1c31c9bb494e8f77b62993b62b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Refer from\n",
    "#  https://pasus.tistory.com/138\n",
    "#  https://horomary.hatenablog.com/entry/2020/06/26/003806\n",
    "#  https://keras.io/examples/rl/ddpg_pendulum/\n",
    "#  https://github.com/dongminlee94/Samsung-DRL-Code/blob/master/5_SAC/sac/model.py\n",
    "# ! pip \n",
    "import gym\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, concatenate, Lambda\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-02 12:59:40.669741: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, state_size, action_size, log_std_min, log_std_max):\n",
    "        super(Actor, self).__init__()\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "\n",
    "        self.fc1= Dense(64, activation='relu')\n",
    "        self.fc2= Dense(64, activation='relu')\n",
    "        # self.fc3 = Dense(16, activation='relu')\n",
    "        self.mu = Dense(action_size)\n",
    "        self.log_std= Dense(action_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        x       = self.fc1(x)\n",
    "        x       = self.fc2(x)\n",
    "        # x       = self.fc3(x)\n",
    "        mu = self.mu(x)\n",
    "        log_std = self.log_std(x)\n",
    "        log_std = tf.clip_by_value(log_std, self.log_std_min, self.log_std_max)\n",
    "        std = tf.math.exp(log_std)\n",
    "        return mu, std\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.s1 = Dense(16, activation='relu')\n",
    "        self.s2 = Dense(32, activation='relu')\n",
    "        self.a1 = Dense(32, activation='relu')\n",
    "        self.a2 = Dense(32, activation='relu')\n",
    "        self.fc1= Dense(64, activation='relu')\n",
    "        self.fc2= Dense(64, activation='relu')\n",
    "        self.out= Dense(1,  activation='linear')\n",
    "\n",
    "    def call(self,state,action):\n",
    "        # state  = state_action[0]\n",
    "        # action = state_action[1]\n",
    "        s = self.s1(state)\n",
    "        s = self.s2(s)\n",
    "        a = self.a1(action)\n",
    "        a = self.a2(a)\n",
    "        c = concatenate([s,a],axis=-1)\n",
    "        x = self.fc1(c)\n",
    "        x = self.fc2(x)\n",
    "        q = self.out(x)\n",
    "        return q"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# https://github.com/dongminlee94/Samsung-DRL-Code/blob/master/5_SAC/sac/utils.py\n",
    "# https://github.com/RickyMexx/SAC-tf2/blob/master/SAC/SAC_rla.py\n",
    "# https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch/blob/b338c87bebb672e39304e47e0eed55aeb462b243/agents/Base_Agent.py#L278\n",
    "# \n",
    "class TD3Agent:\n",
    "    def __init__(self, state_size, action_size, action_min, action_max):\n",
    "        self.state_size = state_size\n",
    "        self.action_size= action_size\n",
    "        self.action_min = action_min\n",
    "        self.action_max = action_max\n",
    "\n",
    "        # Hyper params for learning\n",
    "        self.discount_factor = 0.99\n",
    "        self.critic_learning_rate = 0.002\n",
    "        self.actor_learning_rate  = 0.001\n",
    "        self.alpha_learning_rate  = 0.001\n",
    "        self.tau = 0.005\n",
    "        self.alpha = 0.200 # temperature\n",
    "\n",
    "        # Experience Replay\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 2000\n",
    "        self.memory = deque(maxlen=50000)\n",
    "\n",
    "        self.critic1_optimizer  = tf.keras.optimizers.Adam(learning_rate=self.critic_learning_rate)\n",
    "        self.critic2_optimizer  = tf.keras.optimizers.Adam(learning_rate=self.critic_learning_rate)\n",
    "        self.actor_optimizer    = tf.keras.optimizers.Adam(learning_rate=self.actor_learning_rate)\n",
    "        self.alpha_optimizer    = tf.keras.optimizers.Adam(learning_rate=self.alpha_learning_rate)\n",
    "\n",
    "        self.critic1        = Critic(self.state_size, self.action_size)\n",
    "        self.critic2        = Critic(self.state_size, self.action_size)\n",
    "        self.target_critic1 = Critic(self.state_size, self.action_size)\n",
    "        self.target_critic2 = Critic(self.state_size, self.action_size)\n",
    "        self.actor          = Actor(self.state_size, self.action_size, self.action_min, self.action_max)\n",
    "        self.target_actor   = Actor(self.state_size, self.action_size, self.action_min, self.action_max)\n",
    "        self.log_alpha      = tf.math.log(0.2)\n",
    "        self.alpha          = tf.math.exp(self.log_alpha)\n",
    "\n",
    "        self.actor.build(input_shape=(None, self.state_size))\n",
    "        self.target_actor.build(input_shape=(None, self.state_size))\n",
    "        state_in = Input(shape=(self.state_size,),dtype=tf.float32)\n",
    "        action_in = Input(shape=(self.action_size,),dtype=tf.float32)\n",
    "        self.actor(state_in)\n",
    "        self.target_actor(state_in)\n",
    "        self.critic1(state_in, action_in)\n",
    "        self.critic2(state_in, action_in)\n",
    "        self.target_critic1(state_in, action_in)\n",
    "        self.target_critic2(state_in, action_in)\n",
    "\n",
    "        self.actor.summary()\n",
    "        self.critic1.summary()\n",
    "        # self.critic2.summary()\n",
    "        \n",
    "        self.hard_update_target_model()\n",
    "        self.target_entropy = -tf.convert_to_tensor(np.array(self.action_size,dtype=np.float32),dtype=tf.float32)\n",
    "\n",
    "        self.train_idx = 0\n",
    "        self.show_media_info = False\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def hard_update_target_model(self):\n",
    "        self.target_actor.set_weights(self.actor.get_weights())\n",
    "        self.target_critic1.set_weights(self.critic1.get_weights())\n",
    "        self.target_critic2.set_weights(self.critic2.get_weights())\n",
    "\n",
    "    def soft_update_target_model(self):\n",
    "        tau = self.tau\n",
    "        for (net, target_net) in zip(   self.actor.trainable_variables,\n",
    "                                        self.target_actor.trainable_variables):\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\n",
    "        for (net, target_net) in zip(   self.critic1.trainable_variables,\n",
    "                                        self.target_critic1.trainable_variables):\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\n",
    "        for (net, target_net) in zip(   self.critic2.trainable_variables,\n",
    "                                        self.target_critic2.trainable_variables):\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        mu, std = self.actor(state)\n",
    "        action, _ = self.eval_action(mu,std)\n",
    "        return action.numpy()[0]\n",
    "\n",
    "    def eval_action(self, mu, std, epsilon=1e-6):\n",
    "        action_prob = tfd.Normal(loc=mu, scale=std)\n",
    "        z = action_prob.sample()\n",
    "        action = tf.math.tanh(z)\n",
    "        # action = tf.stop_gradient(action)\n",
    "        log_prob = action_prob.log_prob(z) - tf.math.log(1.0 - tf.pow(action,2) + epsilon)\n",
    "        log_prob = tf.reduce_sum(log_prob, axis=-1, keepdims=True)\n",
    "\n",
    "        return action, log_prob\n",
    "\n",
    "    # def get_td_error(self,critic, target_critic):\n",
    "    #     # Update critic\n",
    "    #     mu, std = self.actor(next_states,training=True)\n",
    "    #     q = self.target_critic1(next_states,next_actions,training=True)\n",
    "    #     next_actions, log_pi = self.eval_action(mu, std)\n",
    "    #     target_q1 = self.target_critic1(next_states,next_actions,training=True)\n",
    "    #     target_q2 = self.target_critic2(next_states,next_actions,training=True)\n",
    "    #     target_q_min = tf.minimum(target_q1, target_q2) # Clipping Double Q\n",
    "    #     td_error = \n",
    "\n",
    "    def train_model(self):\n",
    "        # Train from Experience Replay\n",
    "        # Training Condition - Memory Size\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return 0.0,0.0\n",
    "        # Sampling from the memory\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        states      = tf.convert_to_tensor(np.array([sample[0] for sample in mini_batch]))\n",
    "        actions     = tf.convert_to_tensor(np.array([sample[1] for sample in mini_batch]))\n",
    "        rewards     = tf.convert_to_tensor(np.array([sample[2] for sample in mini_batch]),dtype=tf.float32)\n",
    "        rewards     = tf.expand_dims(rewards, axis = 1)\n",
    "        next_states = tf.convert_to_tensor(np.array([sample[3] for sample in mini_batch]))\n",
    "        dones       = tf.convert_to_tensor(np.array([sample[4] for sample in mini_batch]),dtype=tf.float32)\n",
    "        dones       = tf.expand_dims(dones, axis = 1)\n",
    "        \n",
    "        if self.show_media_info == False:\n",
    "            self.show_media_info = True\n",
    "            print('Start to train, check batch shapes')\n",
    "            print('**** shape of states', np.shape(states),type(states))\n",
    "            print('**** shape of actions', np.shape(actions),type(actions))\n",
    "            print('**** shape of rewards', np.shape(rewards),type(rewards))\n",
    "            print('**** shape of next_states', np.shape(next_states),type(next_states))\n",
    "            print('**** shape of dones', np.shape(dones),type(dones))\n",
    "        \n",
    "        # Update critic\n",
    "        mu, std = self.actor(next_states,training=True)\n",
    "        next_actions, next_log_pi = self.eval_action(mu, std)\n",
    "        target_q1 = self.target_critic1(next_states,next_actions,training=True)\n",
    "        target_q2 = self.target_critic2(next_states,next_actions,training=True)\n",
    "        target_q_min = tf.minimum(target_q1, target_q2) # Clipping Double Q\n",
    "        target_value = rewards + (1.0 - dones) * self.discount_factor * (target_q_min - self.alpha * next_log_pi)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            q1 = self.critic1(states, actions, training=True)\n",
    "            critic1_loss = tf.math.reduce_mean(tf.math.square(target_value - q1))\n",
    "        critic1_params = self.critic1.trainable_variables\n",
    "        critic1_grads = tape.gradient(critic1_loss, critic1_params)\n",
    "        self.critic1_optimizer.apply_gradients(zip(critic1_grads, critic1_params))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            q2 = self.critic2(states, actions, training=True)\n",
    "            critic2_loss = tf.math.reduce_mean(tf.math.square(target_value - q2))\n",
    "        critic2_params = self.critic2.trainable_variables\n",
    "        critic2_grads = tape.gradient(critic2_loss, critic2_params)\n",
    "        self.critic2_optimizer.apply_gradients(zip(critic2_grads, critic2_params))\n",
    "\n",
    "        # Update actor\n",
    "        with tf.GradientTape() as tape:\n",
    "            mu, std = self.actor(states,training=True)\n",
    "            new_actions, new_log_pi = self.eval_action(mu,std)\n",
    "            new_q1 = self.critic1(states, new_actions,training=True)\n",
    "            new_q2 = self.critic2(states, new_actions,training=True)\n",
    "            new_q_min = tf.minimum(new_q1, new_q2)\n",
    "            actor_loss = tf.reduce_mean(self.alpha * new_log_pi - new_q_min)\n",
    "        actor_params = self.actor.trainable_variables\n",
    "        actor_grads = tape.gradient(actor_loss, actor_params)\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, actor_params))\n",
    "\n",
    "        # Update alpha\n",
    "        # with tf.GradientTape() as tape:\n",
    "        #     alpha_loss = - tf.reduce_mean(self.log_alpha * (new_log_pi + self.target_entropy))\n",
    "        # alpha_params = self.log_alpha\n",
    "        # alpha_grads = tape.gradient(alpha_loss, alpha_params)\n",
    "        # self.alpha_optimizer.apply_gradients(zip(alpha_grads, alpha_params))\n",
    "        # self.alpha = tf.math.exp(self.log_alpha)\n",
    "\n",
    "        self.soft_update_target_model()\n",
    "        critic_loss = 0.5*(critic1_loss.numpy() + critic2_loss.numpy())\n",
    "        return critic_loss, actor_loss.numpy()\n",
    "        # return\n",
    "\n",
    "    def save_model(self):\n",
    "        self.actor.save_weights(\"./save_model/pendulum_sac_TF_actor\", save_format=\"tf\")\n",
    "        self.critic1.save_weights(\"./save_model/pendulum_sac_TF_critic1\", save_format=\"tf\")\n",
    "        self.critic2.save_weights(\"./save_model/pendulum_sac_TF_critic2\", save_format=\"tf\")\n",
    "        return\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "%matplotlib tk\n",
    "\n",
    "ENV_NAME = 'Pendulum-v0'\n",
    "EPISODES = 500\n",
    "END_SCORE = -200\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(ENV_NAME)\n",
    "    state_size      = env.observation_space.shape[0]\n",
    "    action_size     = env.action_space.shape[0]\n",
    "    log_std_min     = -20.0\n",
    "    log_std_max     = 5.0\n",
    "\n",
    "    agent = TD3Agent(state_size, action_size, log_std_min, log_std_max)\n",
    "    print('Env Name : ',ENV_NAME)\n",
    "    print('States {0}, Actions {1}'.format(state_size, action_size))\n",
    "    print('Action scale exp({0:.2f} ~ {1:.2f})'.format(log_std_min, log_std_max))\n",
    "    scores_avg, scores_raw, episodes, losses = [], [], [], []\n",
    "    critic_mean, actor_mean = [], []\n",
    "    score_avg = 0\n",
    "    critics = 0\n",
    "    actors = 0\n",
    "\n",
    "    end = False\n",
    "    show_media_info = True\n",
    "    \n",
    "    fig = plt.figure(1)\n",
    "    fig.clf()\n",
    "    \n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        critic_losses = []\n",
    "        actor_losses = []\n",
    "        while not done:\n",
    "            # if e%100 == 0:\n",
    "            #     env.render()\n",
    "            # Interact with env.\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            critic_loss, actor_loss = agent.train_model()\n",
    "            state = next_state\n",
    "            # \n",
    "            score += reward\n",
    "            critic_losses.append(critic_loss)\n",
    "            actor_losses.append(actor_loss)\n",
    "            if show_media_info:\n",
    "                print(\"-------------- Variable shapes --------------\")\n",
    "                print(\"State Shape : \", np.shape(state))\n",
    "                print(\"Action Shape : \", np.shape(action))\n",
    "                print(\"Reward Shape : \", np.shape(reward))\n",
    "                print(\"done Shape : \", np.shape(done))\n",
    "                print(\"---------------------------------------------\")\n",
    "                show_media_info = False\n",
    "            if done:\n",
    "                score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\n",
    "                print(\"episode: {0:3d} | score avg: {1:3.2f} | mem size {2:6d} |\"\n",
    "                    .format(e, score_avg, len(agent.memory)))\n",
    "\n",
    "                episodes.append(e)\n",
    "                scores_avg.append(score_avg)\n",
    "                scores_raw.append(score)\n",
    "\n",
    "                critic_mean.append(np.mean(critic_losses))\n",
    "                actor_mean.append(np.mean(actor_losses))\n",
    "                plt.subplot(311)\n",
    "                plt.plot(episodes, scores_avg, 'b')\n",
    "                plt.xlabel('episode'); plt.ylabel('average score'); plt.grid()\n",
    "                plt.title('pendulum SAC')\n",
    "                plt.subplot(312)\n",
    "                plt.plot(episodes, critic_mean, 'b.')\n",
    "                plt.xlabel('episode'); plt.ylabel('critic loss'); plt.grid()\n",
    "                plt.subplot(313)\n",
    "                plt.plot(episodes, actor_mean, 'b.')\n",
    "                plt.xlabel('episode'); plt.ylabel('actor loss'); plt.grid()\n",
    "                plt.savefig(\"./result/pendulum_sac_TF.png\")\n",
    "\n",
    "\n",
    "                # 이동 평균이 0 이상일 때 종료\n",
    "                if score_avg > END_SCORE:\n",
    "                    agent.save_model()\n",
    "                    end = True\n",
    "                    break\n",
    "                \n",
    "            # break\n",
    "        if end == True:\n",
    "            env.close()\n",
    "            np.save('./save_model/data/pendulum_sac_TF_epi',  episodes)\n",
    "            np.save('./save_model/data/pendulum_sac_TF_scores_avg',scores_avg)\n",
    "            np.save('./save_model/data/pendulum_sac_TF_scores_raw',scores_raw)\n",
    "            print(\"End\")\n",
    "            break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-08-02 12:59:49.128640: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-08-02 12:59:49.148624: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-08-02 12:59:49.242676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:65:00.0 name: GeForce RTX 3080 computeCapability: 8.6\n",
      "coreClock: 1.74GHz coreCount: 68 deviceMemorySize: 9.75GiB deviceMemoryBandwidth: 707.88GiB/s\n",
      "2021-08-02 12:59:49.242755: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2021-08-02 12:59:49.607649: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2021-08-02 12:59:49.607797: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-08-02 12:59:49.667626: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-08-02 12:59:49.695656: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-08-02 12:59:49.696216: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\n",
      "2021-08-02 12:59:49.741372: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2021-08-02 12:59:49.749221: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-08-02 12:59:49.749268: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-08-02 12:59:49.749976: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-08-02 12:59:49.751806: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-08-02 12:59:49.751852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-08-02 12:59:49.751862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"actor\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             multiple                  256       \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             multiple                  65        \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             multiple                  65        \n",
      "=================================================================\n",
      "Total params: 4,546\n",
      "Trainable params: 4,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"critic\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  64        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  544       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  64        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  1056      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  65        \n",
      "=================================================================\n",
      "Total params: 10,113\n",
      "Trainable params: 10,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Env Name :  Pendulum-v0\n",
      "States 3, Actions 1\n",
      "Action scale exp(-20.00 ~ 5.00)\n",
      "-------------- Variable shapes --------------\n",
      "State Shape :  (3,)\n",
      "Action Shape :  (1,)\n",
      "Reward Shape :  ()\n",
      "done Shape :  ()\n",
      "---------------------------------------------\n",
      "episode:   0 | score avg: -1302.10 | mem size    200 |\n",
      "episode:   1 | score avg: -1303.54 | mem size    400 |\n",
      "episode:   2 | score avg: -1303.14 | mem size    600 |\n",
      "episode:   3 | score avg: -1259.36 | mem size    800 |\n",
      "episode:   4 | score avg: -1216.80 | mem size   1000 |\n",
      "episode:   5 | score avg: -1254.23 | mem size   1200 |\n",
      "episode:   6 | score avg: -1246.37 | mem size   1400 |\n",
      "episode:   7 | score avg: -1239.19 | mem size   1600 |\n",
      "episode:   8 | score avg: -1305.25 | mem size   1800 |\n",
      "Start to train, check batch shapes\n",
      "**** shape of states (64, 3) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of actions (64, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of rewards (64, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of next_states (64, 3) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of dones (64, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "episode:   9 | score avg: -1265.18 | mem size   2000 |\n",
      "episode:  10 | score avg: -1213.82 | mem size   2200 |\n",
      "episode:  11 | score avg: -1200.37 | mem size   2400 |\n",
      "episode:  12 | score avg: -1205.34 | mem size   2600 |\n",
      "episode:  13 | score avg: -1254.47 | mem size   2800 |\n",
      "episode:  14 | score avg: -1318.16 | mem size   3000 |\n",
      "episode:  15 | score avg: -1341.92 | mem size   3200 |\n",
      "episode:  16 | score avg: -1348.26 | mem size   3400 |\n",
      "episode:  17 | score avg: -1360.86 | mem size   3600 |\n",
      "episode:  18 | score avg: -1336.37 | mem size   3800 |\n",
      "episode:  19 | score avg: -1302.26 | mem size   4000 |\n",
      "episode:  20 | score avg: -1299.82 | mem size   4200 |\n",
      "episode:  21 | score avg: -1274.74 | mem size   4400 |\n",
      "episode:  22 | score avg: -1242.26 | mem size   4600 |\n",
      "episode:  23 | score avg: -1244.46 | mem size   4800 |\n",
      "episode:  24 | score avg: -1252.36 | mem size   5000 |\n",
      "episode:  25 | score avg: -1185.80 | mem size   5200 |\n",
      "episode:  26 | score avg: -1186.15 | mem size   5400 |\n",
      "episode:  27 | score avg: -1162.33 | mem size   5600 |\n",
      "episode:  28 | score avg: -1107.01 | mem size   5800 |\n",
      "episode:  29 | score avg: -1071.36 | mem size   6000 |\n",
      "episode:  30 | score avg: -1051.90 | mem size   6200 |\n",
      "episode:  31 | score avg: -959.43 | mem size   6400 |\n",
      "episode:  32 | score avg: -937.69 | mem size   6600 |\n",
      "episode:  33 | score avg: -963.28 | mem size   6800 |\n",
      "episode:  34 | score avg: -942.03 | mem size   7000 |\n",
      "episode:  35 | score avg: -860.23 | mem size   7200 |\n",
      "episode:  36 | score avg: -864.99 | mem size   7400 |\n",
      "episode:  37 | score avg: -868.47 | mem size   7600 |\n",
      "episode:  38 | score avg: -781.69 | mem size   7800 |\n",
      "episode:  39 | score avg: -776.47 | mem size   8000 |\n",
      "episode:  40 | score avg: -698.89 | mem size   8200 |\n",
      "episode:  41 | score avg: -705.76 | mem size   8400 |\n",
      "episode:  42 | score avg: -683.56 | mem size   8600 |\n",
      "episode:  43 | score avg: -627.78 | mem size   8800 |\n",
      "episode:  44 | score avg: -614.07 | mem size   9000 |\n",
      "episode:  45 | score avg: -613.53 | mem size   9200 |\n",
      "episode:  46 | score avg: -716.67 | mem size   9400 |\n",
      "episode:  47 | score avg: -735.94 | mem size   9600 |\n",
      "episode:  48 | score avg: -698.54 | mem size   9800 |\n",
      "episode:  49 | score avg: -664.29 | mem size  10000 |\n",
      "episode:  50 | score avg: -610.07 | mem size  10200 |\n",
      "episode:  51 | score avg: -636.99 | mem size  10400 |\n",
      "episode:  52 | score avg: -650.78 | mem size  10600 |\n",
      "episode:  53 | score avg: -660.25 | mem size  10800 |\n",
      "episode:  54 | score avg: -642.60 | mem size  11000 |\n",
      "episode:  55 | score avg: -590.76 | mem size  11200 |\n",
      "episode:  56 | score avg: -579.20 | mem size  11400 |\n",
      "episode:  57 | score avg: -545.83 | mem size  11600 |\n",
      "episode:  58 | score avg: -515.44 | mem size  11800 |\n",
      "episode:  59 | score avg: -476.43 | mem size  12000 |\n",
      "episode:  60 | score avg: -490.36 | mem size  12200 |\n",
      "episode:  61 | score avg: -453.85 | mem size  12400 |\n",
      "episode:  62 | score avg: -458.98 | mem size  12600 |\n",
      "episode:  63 | score avg: -437.19 | mem size  12800 |\n",
      "episode:  64 | score avg: -405.96 | mem size  13000 |\n",
      "episode:  65 | score avg: -377.75 | mem size  13200 |\n",
      "episode:  66 | score avg: -352.22 | mem size  13400 |\n",
      "episode:  67 | score avg: -340.82 | mem size  13600 |\n",
      "episode:  68 | score avg: -319.00 | mem size  13800 |\n",
      "episode:  69 | score avg: -299.64 | mem size  14000 |\n",
      "episode:  70 | score avg: -318.09 | mem size  14200 |\n",
      "episode:  71 | score avg: -350.76 | mem size  14400 |\n",
      "episode:  72 | score avg: -328.08 | mem size  14600 |\n",
      "episode:  73 | score avg: -307.36 | mem size  14800 |\n",
      "episode:  74 | score avg: -314.70 | mem size  15000 |\n",
      "episode:  75 | score avg: -372.01 | mem size  15200 |\n",
      "episode:  76 | score avg: -360.37 | mem size  15400 |\n",
      "episode:  77 | score avg: -324.41 | mem size  15600 |\n",
      "episode:  78 | score avg: -292.06 | mem size  15800 |\n",
      "episode:  79 | score avg: -310.72 | mem size  16000 |\n",
      "episode:  80 | score avg: -291.78 | mem size  16200 |\n",
      "episode:  81 | score avg: -286.47 | mem size  16400 |\n",
      "episode:  82 | score avg: -283.05 | mem size  16600 |\n",
      "episode:  83 | score avg: -266.74 | mem size  16800 |\n",
      "episode:  84 | score avg: -275.57 | mem size  17000 |\n",
      "episode:  85 | score avg: -316.56 | mem size  17200 |\n",
      "episode:  86 | score avg: -297.31 | mem size  17400 |\n",
      "episode:  87 | score avg: -279.73 | mem size  17600 |\n",
      "episode:  88 | score avg: -275.90 | mem size  17800 |\n",
      "episode:  89 | score avg: -296.55 | mem size  18000 |\n",
      "episode:  90 | score avg: -302.13 | mem size  18200 |\n",
      "episode:  91 | score avg: -319.50 | mem size  18400 |\n",
      "episode:  92 | score avg: -299.84 | mem size  18600 |\n",
      "episode:  93 | score avg: -293.68 | mem size  18800 |\n",
      "episode:  94 | score avg: -324.38 | mem size  19000 |\n",
      "episode:  95 | score avg: -304.47 | mem size  19200 |\n",
      "episode:  96 | score avg: -309.58 | mem size  19400 |\n",
      "episode:  97 | score avg: -290.75 | mem size  19600 |\n",
      "episode:  98 | score avg: -273.94 | mem size  19800 |\n",
      "episode:  99 | score avg: -246.70 | mem size  20000 |\n",
      "episode: 100 | score avg: -234.45 | mem size  20200 |\n",
      "episode: 101 | score avg: -260.12 | mem size  20400 |\n",
      "episode: 102 | score avg: -283.92 | mem size  20600 |\n",
      "episode: 103 | score avg: -267.82 | mem size  20800 |\n",
      "episode: 104 | score avg: -265.00 | mem size  21000 |\n",
      "episode: 105 | score avg: -238.57 | mem size  21200 |\n",
      "episode: 106 | score avg: -275.33 | mem size  21400 |\n",
      "episode: 107 | score avg: -260.38 | mem size  21600 |\n",
      "episode: 108 | score avg: -258.09 | mem size  21800 |\n",
      "episode: 109 | score avg: -256.52 | mem size  22000 |\n",
      "episode: 110 | score avg: -242.79 | mem size  22200 |\n",
      "episode: 111 | score avg: -254.50 | mem size  22400 |\n",
      "episode: 112 | score avg: -341.01 | mem size  22600 |\n",
      "episode: 113 | score avg: -330.80 | mem size  22800 |\n",
      "episode: 114 | score avg: -346.31 | mem size  23000 |\n",
      "episode: 115 | score avg: -324.15 | mem size  23200 |\n",
      "episode: 116 | score avg: -304.22 | mem size  23400 |\n",
      "episode: 117 | score avg: -286.06 | mem size  23600 |\n",
      "episode: 118 | score avg: -422.06 | mem size  23800 |\n",
      "episode: 119 | score avg: -414.80 | mem size  24000 |\n",
      "episode: 120 | score avg: -397.50 | mem size  24200 |\n",
      "episode: 121 | score avg: -393.20 | mem size  24400 |\n",
      "episode: 122 | score avg: -366.18 | mem size  24600 |\n",
      "episode: 123 | score avg: -365.81 | mem size  24800 |\n",
      "episode: 124 | score avg: -341.81 | mem size  25000 |\n",
      "episode: 125 | score avg: -355.81 | mem size  25200 |\n",
      "episode: 126 | score avg: -370.07 | mem size  25400 |\n",
      "episode: 127 | score avg: -357.08 | mem size  25600 |\n",
      "episode: 128 | score avg: -389.42 | mem size  25800 |\n",
      "episode: 129 | score avg: -363.08 | mem size  26000 |\n",
      "episode: 130 | score avg: -362.80 | mem size  26200 |\n",
      "episode: 131 | score avg: -361.95 | mem size  26400 |\n",
      "episode: 132 | score avg: -361.71 | mem size  26600 |\n",
      "episode: 133 | score avg: -374.22 | mem size  26800 |\n",
      "episode: 134 | score avg: -360.42 | mem size  27000 |\n",
      "episode: 135 | score avg: -336.95 | mem size  27200 |\n",
      "episode: 136 | score avg: -339.70 | mem size  27400 |\n",
      "episode: 137 | score avg: -318.32 | mem size  27600 |\n",
      "episode: 138 | score avg: -299.11 | mem size  27800 |\n",
      "episode: 139 | score avg: -304.63 | mem size  28000 |\n",
      "episode: 140 | score avg: -323.45 | mem size  28200 |\n",
      "episode: 141 | score avg: -291.15 | mem size  28400 |\n",
      "episode: 142 | score avg: -298.04 | mem size  28600 |\n",
      "episode: 143 | score avg: -268.28 | mem size  28800 |\n",
      "episode: 144 | score avg: -289.23 | mem size  29000 |\n",
      "episode: 145 | score avg: -309.66 | mem size  29200 |\n",
      "episode: 146 | score avg: -291.13 | mem size  29400 |\n",
      "episode: 147 | score avg: -286.84 | mem size  29600 |\n",
      "episode: 148 | score avg: -310.45 | mem size  29800 |\n",
      "episode: 149 | score avg: -291.51 | mem size  30000 |\n",
      "episode: 150 | score avg: -262.40 | mem size  30200 |\n",
      "episode: 151 | score avg: -272.26 | mem size  30400 |\n",
      "episode: 152 | score avg: -257.63 | mem size  30600 |\n",
      "episode: 153 | score avg: -280.11 | mem size  30800 |\n",
      "episode: 154 | score avg: -343.38 | mem size  31000 |\n",
      "episode: 155 | score avg: -344.54 | mem size  31200 |\n",
      "episode: 156 | score avg: -322.49 | mem size  31400 |\n",
      "episode: 157 | score avg: -356.32 | mem size  31600 |\n",
      "episode: 158 | score avg: -333.29 | mem size  31800 |\n",
      "episode: 159 | score avg: -312.18 | mem size  32000 |\n",
      "episode: 160 | score avg: -316.63 | mem size  32200 |\n",
      "episode: 161 | score avg: -321.68 | mem size  32400 |\n",
      "episode: 162 | score avg: -289.57 | mem size  32600 |\n",
      "episode: 163 | score avg: -309.27 | mem size  32800 |\n",
      "episode: 164 | score avg: -313.55 | mem size  33000 |\n",
      "episode: 165 | score avg: -294.28 | mem size  33200 |\n",
      "episode: 166 | score avg: -277.27 | mem size  33400 |\n",
      "episode: 167 | score avg: -261.86 | mem size  33600 |\n",
      "episode: 168 | score avg: -260.19 | mem size  33800 |\n",
      "episode: 169 | score avg: -269.13 | mem size  34000 |\n",
      "episode: 170 | score avg: -302.69 | mem size  34200 |\n",
      "episode: 171 | score avg: -320.95 | mem size  34400 |\n",
      "episode: 172 | score avg: -301.42 | mem size  34600 |\n",
      "episode: 173 | score avg: -334.06 | mem size  34800 |\n",
      "episode: 174 | score avg: -335.81 | mem size  35000 |\n",
      "episode: 175 | score avg: -314.80 | mem size  35200 |\n",
      "episode: 176 | score avg: -306.91 | mem size  35400 |\n",
      "episode: 177 | score avg: -312.53 | mem size  35600 |\n",
      "episode: 178 | score avg: -293.41 | mem size  35800 |\n",
      "episode: 179 | score avg: -326.57 | mem size  36000 |\n",
      "episode: 180 | score avg: -318.05 | mem size  36200 |\n",
      "episode: 181 | score avg: -310.18 | mem size  36400 |\n",
      "episode: 182 | score avg: -291.74 | mem size  36600 |\n",
      "episode: 183 | score avg: -287.02 | mem size  36800 |\n",
      "episode: 184 | score avg: -270.82 | mem size  37000 |\n",
      "episode: 185 | score avg: -255.97 | mem size  37200 |\n",
      "episode: 186 | score avg: -266.42 | mem size  37400 |\n",
      "episode: 187 | score avg: -239.93 | mem size  37600 |\n",
      "episode: 188 | score avg: -227.96 | mem size  37800 |\n",
      "episode: 189 | score avg: -217.27 | mem size  38000 |\n",
      "episode: 190 | score avg: -244.67 | mem size  38200 |\n",
      "episode: 191 | score avg: -244.07 | mem size  38400 |\n",
      "episode: 192 | score avg: -243.86 | mem size  38600 |\n",
      "episode: 193 | score avg: -219.62 | mem size  38800 |\n",
      "episode: 194 | score avg: -221.25 | mem size  39000 |\n",
      "episode: 195 | score avg: -211.72 | mem size  39200 |\n",
      "episode: 196 | score avg: -202.77 | mem size  39400 |\n",
      "episode: 197 | score avg: -254.02 | mem size  39600 |\n",
      "episode: 198 | score avg: -240.92 | mem size  39800 |\n",
      "episode: 199 | score avg: -229.17 | mem size  40000 |\n",
      "episode: 200 | score avg: -219.00 | mem size  40200 |\n",
      "episode: 201 | score avg: -221.68 | mem size  40400 |\n",
      "episode: 202 | score avg: -199.66 | mem size  40600 |\n",
      "End\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "env.close()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}