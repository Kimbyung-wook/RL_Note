{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('TF240': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8f237aa33f5a133d3a67a1e00bf3cb9d47b6c38bcc6ab493273f5d4df41b8866"
    }
   }
  },
  "interpreter": {
   "hash": "61683dc6b2a2d3d4f2fca4fc9c31d7600238da1c31c9bb494e8f77b62993b62b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Find RL_Note path and append sys path\r\n",
    "import os, sys\r\n",
    "cwd = os.getcwd()\r\n",
    "pos = cwd.find('RL_Note')\r\n",
    "root_path = cwd[0:pos] + 'RL_Note'\r\n",
    "sys.path.append(root_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Refer from\n",
    "#  https://pasus.tistory.com/138\n",
    "#  https://horomary.hatenablog.com/entry/2020/06/26/003806\n",
    "#  https://keras.io/examples/rl/ddpg_pendulum/\n",
    "#  https://github.com/dongminlee94/Samsung-DRL-Code/blob/master/5_SAC/sac/model.py\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, concatenate, Lambda\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "import matplotlib.pyplot as plt\n",
    "from pys.utils.memory import ReplayMemory\n",
    "from pys.utils.prioritized_memory import ProportionalPrioritizedMemory"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class Actor(tf.keras.Model):\r\n",
    "    def __init__(self, state_size, action_size, log_std_min, log_std_max):\r\n",
    "        super(Actor, self).__init__()\r\n",
    "        self.log_std_min = log_std_min\r\n",
    "        self.log_std_max = log_std_max\r\n",
    "\r\n",
    "        self.fc1= Dense(64, activation='relu')\r\n",
    "        self.fc2= Dense(64, activation='relu')\r\n",
    "        # self.fc3 = Dense(16, activation='relu')\r\n",
    "        self.mu = Dense(action_size)\r\n",
    "        self.log_std= Dense(action_size)\r\n",
    "\r\n",
    "    def call(self, x):\r\n",
    "        x       = self.fc1(x)\r\n",
    "        x       = self.fc2(x)\r\n",
    "        # x       = self.fc3(x)\r\n",
    "        mu = self.mu(x)\r\n",
    "        log_std = self.log_std(x)\r\n",
    "        log_std = tf.clip_by_value(log_std, self.log_std_min, self.log_std_max)\r\n",
    "        std = tf.math.exp(log_std)\r\n",
    "        return mu, std\r\n",
    "\r\n",
    "class Critic(tf.keras.Model):\r\n",
    "    def __init__(self, state_size, action_size):\r\n",
    "        super(Critic, self).__init__()\r\n",
    "        self.s1 = Dense(16, activation='relu')\r\n",
    "        self.s2 = Dense(32, activation='relu')\r\n",
    "        self.a1 = Dense(32, activation='relu')\r\n",
    "        self.a2 = Dense(32, activation='relu')\r\n",
    "        self.fc1= Dense(64, activation='relu')\r\n",
    "        self.fc2= Dense(64, activation='relu')\r\n",
    "        self.out= Dense(1,  activation='linear')\r\n",
    "\r\n",
    "    def call(self,state_action):\r\n",
    "        state  = state_action[0]\r\n",
    "        action = state_action[1]\r\n",
    "        s = self.s1(state)\r\n",
    "        s = self.s2(s)\r\n",
    "        a = self.a1(action)\r\n",
    "        a = self.a2(a)\r\n",
    "        c = concatenate([s,a],axis=-1)\r\n",
    "        x = self.fc1(c)\r\n",
    "        x = self.fc2(x)\r\n",
    "        q = self.out(x)\r\n",
    "        return q"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# https://github.com/dongminlee94/Samsung-DRL-Code/blob/master/5_SAC/sac/utils.py\n",
    "# https://github.com/RickyMexx/SAC-tf2/blob/master/SAC/SAC_rla.py\n",
    "# https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch/blob/b338c87bebb672e39304e47e0eed55aeb462b243/agents/Base_Agent.py#L278\n",
    "# \n",
    "class SACAgent:\n",
    "    def __init__(self, state_size, action_size, action_min, action_max, cfg:dict):\n",
    "        self.state_size = state_size\n",
    "        self.action_size= action_size\n",
    "        self.action_min = action_min\n",
    "        self.action_max = action_max\n",
    "        self.env_name   = cfg[\"ENV\"]\n",
    "        self.rl_type    = \"SAC\"\n",
    "        self.er_type    = cfg[\"ER\"].upper()\n",
    "\n",
    "        # Hyper params for learning\n",
    "        self.discount_factor = 0.99\n",
    "        self.critic_learning_rate = 0.002\n",
    "        self.actor_learning_rate  = 0.001\n",
    "        self.alpha_learning_rate  = 0.001\n",
    "        self.tau = 0.005\n",
    "        self.alpha = 0.200 # temperature\n",
    "\n",
    "        # Experience Replay\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 2000\n",
    "        self.buffer_size = 50000\n",
    "        if self.er_type == \"ER\":\n",
    "            self.memory = ReplayMemory(capacity=self.buffer_size)\n",
    "        elif self.er_type == \"PER\":\n",
    "            self.memory = ProportionalPrioritizedMemory(capacity=self.buffer_size)\n",
    "\n",
    "        self.critic1_optimizer  = tf.keras.optimizers.Adam(learning_rate=self.critic_learning_rate)\n",
    "        self.critic2_optimizer  = tf.keras.optimizers.Adam(learning_rate=self.critic_learning_rate)\n",
    "        self.actor_optimizer    = tf.keras.optimizers.Adam(learning_rate=self.actor_learning_rate)\n",
    "        self.alpha_optimizer    = tf.keras.optimizers.Adam(learning_rate=self.alpha_learning_rate)\n",
    "\n",
    "        self.critic1        = Critic(self.state_size, self.action_size)\n",
    "        self.critic2        = Critic(self.state_size, self.action_size)\n",
    "        self.target_critic1 = Critic(self.state_size, self.action_size)\n",
    "        self.target_critic2 = Critic(self.state_size, self.action_size)\n",
    "        self.actor          = Actor(self.state_size, self.action_size, self.action_min, self.action_max)\n",
    "        self.target_actor   = Actor(self.state_size, self.action_size, self.action_min, self.action_max)\n",
    "        self.log_alpha      = tf.math.log(0.2)\n",
    "        self.alpha          = tf.math.exp(self.log_alpha)\n",
    "\n",
    "        self.actor.build(input_shape=(None, self.state_size))\n",
    "        self.target_actor.build(input_shape=(None, self.state_size))\n",
    "        state_in = Input(shape=(self.state_size,),dtype=tf.float32)\n",
    "        action_in = Input(shape=(self.action_size,),dtype=tf.float32)\n",
    "        self.actor(state_in)\n",
    "        self.target_actor(state_in)\n",
    "        self.critic1([state_in, action_in])\n",
    "        self.critic2([state_in, action_in])\n",
    "        self.target_critic1([state_in, action_in])\n",
    "        self.target_critic2([state_in, action_in])\n",
    "\n",
    "        self.actor.summary()\n",
    "        self.critic1.summary()\n",
    "        # self.critic2.summary()\n",
    "        \n",
    "        self.hard_update_target_model()\n",
    "        self.target_entropy = -tf.convert_to_tensor(np.array(self.action_size,dtype=np.float32),dtype=tf.float32)\n",
    "\n",
    "        self.train_idx = 0\n",
    "        self.show_media_info = False\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        state       = np.array(state,dtype=np.float32)\n",
    "        action      = np.array(action,dtype=np.float32)\n",
    "        reward      = np.array([reward],dtype=np.float32)\n",
    "        done        = np.array([done],dtype=np.float32)\n",
    "        next_state  = np.array(next_state,dtype=np.float32)\n",
    "        transition  = (state, action, reward, next_state, done)\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def hard_update_target_model(self):\n",
    "        self.target_actor.set_weights(self.actor.get_weights())\n",
    "        self.target_critic1.set_weights(self.critic1.get_weights())\n",
    "        self.target_critic2.set_weights(self.critic2.get_weights())\n",
    "\n",
    "    def soft_update_target_model(self):\n",
    "        tau = self.tau\n",
    "        for (net, target_net) in zip(   self.actor.trainable_variables,\n",
    "                                        self.target_actor.trainable_variables):\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\n",
    "        for (net, target_net) in zip(   self.critic1.trainable_variables,\n",
    "                                        self.target_critic1.trainable_variables):\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\n",
    "        for (net, target_net) in zip(   self.critic2.trainable_variables,\n",
    "                                        self.target_critic2.trainable_variables):\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        mu, std = self.actor(state)\n",
    "        action, _ = self.eval_action(mu,std)\n",
    "        return action.numpy()[0]\n",
    "\n",
    "    def eval_action(self, mu, std, epsilon=1e-6):\n",
    "        action_prob = tfd.Normal(loc=mu, scale=std)\n",
    "        z = action_prob.sample()\n",
    "        action = tf.math.tanh(z)\n",
    "        log_prob = action_prob.log_prob(z) - tf.math.log(1.0 - tf.pow(action,2) + epsilon)\n",
    "        log_prob = tf.reduce_sum(log_prob, axis=-1, keepdims=True)\n",
    "\n",
    "        return action, log_prob\n",
    "\n",
    "    def train_model(self):\n",
    "        # Train from Experience Replay\n",
    "        # Training Condition - Memory Size\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return 0.0,0.0\n",
    "        # Sampling from the memory\n",
    "        if self.er_type == \"ER\":\n",
    "            mini_batch = self.memory.sample(self.batch_size)\n",
    "        elif self.er_type == \"PER\":\n",
    "            mini_batch, idxs, is_weights = self.memory.sample(self.batch_size)\n",
    "\n",
    "        states      = tf.convert_to_tensor(np.array([sample[0] for sample in mini_batch]))\n",
    "        actions     = tf.convert_to_tensor(np.array([sample[1] for sample in mini_batch]))\n",
    "        rewards     = tf.convert_to_tensor(np.array([sample[2] for sample in mini_batch]))\n",
    "        next_states = tf.convert_to_tensor(np.array([sample[3] for sample in mini_batch]))\n",
    "        dones       = tf.convert_to_tensor(np.array([sample[4] for sample in mini_batch]))\n",
    "        \n",
    "        if self.show_media_info == False:\n",
    "            self.show_media_info = True\n",
    "            print('Start to train, check batch shapes')\n",
    "            print('**** shape of states', np.shape(states),type(states))\n",
    "            print('**** shape of actions', np.shape(actions),type(actions))\n",
    "            print('**** shape of rewards', np.shape(rewards),type(rewards))\n",
    "            print('**** shape of next_states', np.shape(next_states),type(next_states))\n",
    "            print('**** shape of dones', np.shape(dones),type(dones))\n",
    "        \n",
    "        # Update critic\n",
    "        mu, std = self.actor(next_states,training=True)\n",
    "        next_actions, next_log_pi = self.eval_action(mu, std)\n",
    "        target_q1 = self.target_critic1([next_states,next_actions],training=True)\n",
    "        target_q2 = self.target_critic2([next_states,next_actions],training=True)\n",
    "        target_q_min = tf.minimum(target_q1, target_q2) # Clipping Double Q\n",
    "        target_value = rewards + (1.0 - dones) * self.discount_factor * (target_q_min - self.alpha * next_log_pi)\n",
    "\n",
    "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "            q1 = self.critic1([states, actions], training=True)\n",
    "            q2 = self.critic2([states, actions], training=True)\n",
    "            td_error = (tf.abs(target_value - q1) + tf.abs(target_value - q2))/2.0\n",
    "            if self.er_type == \"ER\":\n",
    "                critic1_loss = tf.math.reduce_mean(tf.math.square(target_value - q1))\n",
    "                critic2_loss = tf.math.reduce_mean(tf.math.square(target_value - q2))\n",
    "            elif self.er_type == \"PER\":\n",
    "                critic1_loss = tf.math.reduce_mean(is_weights * tf.math.square(target_value - q1))\n",
    "                critic2_loss = tf.math.reduce_mean(is_weights * tf.math.square(target_value - q2))\n",
    "        critic1_params = self.critic1.trainable_variables\n",
    "        critic2_params = self.critic2.trainable_variables\n",
    "        critic1_grads = tape1.gradient(critic1_loss, critic1_params)\n",
    "        critic2_grads = tape2.gradient(critic2_loss, critic2_params)\n",
    "        self.critic1_optimizer.apply_gradients(zip(critic1_grads, critic1_params))\n",
    "        self.critic2_optimizer.apply_gradients(zip(critic2_grads, critic2_params))\n",
    "\n",
    "        # Update actor\n",
    "        actor_loss_out = 0.0\n",
    "        with tf.GradientTape() as tape:\n",
    "            mu, std = self.actor(states,training=True)\n",
    "            new_actions, new_log_pi = self.eval_action(mu,std)\n",
    "            new_q1 = self.critic1([states, new_actions],training=True)\n",
    "            new_q2 = self.critic2([states, new_actions],training=True)\n",
    "            new_q_min = tf.minimum(new_q1, new_q2)\n",
    "            actor_loss = tf.reduce_mean(self.alpha * new_log_pi - new_q_min)\n",
    "        actor_loss_out = actor_loss.numpy()\n",
    "        actor_params = self.actor.trainable_variables\n",
    "        actor_grads = tape.gradient(actor_loss, actor_params)\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, actor_params))\n",
    "\n",
    "        # Update alpha\n",
    "        # with tf.GradientTape() as tape:\n",
    "        #     alpha_loss = - tf.reduce_mean(self.log_alpha * (new_log_pi + self.target_entropy))\n",
    "        # alpha_params = self.log_alpha\n",
    "        # alpha_grads = tape.gradient(alpha_loss, alpha_params)\n",
    "        # self.alpha_optimizer.apply_gradients(zip(alpha_grads, alpha_params))\n",
    "        # self.alpha = tf.math.exp(self.log_alpha)\n",
    "\n",
    "        if self.er_type == \"PER\":\n",
    "            sample_importance = td_error.numpy()\n",
    "            for i in range(self.batch_size):\n",
    "                self.memory.update(idxs[i], sample_importance[i])\n",
    "\n",
    "        self.soft_update_target_model()\n",
    "        critic_loss_out = 0.5*(critic1_loss.numpy() + critic2_loss.numpy())\n",
    "        return critic_loss_out, actor_loss_out\n",
    "\n",
    "    def save_model(self):\n",
    "        self.actor.save_weights(  \"./save_model/\" + self.env_name + \"_\" + self.rl_type + \"_\" + self.er_type + \"_TF_actor\", save_format=\"tf\")\n",
    "        self.critic1.save_weights(\"./save_model/\" + self.env_name + \"_\" + self.rl_type + \"_\" + self.er_type + \"_TF_critic1\", save_format=\"tf\")\n",
    "        self.critic2.save_weights(\"./save_model/\" + self.env_name + \"_\" + self.rl_type + \"_\" + self.er_type + \"_TF_critic2\", save_format=\"tf\")\n",
    "        return\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# %matplotlib tk\n",
    "cfg = {\"ENV\":\"Pendulum-v0\",\\\n",
    "        \"ER\":\"ER\",\\\n",
    "        \"RL\":\"SAC\"}\n",
    "EPISODES    = 500\n",
    "END_SCORE   = -200\n",
    "figure = plt.gcf()\n",
    "figure.set_size_inches(8,6)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(cfg[\"ENV\"])\n",
    "    state_size      = env.observation_space.shape[0]\n",
    "    action_size     = env.action_space.shape[0]\n",
    "    log_std_min     = -20.0\n",
    "    log_std_max     = 5.0\n",
    "\n",
    "    agent = SACAgent(state_size, action_size, log_std_min, log_std_max, cfg)\n",
    "    print('Env Name : ',cfg[\"ENV\"])\n",
    "    print('States {0}, Actions {1}'.format(state_size, action_size))\n",
    "    print('Action scale exp({0:.2f} ~ {1:.2f})'.format(log_std_min, log_std_max))\n",
    "    scores_avg, scores_raw, episodes, losses = [], [], [], []\n",
    "    critic_mean, actor_mean = [], []\n",
    "    score_avg = 0\n",
    "    critics = 0\n",
    "    actors = 0\n",
    "\n",
    "    end = False\n",
    "    show_media_info = True\n",
    "    \n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        critic_losses = []\n",
    "        actor_losses = []\n",
    "        while not done:\n",
    "            # if e%100 == 0:\n",
    "            #     env.render()\n",
    "            # Interact with env.\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            critic_loss, actor_loss = agent.train_model()\n",
    "            state = next_state\n",
    "            # \n",
    "            score += reward\n",
    "            critic_losses.append(critic_loss)\n",
    "            actor_losses.append(actor_loss)\n",
    "            if show_media_info:\n",
    "                print(\"-------------- Variable shapes --------------\")\n",
    "                print(\"State Shape : \", np.shape(state))\n",
    "                print(\"Action Shape : \", np.shape(action))\n",
    "                print(\"Reward Shape : \", np.shape(reward))\n",
    "                print(\"done Shape : \", np.shape(done))\n",
    "                print(\"---------------------------------------------\")\n",
    "                show_media_info = False\n",
    "            if done:\n",
    "                score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\n",
    "                print(\"episode: {0:3d} | score avg: {1:3.2f} | mem size {2:6d} |\"\n",
    "                    .format(e, score_avg, len(agent.memory)))\n",
    "\n",
    "                episodes.append(e)\n",
    "                scores_avg.append(score_avg)\n",
    "                scores_raw.append(score)\n",
    "                critic_mean.append(np.mean(critic_losses))\n",
    "                actor_mean.append(np.mean(actor_losses))\n",
    "                # View data\n",
    "                plt.clf()\n",
    "                plt.subplot(311)\n",
    "                plt.plot(episodes, scores_avg, 'b')\n",
    "                plt.plot(episodes, scores_raw, 'b', alpha=0.8, linewidth=0.5)\n",
    "                plt.xlabel('episode'); plt.ylabel('average score'); plt.grid()\n",
    "                plt.title(cfg[\"ENV\"] +'_' + cfg[\"RL\"] +'_' + cfg[\"ER\"])\n",
    "                plt.subplot(312)\n",
    "                plt.plot(episodes, critic_mean, 'b.',markersize=3)\n",
    "                plt.xlabel('episode'); plt.ylabel('critic loss'); plt.grid()\n",
    "                plt.subplot(313)\n",
    "                plt.plot(episodes, actor_mean, 'b.',markersize=3)\n",
    "                plt.xlabel('episode'); plt.ylabel('actor loss'); plt.grid()\n",
    "                # print(\"./result/\" + ENV_NAME +'_' + RL_TYPE +'_' + ER_TYPE + \"_TF.png\")\n",
    "                plt.savefig(\"./result/\" + cfg[\"ENV\"] +'_' + cfg[\"RL\"] +'_' + cfg[\"ER\"] + \"_TF.jpg\", dpi=100)\n",
    "\n",
    "                # 이동 평균이 0 이상일 때 종료\n",
    "                if score_avg > END_SCORE:\n",
    "                    agent.save_model()\n",
    "                    end = True\n",
    "                    break\n",
    "                \n",
    "            # break\n",
    "        if end == True:\n",
    "            env.close()\n",
    "            np.save(\"./save_model/data/\" + cfg[\"ENV\"] + '_' + cfg[\"RL\"] + '_' + cfg[\"ER\"] + \"_TF_epi\",  episodes)\n",
    "            np.save(\"./save_model/data/\" + cfg[\"ENV\"] + '_' + cfg[\"RL\"] + '_' + cfg[\"ER\"] + \"_TF_scores_avg\",scores_avg)\n",
    "            np.save(\"./save_model/data/\" + cfg[\"ENV\"] + '_' + cfg[\"RL\"] + '_' + cfg[\"ER\"] + \"_TF_scores_raw\",scores_raw)\n",
    "            np.save(\"./save_model/data/\" + cfg[\"ENV\"] + '_' + cfg[\"RL\"] + '_' + cfg[\"ER\"] + \"_TF_critic_mean\",critic_mean)\n",
    "            np.save(\"./save_model/data/\" + cfg[\"ENV\"] + '_' + cfg[\"RL\"] + '_' + cfg[\"ER\"] + \"_TF_actor_mean\",actor_mean)\n",
    "            print(\"End\")\n",
    "            break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"actor\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             multiple                  256       \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             multiple                  65        \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             multiple                  65        \n",
      "=================================================================\n",
      "Total params: 4,546\n",
      "Trainable params: 4,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"critic\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  64        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  544       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  64        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  1056      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  65        \n",
      "=================================================================\n",
      "Total params: 10,113\n",
      "Trainable params: 10,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Env Name :  Pendulum-v0\n",
      "States 3, Actions 1\n",
      "Action scale exp(-20.00 ~ 5.00)\n",
      "-------------- Variable shapes --------------\n",
      "State Shape :  (3,)\n",
      "Action Shape :  (1,)\n",
      "Reward Shape :  ()\n",
      "done Shape :  ()\n",
      "---------------------------------------------\n",
      "episode:   0 | score avg: -950.38 | mem size    200 |\n",
      "episode:   1 | score avg: -984.62 | mem size    400 |\n",
      "episode:   2 | score avg: -971.81 | mem size    600 |\n",
      "episode:   3 | score avg: -990.37 | mem size    800 |\n",
      "episode:   4 | score avg: -1069.58 | mem size   1000 |\n",
      "episode:   5 | score avg: -1088.75 | mem size   1200 |\n",
      "episode:   6 | score avg: -1077.59 | mem size   1400 |\n",
      "episode:   7 | score avg: -1114.94 | mem size   1600 |\n",
      "episode:   8 | score avg: -1120.99 | mem size   1800 |\n",
      "Start to train, check batch shapes\n",
      "**** shape of states (32, 3) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of actions (32, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of rewards (32, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of next_states (32, 3) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of dones (32, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "episode:   9 | score avg: -1091.85 | mem size   2000 |\n",
      "episode:  10 | score avg: -1128.88 | mem size   2200 |\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ef4e3ed49f23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mcritic_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-9d38bf210f06>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[0mactor_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[0mactor_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactor_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;31m# Update alpha\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    633\u001b[0m           \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m           kwargs={\n\u001b[1;32m--> 635\u001b[1;33m               \u001b[1;34m\"name\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m           })\n\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mmerge_call\u001b[1;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[0;32m   2939\u001b[0m     merge_fn = autograph.tf_convert(\n\u001b[0;32m   2940\u001b[0m         merge_fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[1;32m-> 2941\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_merge_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerge_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2942\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2943\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_merge_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_merge_call\u001b[1;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[0;32m   2946\u001b[0m         distribution_strategy_context._CrossReplicaThreadMode(self._strategy))  # pylint: disable=protected-access\n\u001b[0;32m   2947\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2948\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmerge_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2949\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2950\u001b[0m       \u001b[0m_pop_per_thread_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    570\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36m_distributed_apply\u001b[1;34m(self, distribution, grads_and_vars, name, apply_state)\u001b[0m\n\u001b[0;32m    681\u001b[0m                               \"update_\" + var.op.name, skip_on_eager=True):\n\u001b[0;32m    682\u001b[0m             update_ops.extend(distribution.extended.update(\n\u001b[1;32m--> 683\u001b[1;33m                 var, apply_grad_to_update_var, args=(grad,), group=False))\n\u001b[0m\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m       any_symbolic = any(isinstance(i, ops.Operation) or\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   2492\u001b[0m         fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m   2493\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2494\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2495\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2496\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_update\u001b[1;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[0;32m   3429\u001b[0m     \u001b[1;31m# The implementations of _update() and _update_non_slot() are identical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3430\u001b[0m     \u001b[1;31m# except _update() passes `var` as the first argument to `fn()`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3431\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_non_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3433\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_update_non_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\u001b[0m in \u001b[0;36m_update_non_slot\u001b[1;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[0;32m   3435\u001b[0m     \u001b[1;31m# once that value is used for something.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3436\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mUpdateContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3437\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3438\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mshould_group\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3439\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    570\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mismethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_grad_to_update_var\u001b[1;34m(var, grad)\u001b[0m\n\u001b[0;32m    656\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;34m\"apply_state\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dense_apply_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[0mapply_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"apply_state\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m       \u001b[0mupdate_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_resource_apply_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mapply_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstraint\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\adam.py\u001b[0m in \u001b[0;36m_resource_apply_dense\u001b[1;34m(self, grad, var, apply_state)\u001b[0m\n\u001b[0;32m    184\u001b[0m           \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcoefficients\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epsilon'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m           \u001b[0mgrad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m           use_locking=self._use_locking)\n\u001b[0m\u001b[0;32m    187\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m       \u001b[0mvhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_slot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vhat'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\util\\tf_export.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    402\u001b[0m           \u001b[1;34m'Please pass these args as kwargs instead.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m           .format(f=f.__name__, kwargs=f_argspec.args))\n\u001b[1;32m--> 404\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorator_argspec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf_argspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\ops\\gen_training_ops.py\u001b[0m in \u001b[0;36mresource_apply_adam\u001b[1;34m(var, m, v, beta1_power, beta2_power, lr, beta1, beta2, epsilon, grad, use_locking, use_nesterov, name)\u001b[0m\n\u001b[0;32m   1421\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ResourceApplyAdam\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta1_power\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta2_power\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m         \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"use_locking\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1423\u001b[1;33m         \"use_nesterov\", use_nesterov)\n\u001b[0m\u001b[0;32m   1424\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1425\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "env.close()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}