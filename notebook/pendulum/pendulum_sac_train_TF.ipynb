{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('tf240': conda)"
  },
  "interpreter": {
   "hash": "fbba320975a9114d2433fba427f26c389728c846a7c4900c481dce2a1a9f6231"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Refer from\r\n",
    "#  https://pasus.tistory.com/138\r\n",
    "#  https://horomary.hatenablog.com/entry/2020/06/26/003806\r\n",
    "#  https://keras.io/examples/rl/ddpg_pendulum/\r\n",
    "#  https://github.com/dongminlee94/Samsung-DRL-Code/blob/master/5_SAC/sac/model.py\r\n",
    "# ! pip \r\n",
    "import gym\r\n",
    "import sys\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, concatenate, Lambda\r\n",
    "import tensorflow_probability as tfp\r\n",
    "tfd = tfp.distributions\r\n",
    "from collections import deque\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class Actor(tf.keras.Model):\r\n",
    "    def __init__(self, state_size, action_size, log_std_min, log_std_max):\r\n",
    "        super(Actor, self).__init__()\r\n",
    "        self.log_std_min = log_std_min\r\n",
    "        self.log_std_max = log_std_max\r\n",
    "\r\n",
    "        self.fc1= Dense(64, activation='relu')\r\n",
    "        self.fc2= Dense(64, activation='relu')\r\n",
    "        # self.fc3 = Dense(16, activation='relu')\r\n",
    "        self.mu = Dense(action_size)\r\n",
    "        self.log_std= Dense(action_size)\r\n",
    "\r\n",
    "    def call(self, x):\r\n",
    "        x       = self.fc1(x)\r\n",
    "        x       = self.fc2(x)\r\n",
    "        # x       = self.fc3(x)\r\n",
    "        mu = self.mu(x)\r\n",
    "        log_std = self.log_std(x)\r\n",
    "        log_std = tf.clip_by_value(log_std, self.log_std_min, self.log_std_max)\r\n",
    "        std = tf.math.exp(log_std)\r\n",
    "        return mu, std\r\n",
    "\r\n",
    "class Critic(tf.keras.Model):\r\n",
    "    def __init__(self, state_size, action_size):\r\n",
    "        super(Critic, self).__init__()\r\n",
    "        self.s1 = Dense(16, activation='relu')\r\n",
    "        self.s2 = Dense(32, activation='relu')\r\n",
    "        self.a1 = Dense(32, activation='relu')\r\n",
    "        self.a2 = Dense(32, activation='relu')\r\n",
    "        self.fc1= Dense(64, activation='relu')\r\n",
    "        self.fc2= Dense(64, activation='relu')\r\n",
    "        self.out= Dense(1,  activation='linear')\r\n",
    "\r\n",
    "    def call(self,state,action):\r\n",
    "        # state  = state_action[0]\r\n",
    "        # action = state_action[1]\r\n",
    "        s = self.s1(state)\r\n",
    "        s = self.s2(s)\r\n",
    "        a = self.a1(action)\r\n",
    "        a = self.a2(a)\r\n",
    "        c = concatenate([s,a],axis=-1)\r\n",
    "        x = self.fc1(c)\r\n",
    "        x = self.fc2(x)\r\n",
    "        q = self.out(x)\r\n",
    "        return q"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# https://github.com/dongminlee94/Samsung-DRL-Code/blob/master/5_SAC/sac/utils.py\r\n",
    "# https://github.com/RickyMexx/SAC-tf2/blob/master/SAC/SAC_rla.py\r\n",
    "# https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch/blob/b338c87bebb672e39304e47e0eed55aeb462b243/agents/Base_Agent.py#L278\r\n",
    "# \r\n",
    "class TD3Agent:\r\n",
    "    def __init__(self, state_size, action_size, action_min, action_max):\r\n",
    "        self.state_size = state_size\r\n",
    "        self.action_size= action_size\r\n",
    "        self.action_min = action_min\r\n",
    "        self.action_max = action_max\r\n",
    "\r\n",
    "        # Hyper params for learning\r\n",
    "        self.discount_factor = 0.99\r\n",
    "        self.critic_learning_rate = 0.002\r\n",
    "        self.actor_learning_rate  = 0.001\r\n",
    "        self.alpha_learning_rate  = 0.001\r\n",
    "        self.tau = 0.005\r\n",
    "        self.alpha = 0.200 # temperature\r\n",
    "\r\n",
    "        # Experience Replay\r\n",
    "        self.batch_size = 64\r\n",
    "        self.train_start = 2000\r\n",
    "        self.memory = deque(maxlen=50000)\r\n",
    "\r\n",
    "        self.critic1_optimizer  = tf.keras.optimizers.Adam(learning_rate=self.critic_learning_rate)\r\n",
    "        self.critic2_optimizer  = tf.keras.optimizers.Adam(learning_rate=self.critic_learning_rate)\r\n",
    "        self.actor_optimizer    = tf.keras.optimizers.Adam(learning_rate=self.actor_learning_rate)\r\n",
    "        # self.alpha_optimizer    = tf.keras.optimizers.Adam(learning_rate=self.alpha_learning_rate)\r\n",
    "\r\n",
    "        self.critic1        = Critic(self.state_size, self.action_size)\r\n",
    "        self.critic2        = Critic(self.state_size, self.action_size)\r\n",
    "        self.target_critic1 = Critic(self.state_size, self.action_size)\r\n",
    "        self.target_critic2 = Critic(self.state_size, self.action_size)\r\n",
    "        self.actor          = Actor(self.state_size, self.action_size, self.action_min, self.action_max)\r\n",
    "        self.target_actor   = Actor(self.state_size, self.action_size, self.action_min, self.action_max)\r\n",
    "        self.log_alpha      = tf.math.log(0.2)\r\n",
    "        self.alpha          = tf.math.exp(self.log_alpha)\r\n",
    "\r\n",
    "        self.actor.build(input_shape=(None, self.state_size))\r\n",
    "        self.target_actor.build(input_shape=(None, self.state_size))\r\n",
    "        state_in = Input(shape=(self.state_size,),dtype=tf.float32)\r\n",
    "        action_in = Input(shape=(self.action_size,),dtype=tf.float32)\r\n",
    "        self.actor(state_in)\r\n",
    "        self.target_actor(state_in)\r\n",
    "        self.critic1(state_in, action_in)\r\n",
    "        self.critic2(state_in, action_in)\r\n",
    "        self.target_critic1(state_in, action_in)\r\n",
    "        self.target_critic2(state_in, action_in)\r\n",
    "\r\n",
    "        self.actor.summary()\r\n",
    "        self.critic1.summary()\r\n",
    "        # self.critic2.summary()\r\n",
    "        \r\n",
    "        self.hard_update_target_model()\r\n",
    "        # self.target_entropy = -tf.reduce_prod(tf.Tensor(self.action_size,dtype=tf.float32))\r\n",
    "\r\n",
    "        self.update_freq = 2\r\n",
    "        self.train_idx = 0\r\n",
    "        self.show_media_info = False\r\n",
    "\r\n",
    "    def remember(self, state, action, reward, next_state, done):\r\n",
    "        self.memory.append((state, action, reward, next_state, done))\r\n",
    "\r\n",
    "    def hard_update_target_model(self):\r\n",
    "        self.target_actor.set_weights(self.actor.get_weights())\r\n",
    "        self.target_critic1.set_weights(self.critic1.get_weights())\r\n",
    "        self.target_critic2.set_weights(self.critic2.get_weights())\r\n",
    "\r\n",
    "    def soft_update_target_model(self):\r\n",
    "        tau = self.tau\r\n",
    "        for (net, target_net) in zip(   self.actor.trainable_variables,\r\n",
    "                                        self.target_actor.trainable_variables):\r\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\r\n",
    "        for (net, target_net) in zip(   self.critic1.trainable_variables,\r\n",
    "                                        self.target_critic1.trainable_variables):\r\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\r\n",
    "        for (net, target_net) in zip(   self.critic2.trainable_variables,\r\n",
    "                                        self.target_critic2.trainable_variables):\r\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\r\n",
    "\r\n",
    "    def get_action(self, state):\r\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\r\n",
    "        mu, std = self.actor(state)\r\n",
    "        action, _ = self.eval_action(mu,std)\r\n",
    "        return action.numpy()[0]\r\n",
    "\r\n",
    "    def eval_action(self, mu, std, epsilon=1e-6):\r\n",
    "        action_prob = tfd.Normal(loc=mu, scale=std)\r\n",
    "        z = action_prob.sample()\r\n",
    "        action = tf.math.tanh(z)\r\n",
    "        # action = tf.stop_gradient(action)\r\n",
    "        log_prob = action_prob.log_prob(z) - tf.math.log(1.0 - tf.pow(action,2) + epsilon)\r\n",
    "        log_prob = tf.reduce_sum(log_prob, axis=-1, keepdims=True)\r\n",
    "\r\n",
    "        return action, log_prob\r\n",
    "\r\n",
    "    # def get_td_error(self,critic, target_critic):\r\n",
    "    #     # Update critic\r\n",
    "    #     mu, std = self.actor(next_states,training=True)\r\n",
    "    #     q = self.target_critic1(next_states,next_actions,training=True)\r\n",
    "    #     next_actions, log_pi = self.eval_action(mu, std)\r\n",
    "    #     target_q1 = self.target_critic1(next_states,next_actions,training=True)\r\n",
    "    #     target_q2 = self.target_critic2(next_states,next_actions,training=True)\r\n",
    "    #     target_q_min = tf.minimum(target_q1, target_q2) # Clipping Double Q\r\n",
    "    #     td_error = \r\n",
    "\r\n",
    "    def train_model(self):\r\n",
    "        # Train from Experience Replay\r\n",
    "        # Training Condition - Memory Size\r\n",
    "        if len(self.memory) < self.train_start:\r\n",
    "            return\r\n",
    "        # Sampling from the memory\r\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\r\n",
    "        \r\n",
    "        states      = tf.convert_to_tensor(np.array([sample[0] for sample in mini_batch]))\r\n",
    "        actions     = tf.convert_to_tensor(np.array([sample[1] for sample in mini_batch]))\r\n",
    "        rewards     = tf.convert_to_tensor(np.array([sample[2] for sample in mini_batch]),dtype=tf.float32)\r\n",
    "        rewards     = tf.expand_dims(rewards, axis = 1)\r\n",
    "        next_states = tf.convert_to_tensor(np.array([sample[3] for sample in mini_batch]))\r\n",
    "        dones       = tf.convert_to_tensor(np.array([sample[4] for sample in mini_batch]),dtype=tf.float32)\r\n",
    "        dones       = tf.expand_dims(dones, axis = 1)\r\n",
    "        \r\n",
    "        if self.show_media_info == False:\r\n",
    "            self.show_media_info = True\r\n",
    "            print('Start to train, check batch shapes')\r\n",
    "            print('**** shape of states', np.shape(states),type(states))\r\n",
    "            print('**** shape of actions', np.shape(actions),type(actions))\r\n",
    "            print('**** shape of rewards', np.shape(rewards),type(rewards))\r\n",
    "            print('**** shape of next_states', np.shape(next_states),type(next_states))\r\n",
    "            print('**** shape of dones', np.shape(dones),type(dones))\r\n",
    "        \r\n",
    "        # Update critic\r\n",
    "        mu, std = self.actor(next_states,training=True)\r\n",
    "        next_actions, next_log_pi = self.eval_action(mu, std)\r\n",
    "        target_q1 = self.target_critic1(next_states,next_actions,training=True)\r\n",
    "        target_q2 = self.target_critic2(next_states,next_actions,training=True)\r\n",
    "        target_q_min = tf.minimum(target_q1, target_q2) # Clipping Double Q\r\n",
    "\r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            target_value = rewards + (1.0 - dones) * self.discount_factor * (target_q_min - self.alpha * next_log_pi)\r\n",
    "            q = self.critic1(states, actions, training=True)\r\n",
    "            critic1_loss = tf.math.reduce_mean(tf.math.square(target_value - q))\r\n",
    "        critic1_params = self.critic1.trainable_variables\r\n",
    "        critic1_grads = tape.gradient(critic1_loss, critic1_params)\r\n",
    "        self.critic1_optimizer.apply_gradients(zip(critic1_grads, critic1_params))\r\n",
    "\r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            target_value = rewards + (1.0 - dones) * self.discount_factor * (target_q_min - self.alpha * next_log_pi)\r\n",
    "            q = self.critic2(states, actions, training=True)\r\n",
    "            critic2_loss = tf.math.reduce_mean(tf.math.square(target_value - q))\r\n",
    "        critic2_params = self.critic2.trainable_variables\r\n",
    "        critic2_grads = tape.gradient(critic2_loss, critic2_params)\r\n",
    "        self.critic2_optimizer.apply_gradients(zip(critic2_grads, critic2_params))\r\n",
    "\r\n",
    "        # Update actor\r\n",
    "        mu, std = self.actor(states,training=True)\r\n",
    "        new_actions, new_log_pi = self.eval_action(mu,std)\r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            new_q1 = self.critic1(states, new_actions,training=True)\r\n",
    "            new_q2 = self.critic2(states, new_actions,training=True)\r\n",
    "            new_q_min = tf.minimum(new_q1, new_q2)\r\n",
    "            # advantage = (new_log_pi - new_q_min)\r\n",
    "            # advantage = tf.stop_gradient(advantage)\r\n",
    "            # actor_loss = tf.reduce_mean(new_log_pi * advantage)\r\n",
    "            actor_loss = tf.reduce_mean(self.alpha * new_log_pi - new_q_min)\r\n",
    "        actor_params = self.actor.trainable_variables\r\n",
    "        actor_grads = tape.gradient(actor_loss, actor_params)\r\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, actor_params))\r\n",
    "\r\n",
    "        # Update alpha\r\n",
    "        # with tf.GradientTape() as tape:\r\n",
    "        #     alpha_loss = - tf.reduce_mean(self.log_alpha * (new_log_pi + self.target_entropy))\r\n",
    "        # alpha_params = self.log_alpha\r\n",
    "        # alpha_grads = tape.gradient(alpha_loss, alpha_params)\r\n",
    "        # self.alpha_optimizer.apply_gradients(zip(alpha_grads, alpha_params))\r\n",
    "        # self.alpha = tf.math.exp(self.log_alpha)\r\n",
    "\r\n",
    "        self.soft_update_target_model()\r\n",
    "        return\r\n",
    "\r\n",
    "    def save_model(self):\r\n",
    "        self.actor.save_weights(\"./save_model/pendulum_td3_TF_actor\", save_format=\"tf\")\r\n",
    "        self.critic1.save_weights(\"./save_model/pendulum_td3_TF_critic1\", save_format=\"tf\")\r\n",
    "        self.critic2.save_weights(\"./save_model/pendulum_td3_TF_critic2\", save_format=\"tf\")\r\n",
    "        return\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# %matplotlib tk\r\n",
    "\r\n",
    "ENV_NAME = 'Pendulum-v0'\r\n",
    "EPISODES = 300\r\n",
    "END_SCORE = -200\r\n",
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "    env = gym.make(ENV_NAME)\r\n",
    "    state_size      = env.observation_space.shape[0]\r\n",
    "    action_size     = env.action_space.shape[0]\r\n",
    "    log_std_min     = -20.0\r\n",
    "    log_std_max     = 5.0\r\n",
    "\r\n",
    "    agent = TD3Agent(state_size, action_size, log_std_min, log_std_max)\r\n",
    "    print('Env Name : ',ENV_NAME)\r\n",
    "    print('States {0}, Actions {1}'.format(state_size, action_size))\r\n",
    "    print('Action scale exp({0:.2f} ~ {1:.2f})'.format(log_std_min, log_std_max))\r\n",
    "    scores, episodes = [], []\r\n",
    "    score_avg = 0\r\n",
    "\r\n",
    "    end = False\r\n",
    "    show_media_info = True\r\n",
    "    \r\n",
    "    fig = plt.figure(1)\r\n",
    "    fig.clf()\r\n",
    "    \r\n",
    "    for e in range(EPISODES):\r\n",
    "        done = False\r\n",
    "        score = 0\r\n",
    "        state = env.reset()\r\n",
    "        while not done:\r\n",
    "            # env.render()\r\n",
    "\r\n",
    "            # Interact with env.\r\n",
    "            action = agent.get_action(state)\r\n",
    "            next_state, reward, done, info = env.step(action)\r\n",
    "            agent.remember(state, action, reward, next_state, done)\r\n",
    "            agent.train_model()\r\n",
    "            state = next_state\r\n",
    "            # \r\n",
    "            score += reward\r\n",
    "            if show_media_info:\r\n",
    "                print(\"-------------- Variable shapes --------------\")\r\n",
    "                print(\"State Shape : \", np.shape(state))\r\n",
    "                print(\"Action Shape : \", np.shape(action))\r\n",
    "                print(\"Reward Shape : \", np.shape(reward))\r\n",
    "                print(\"done Shape : \", np.shape(done))\r\n",
    "                print(\"---------------------------------------------\")\r\n",
    "                show_media_info = False\r\n",
    "            if done:\r\n",
    "                score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\r\n",
    "                print(\"episode: {0:3d} | score avg: {1:3.2f} | mem size {2:6d} |\"\r\n",
    "                    .format(e, score_avg, len(agent.memory)))\r\n",
    "\r\n",
    "                episodes.append(e)\r\n",
    "                scores.append(score_avg)\r\n",
    "\r\n",
    "                plt.plot(episodes, scores, 'b')\r\n",
    "                plt.xlabel('episode')\r\n",
    "                plt.ylabel('average score')\r\n",
    "                plt.title('pendulum DDPG')\r\n",
    "                plt.grid()\r\n",
    "                plt.savefig(\"./save_model/pendulum_sac_TF.png\")\r\n",
    "\r\n",
    "                # 이동 평균이 0 이상일 때 종료\r\n",
    "                if score_avg > END_SCORE:\r\n",
    "                    agent.save_model()\r\n",
    "                    end = True\r\n",
    "                    break\r\n",
    "                \r\n",
    "            # break\r\n",
    "        if end == True:\r\n",
    "            env.close()\r\n",
    "            np.save('./save_model/data/pendulum_sac_TF_epi',  episodes)\r\n",
    "            np.save('./save_model/data/pendulum_sac_TF_score',scores)\r\n",
    "            print(\"End\")\r\n",
    "            break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"actor_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_64 (Dense)             multiple                  256       \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             multiple                  65        \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             multiple                  65        \n",
      "=================================================================\n",
      "Total params: 4,546\n",
      "Trainable params: 4,546\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"critic_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             multiple                  64        \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             multiple                  544       \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             multiple                  64        \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             multiple                  1056      \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             multiple                  65        \n",
      "=================================================================\n",
      "Total params: 10,113\n",
      "Trainable params: 10,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Env Name :  Pendulum-v0\n",
      "States 3, Actions 1\n",
      "Action scale exp(-20.00 ~ 5.00)\n",
      "State Shape :  (3,)\n",
      "Action Shape :  (1,)\n",
      "Reward Shape :  ()\n",
      "done Shape :  ()\n",
      "episode:   0 | score avg: -1327.06 | mem size    200 |\n",
      "episode:   1 | score avg: -1331.56 | mem size    400 |\n",
      "episode:   2 | score avg: -1355.28 | mem size    600 |\n",
      "episode:   3 | score avg: -1323.63 | mem size    800 |\n",
      "episode:   4 | score avg: -1309.27 | mem size   1000 |\n",
      "episode:   5 | score avg: -1308.25 | mem size   1200 |\n",
      "episode:   6 | score avg: -1294.93 | mem size   1400 |\n",
      "episode:   7 | score avg: -1293.16 | mem size   1600 |\n",
      "episode:   8 | score avg: -1280.96 | mem size   1800 |\n",
      "Start to train, check batch shapes\n",
      "**** shape of states (64, 3) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of actions (64, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of rewards (64, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of next_states (64, 3) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of dones (64, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "episode:   9 | score avg: -1270.95 | mem size   2000 |\n",
      "episode:  10 | score avg: -1241.39 | mem size   2200 |\n",
      "episode:  11 | score avg: -1264.79 | mem size   2400 |\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env.close()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}