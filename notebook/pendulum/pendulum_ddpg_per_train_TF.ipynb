{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('TF240': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8f237aa33f5a133d3a67a1e00bf3cb9d47b6c38bcc6ab493273f5d4df41b8866"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Refer from\n",
    "#  https://pasus.tistory.com/138\n",
    "#  https://horomary.hatenablog.com/entry/2020/06/26/003806\n",
    "#  https://keras.io/examples/rl/ddpg_pendulum/\n",
    "#\n",
    "import gym\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, concatenate, Lambda\n",
    "from utils.prioritized_memory import PrioritizedMemory\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# https://keras.io/examples/rl/ddpg_pendulum/\r\n",
    "class OUActionNoise:\r\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\r\n",
    "        self.theta = theta\r\n",
    "        self.mean = mean\r\n",
    "        self.std_dev = std_deviation\r\n",
    "        self.dt = dt\r\n",
    "        self.x_initial = x_initial\r\n",
    "        self.reset()\r\n",
    "\r\n",
    "    def __call__(self):\r\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\r\n",
    "        x = (\r\n",
    "            self.x_prev\r\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\r\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\r\n",
    "        )\r\n",
    "        # Store x into x_prev\r\n",
    "        # Makes next noise dependent on current one\r\n",
    "        self.x_prev = x\r\n",
    "        return x\r\n",
    "\r\n",
    "    def reset(self):\r\n",
    "        if self.x_initial is not None:\r\n",
    "            self.x_prev = self.x_initial\r\n",
    "        else:\r\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class Actor(tf.keras.Model):\r\n",
    "    def __init__(self, state_size, action_size, action_min, action_max):\r\n",
    "        super(Actor, self).__init__()\r\n",
    "        self.action_min = action_min\r\n",
    "        self.action_max = action_max\r\n",
    "\r\n",
    "        self.fc1 = Dense(64, activation='relu')\r\n",
    "        self.fc2 = Dense(64, activation='relu')\r\n",
    "        # self.fc3 = Dense(16, activation='relu')\r\n",
    "        self.out= Dense(action_size, activation='tanh',kernel_initializer = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)) # -1 ~ +1\r\n",
    "\r\n",
    "    def call(self, x):\r\n",
    "        x       = self.fc1(x)\r\n",
    "        x       = self.fc2(x)\r\n",
    "        # x       = self.fc3(x)\r\n",
    "        action  = self.out(x)\r\n",
    "        # return self.projected_to_action_space(action)\r\n",
    "        a = Lambda(lambda x: x*self.action_max)(action)\r\n",
    "        return a\r\n",
    "\r\n",
    "class Critic(tf.keras.Model):\r\n",
    "    def __init__(self, state_size, action_size):\r\n",
    "        super(Critic, self).__init__()\r\n",
    "        self.s1 = Dense(16, activation='relu')\r\n",
    "        self.s2 = Dense(32, activation='relu')\r\n",
    "        self.a1 = Dense(32, activation='relu')\r\n",
    "        self.a2 = Dense(32, activation='relu')\r\n",
    "        self.fc1= Dense(64, activation='relu')\r\n",
    "        self.fc2= Dense(64, activation='relu')\r\n",
    "        self.out= Dense(1,  activation='linear')\r\n",
    "\r\n",
    "    def call(self,state,action):\r\n",
    "        # state  = state_action[0]\r\n",
    "        # action = state_action[1]\r\n",
    "        s = self.s1(state)\r\n",
    "        s = self.s2(s)\r\n",
    "        a = self.a1(action)\r\n",
    "        a = self.a2(a)\r\n",
    "        c = concatenate([s,a],axis=-1)\r\n",
    "        x = self.fc1(c)\r\n",
    "        x = self.fc2(x)\r\n",
    "        q = self.out(x)\r\n",
    "        return q"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self, state_size, action_size, action_min, action_max):\n",
    "        self.state_size = state_size\n",
    "        self.action_size= action_size\n",
    "        self.action_min = action_min\n",
    "        self.action_max = action_max\n",
    "\n",
    "        # Hyper params for learning\n",
    "        self.discount_factor = 0.99\n",
    "        self.actor_learning_rate  = 0.001\n",
    "        self.critic_learning_rate = 0.002\n",
    "        self.tau = 0.005\n",
    "\n",
    "        # Experience Replay\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 2000\n",
    "        self.memory_size = 50000\n",
    "        # ER\n",
    "        # self.memory = deque(maxlen=self.memory_size)\n",
    "        # PER\n",
    "        self.memory = PrioritizedMemory(capacity=self.memory_size)\n",
    "        # HER\n",
    "\n",
    "\n",
    "        # self.critic         = get_critic(self.state_size, self.action_size)\n",
    "        # self.target_critic  = get_critic(self.state_size, self.action_size)\n",
    "        # self.actor          = get_actor(self.state_size, self.action_size, self.action_max)\n",
    "        # self.target_actor   = get_actor(self.state_size, self.action_size, self.action_max)\n",
    "        self.critic_optimizer   = tf.keras.optimizers.Adam(lr=self.critic_learning_rate)\n",
    "        self.actor_optimizer    = tf.keras.optimizers.Adam(lr=self.actor_learning_rate)\n",
    "\n",
    "        self.critic         = Critic(self.state_size, self.action_size)\n",
    "        self.target_critic  = Critic(self.state_size, self.action_size)\n",
    "        self.actor          = Actor(self.state_size, self.action_size, self.action_min, self.action_max)\n",
    "        self.target_actor   = Actor(self.state_size, self.action_size, self.action_min, self.action_max)\n",
    "        self.actor.build(input_shape=(None, self.state_size))\n",
    "        self.target_actor.build(input_shape=(None, self.state_size))\n",
    "        state_in = Input((self.state_size,))\n",
    "        action_in = Input((self.action_size,))\n",
    "        self.actor(state_in)\n",
    "        self.target_actor(state_in)\n",
    "        self.critic(state_in, action_in)\n",
    "        self.target_critic(state_in, action_in)\n",
    "        self.actor.summary()\n",
    "        self.critic.summary()\n",
    "        \n",
    "        self.target_actor.set_weights(self.actor.get_weights())\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\n",
    "\n",
    "        std_dev = 0.1\n",
    "        self.ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "        \n",
    "        self.show_media_info = False\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        transition = (state, action, reward, next_state, done)\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        action = tf.convert_to_tensor([action], dtype=tf.float32)\n",
    "        next_state = tf.convert_to_tensor([next_state], dtype=tf.float32)\n",
    "        # ER\n",
    "        # self.memory.append(transition)\n",
    "        # PER\n",
    "        target_action = self.target_actor(next_state)\n",
    "        target_q = self.target_critic(next_state,target_action)\n",
    "        target_value = reward + (1 - done) * self.discount_factor * target_q\n",
    "        q = self.critic(state, action)\n",
    "        td_error = target_value - q\n",
    "        self.memory.add(td_error[0][0], transition)\n",
    "\n",
    "    def update_target_model(self):\n",
    "        tau = self.tau\n",
    "        for (net, target_net) in zip(   self.actor.trainable_variables,\n",
    "                                        self.target_actor.trainable_variables):\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\n",
    "        for (net, target_net) in zip(   self.critic.trainable_variables,\n",
    "                                        self.target_critic.trainable_variables):\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\n",
    "\n",
    "    def get_action(self,state):\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        action = self.actor(state)\n",
    "        # Exploration and Exploitation\n",
    "        action_from_net = action.numpy()[0]\n",
    "        action_from_noise = self.ou_noise()\n",
    "        return np.clip(action_from_net+action_from_noise,self.action_min,self.action_max)\n",
    "\n",
    "    def train_model(self):\n",
    "        # Train from Experience Replay\n",
    "        # Training Condition - Memory Size\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        # Sampling from the memory\n",
    "        # ER\n",
    "        # mini_batch = random.sample(self.memory, self.batch_size)\n",
    "        # PER\n",
    "        mini_batch, idxs, is_weights = self.memory.sample(self.batch_size)\n",
    "        is_weights = tf.convert_to_tensor([is_weights],dtype=tf.float32)\n",
    "        states      = tf.convert_to_tensor(np.array([sample[0] for sample in mini_batch]))\n",
    "        actions     = tf.convert_to_tensor(np.array([sample[1] for sample in mini_batch]))\n",
    "        rewards     = tf.convert_to_tensor(np.array([sample[2] for sample in mini_batch]),dtype=tf.float32)\n",
    "        rewards     = tf.expand_dims(rewards, axis = 1)\n",
    "        next_states = tf.convert_to_tensor(np.array([sample[3] for sample in mini_batch]))\n",
    "        dones       = tf.convert_to_tensor(np.array([sample[4] for sample in mini_batch]),dtype=tf.float32)\n",
    "        dones       = tf.expand_dims(dones, axis = 1)\n",
    "        \n",
    "        if self.show_media_info == False:\n",
    "            self.show_media_info = True\n",
    "            print('Start to train, check batch shapes')\n",
    "            print('shape of states', np.shape(states),type(states))\n",
    "            print('shape of actions', np.shape(actions),type(actions))\n",
    "            print('shape of next_states', np.shape(next_states),type(next_states)) \n",
    "            print('shape of dones', np.shape(dones),type(dones))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor(next_states,training=True)\n",
    "            target_q = self.target_critic(next_states,target_actions,training=True)\n",
    "            target_value = rewards + (1 - dones) * self.discount_factor * target_q\n",
    "            q = self.critic(states, actions,training=True)\n",
    "            td_error = target_value - q\n",
    "            critic_loss = is_weights * tf.math.reduce_mean(tf.math.square(target_value - q))\n",
    "        critic_params = self.critic.trainable_variables\n",
    "        critic_grads = tape.gradient(critic_loss, critic_params)\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grads, critic_params))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            new_actions = self.actor(states,training=True)\n",
    "            new_q = self.critic(states, new_actions,training=True)\n",
    "            actor_loss = -tf.reduce_mean(new_q)\n",
    "        actor_params = self.actor.trainable_variables\n",
    "        actor_grads = tape.gradient(actor_loss, actor_params)\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, actor_params))\n",
    "        \n",
    "        self.update_target_model()\n",
    "        return\n",
    "\n",
    "    def save_model(self):\n",
    "        self.actor.save_weights(\"./save_model/pendulum_ddpg_per_TF_actor\", save_format=\"tf\")\n",
    "        self.critic.save_weights(\"./save_model/pendulum_ddpg_per_TF_critic\", save_format=\"tf\")\n",
    "        return\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# %matplotlib tk\n",
    "\n",
    "ENV_NAME = 'Pendulum-v0'\n",
    "EPISODES = 200\n",
    "END_SCORE = -200\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(ENV_NAME)\n",
    "    state_size  = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.shape[0]\n",
    "    action_min  = env.action_space.low[0]\n",
    "    action_max  = env.action_space.high[0]\n",
    "\n",
    "    agent = DDPGAgent(state_size, action_size, action_min, action_max)\n",
    "    print('Env Name : ',ENV_NAME)\n",
    "    print('States {0}, Actions {1}'.format(state_size, action_size))\n",
    "    print('Action space {0:.2f} ~ {1:.2f}'.format(action_min, action_max))\n",
    "    scores, episodes = [], []\n",
    "    score_avg = 0\n",
    "\n",
    "    end = False\n",
    "    show_media_info = True\n",
    "    \n",
    "    fig = plt.figure(1)\n",
    "    fig.clf()\n",
    "    \n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        while not done:\n",
    "            # env.render()\n",
    "\n",
    "            # Interact with env.\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            agent.train_model()\n",
    "            state = next_state\n",
    "\n",
    "            # \n",
    "            score += reward\n",
    "            if show_media_info:\n",
    "                print(\"State Shape : \", np.shape(state))\n",
    "                print(\"Action Shape : \", np.shape(action))\n",
    "                print(\"Reward Shape : \", np.shape(reward))\n",
    "                print(\"done Shape : \", np.shape(done))\n",
    "                show_media_info = False\n",
    "            if done:\n",
    "                score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\n",
    "                print(\"episode: {0:3d} | score avg: {1:3.2f} | mem size {2:6d} |\"\n",
    "                    .format(e, score_avg, len(agent.memory)))\n",
    "\n",
    "                episodes.append(e)\n",
    "                scores.append(score_avg)\n",
    "\n",
    "                plt.plot(episodes, scores, 'b')\n",
    "                plt.xlabel('episode')\n",
    "                plt.ylabel('average score')\n",
    "                plt.title('pendulum DDPG')\n",
    "                plt.grid()\n",
    "                plt.savefig(\"./save_model/pendulum_ddpg_per_TF.png\")\n",
    "\n",
    "                # 이동 평균이 0 이상일 때 종료\n",
    "                if score_avg > END_SCORE:\n",
    "                    agent.save_model()\n",
    "                    end = True\n",
    "                    break\n",
    "        if end == True:\n",
    "            env.close()\n",
    "            np.save('./save_model/data/pendulum_ddpg_per_TF_epi',  episodes)\n",
    "            np.save('./save_model/data/pendulum_ddpg_per_TF_score',scores)\n",
    "            print(\"End\")\n",
    "            break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"actor_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_34 (Dense)             multiple                  256       \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             multiple                  65        \n",
      "=================================================================\n",
      "Total params: 4,481\n",
      "Trainable params: 4,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"critic_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             multiple                  64        \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             multiple                  544       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             multiple                  64        \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             multiple                  1056      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             multiple                  65        \n",
      "=================================================================\n",
      "Total params: 10,113\n",
      "Trainable params: 10,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Env Name :  Pendulum-v0\n",
      "States 3, Actions 1\n",
      "Action space -2.00 ~ 2.00\n",
      "State Shape :  (3,)\n",
      "Action Shape :  (1,)\n",
      "Reward Shape :  ()\n",
      "done Shape :  ()\n",
      "episode:   0 | score avg: -1098.40 | mem size    200 |\n",
      "episode:   1 | score avg: -1063.02 | mem size    400 |\n",
      "episode:   2 | score avg: -1088.32 | mem size    600 |\n",
      "episode:   3 | score avg: -1108.47 | mem size    800 |\n",
      "episode:   4 | score avg: -1120.85 | mem size   1000 |\n",
      "episode:   5 | score avg: -1148.13 | mem size   1200 |\n",
      "episode:   6 | score avg: -1173.72 | mem size   1400 |\n",
      "episode:   7 | score avg: -1153.20 | mem size   1600 |\n",
      "episode:   8 | score avg: -1124.11 | mem size   1800 |\n",
      "Start to train, check batch shapes\n",
      "shape of states (64, 3) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "shape of actions (64, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "shape of next_states (64, 3) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "shape of dones (64, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "episode:   9 | score avg: -1199.33 | mem size   2000 |\n",
      "episode:  10 | score avg: -1196.11 | mem size   2200 |\n",
      "episode:  11 | score avg: -1195.98 | mem size   2400 |\n",
      "episode:  12 | score avg: -1206.98 | mem size   2600 |\n",
      "episode:  13 | score avg: -1211.69 | mem size   2800 |\n",
      "episode:  14 | score avg: -1230.86 | mem size   3000 |\n",
      "episode:  15 | score avg: -1239.73 | mem size   3200 |\n",
      "episode:  16 | score avg: -1270.33 | mem size   3400 |\n",
      "episode:  17 | score avg: -1295.61 | mem size   3600 |\n",
      "episode:  18 | score avg: -1317.50 | mem size   3800 |\n",
      "episode:  19 | score avg: -1321.60 | mem size   4000 |\n",
      "episode:  20 | score avg: -1324.32 | mem size   4200 |\n",
      "episode:  21 | score avg: -1334.36 | mem size   4400 |\n",
      "episode:  22 | score avg: -1299.06 | mem size   4600 |\n",
      "episode:  23 | score avg: -1334.14 | mem size   4800 |\n",
      "episode:  24 | score avg: -1313.91 | mem size   5000 |\n",
      "episode:  25 | score avg: -1315.63 | mem size   5200 |\n",
      "episode:  26 | score avg: -1337.22 | mem size   5400 |\n",
      "episode:  27 | score avg: -1365.19 | mem size   5600 |\n",
      "episode:  28 | score avg: -1370.98 | mem size   5800 |\n",
      "episode:  29 | score avg: -1386.78 | mem size   6000 |\n",
      "episode:  30 | score avg: -1398.64 | mem size   6200 |\n",
      "episode:  31 | score avg: -1377.88 | mem size   6400 |\n",
      "episode:  32 | score avg: -1346.63 | mem size   6600 |\n",
      "episode:  33 | score avg: -1349.39 | mem size   6800 |\n",
      "episode:  34 | score avg: -1321.27 | mem size   7000 |\n",
      "episode:  35 | score avg: -1343.75 | mem size   7200 |\n",
      "episode:  36 | score avg: -1320.01 | mem size   7400 |\n",
      "episode:  37 | score avg: -1337.10 | mem size   7600 |\n",
      "episode:  38 | score avg: -1323.84 | mem size   7800 |\n",
      "episode:  39 | score avg: -1286.22 | mem size   8000 |\n",
      "episode:  40 | score avg: -1307.65 | mem size   8200 |\n",
      "episode:  41 | score avg: -1278.56 | mem size   8400 |\n",
      "episode:  42 | score avg: -1300.89 | mem size   8600 |\n",
      "episode:  43 | score avg: -1288.96 | mem size   8800 |\n",
      "episode:  44 | score avg: -1279.97 | mem size   9000 |\n",
      "episode:  45 | score avg: -1301.25 | mem size   9200 |\n",
      "episode:  46 | score avg: -1316.24 | mem size   9400 |\n",
      "episode:  47 | score avg: -1334.30 | mem size   9600 |\n",
      "episode:  48 | score avg: -1302.85 | mem size   9800 |\n",
      "episode:  49 | score avg: -1272.28 | mem size  10000 |\n",
      "episode:  50 | score avg: -1248.89 | mem size  10200 |\n",
      "episode:  51 | score avg: -1275.80 | mem size  10400 |\n",
      "episode:  52 | score avg: -1302.44 | mem size  10600 |\n",
      "episode:  53 | score avg: -1317.26 | mem size  10800 |\n",
      "episode:  54 | score avg: -1307.19 | mem size  11000 |\n",
      "episode:  55 | score avg: -1337.73 | mem size  11200 |\n",
      "episode:  56 | score avg: -1365.25 | mem size  11400 |\n",
      "episode:  57 | score avg: -1337.17 | mem size  11600 |\n",
      "episode:  58 | score avg: -1299.83 | mem size  11800 |\n",
      "episode:  59 | score avg: -1296.53 | mem size  12000 |\n",
      "episode:  60 | score avg: -1270.84 | mem size  12200 |\n",
      "episode:  61 | score avg: -1298.98 | mem size  12400 |\n",
      "episode:  62 | score avg: -1252.94 | mem size  12600 |\n",
      "episode:  63 | score avg: -1216.81 | mem size  12800 |\n",
      "episode:  64 | score avg: -1221.56 | mem size  13000 |\n",
      "episode:  65 | score avg: -1253.48 | mem size  13200 |\n",
      "episode:  66 | score avg: -1211.48 | mem size  13400 |\n",
      "episode:  67 | score avg: -1242.10 | mem size  13600 |\n",
      "episode:  68 | score avg: -1268.45 | mem size  13800 |\n",
      "episode:  69 | score avg: -1238.73 | mem size  14000 |\n",
      "episode:  70 | score avg: -1192.92 | mem size  14200 |\n",
      "episode:  71 | score avg: -1134.89 | mem size  14400 |\n",
      "episode:  72 | score avg: -1082.42 | mem size  14600 |\n",
      "episode:  73 | score avg: -1076.42 | mem size  14800 |\n",
      "episode:  74 | score avg: -1035.31 | mem size  15000 |\n",
      "episode:  75 | score avg: -991.94 | mem size  15200 |\n",
      "episode:  76 | score avg: -893.02 | mem size  15400 |\n",
      "episode:  77 | score avg: -864.73 | mem size  15600 |\n",
      "episode:  78 | score avg: -910.77 | mem size  15800 |\n",
      "episode:  79 | score avg: -951.87 | mem size  16000 |\n",
      "episode:  80 | score avg: -934.40 | mem size  16200 |\n",
      "episode:  81 | score avg: -865.51 | mem size  16400 |\n",
      "episode:  82 | score avg: -909.67 | mem size  16600 |\n",
      "episode:  83 | score avg: -881.42 | mem size  16800 |\n",
      "episode:  84 | score avg: -945.05 | mem size  17000 |\n",
      "episode:  85 | score avg: -851.39 | mem size  17200 |\n",
      "episode:  86 | score avg: -916.07 | mem size  17400 |\n",
      "episode:  87 | score avg: -896.77 | mem size  17600 |\n",
      "episode:  88 | score avg: -868.07 | mem size  17800 |\n",
      "episode:  89 | score avg: -831.16 | mem size  18000 |\n",
      "episode:  90 | score avg: -808.84 | mem size  18200 |\n",
      "episode:  91 | score avg: -879.58 | mem size  18400 |\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-34bdff2fa2d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-39d370b79b5b>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[0mtarget_q\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_critic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_actions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[0mtarget_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscount_factor\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtarget_q\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m             \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m             \u001b[0mtd_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_value\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[0mcritic_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_value\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[1;32m-> 1012\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1013\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-1ddca939835d>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ma2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_autocast\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 996\u001b[1;33m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m       \u001b[0minput_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_maybe_cast_inputs\u001b[1;34m(self, inputs, input_list)\u001b[0m\n\u001b[0;32m   2427\u001b[0m         any(map(self._should_cast_single_input, input_list))):\n\u001b[0;32m   2428\u001b[0m       \u001b[1;31m# Only perform expensive `nest` operation when needed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2429\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cast_single_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2430\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2431\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 659\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 659\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_cast_single_input\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   2440\u001b[0m     \u001b[1;34m\"\"\"Cast a single Tensor or TensorSpec to the compute dtype.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2441\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_cast_single_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2442\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2443\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2444\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[1;34m(x, dtype, name)\u001b[0m\n\u001b[0;32m    964\u001b[0m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"x\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    965\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 966\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    967\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    968\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Casting complex to real discards imaginary part.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\TF240\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[1;34m(x, DstT, Truncate, name)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m-> 1823\u001b[1;33m         _ctx, \"Cast\", name, x, \"DstT\", DstT, \"Truncate\", Truncate)\n\u001b[0m\u001b[0;32m   1824\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "env.close()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}