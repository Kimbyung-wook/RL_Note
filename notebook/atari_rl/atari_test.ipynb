{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Refer from\r\n",
    "# https://keras.io/examples/rl/deep_q_network_breakout/\r\n",
    "import gym\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras import Model\r\n",
    "from tensorflow.keras.layers import Dense, Input, ReLU, Conv2D, Flatten\r\n",
    "from tensorflow.keras.optimizers import Adam\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from ER import ReplayMemory\r\n",
    "from PER import ProportionalPrioritizedMemory\r\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\r\n",
    "if gpus:\r\n",
    "  try:\r\n",
    "    tf.config.experimental.set_virtual_device_configuration(\\\r\n",
    "        gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*3)])\r\n",
    "  except RuntimeError as e:\r\n",
    "    print(e)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_q_network(observation_space, action_space):\r\n",
    "  # Input shape is expected as (84,84,4)\r\n",
    "  X_input = Input(shape=observation_space)\r\n",
    "  # Convolution Layers\r\n",
    "  X = X_input\r\n",
    "  X = Conv2D(filters=32, kernel_size=8, strides=4, padding='valid', activation=\"relu\", data_format='channels_last')(X)\r\n",
    "  X = Conv2D(filters=64, kernel_size=4, strides=2, padding='valid', activation=\"relu\", data_format='channels_last')(X)\r\n",
    "  X = Conv2D(filters=64, kernel_size=3, strides=1, padding='valid', activation=\"relu\", data_format='channels_last')(X)\r\n",
    "  X = Flatten()(X)\r\n",
    "  X = Dense(units=512,          activation='relu',   kernel_initializer='he_uniform')(X)\r\n",
    "  X = Dense(units=action_space, activation='linear', kernel_initializer='he_uniform')(X)\r\n",
    "  model = Model(inputs=X_input, outputs=X)\r\n",
    "  model.build(input_shape=observation_space)\r\n",
    "\r\n",
    "  return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class DQNAgent():\r\n",
    "  def __init__(self, env, cfg):\r\n",
    "    self.env_cfg    = cfg['ENV']\r\n",
    "    self.rl_cfg     = cfg['RL']\r\n",
    "    self.er_cfg     = cfg['RL']['ER']\r\n",
    "    self.er_type    = self.er_cfg[\"ALGORITHM\"].upper()\r\n",
    "    self.img_size   = self.env_cfg['IMG_SIZE']\r\n",
    "    self.state_size = self.env_cfg['IMG_SIZE']\r\n",
    "    self.action_size= env.action_space.n\r\n",
    "\r\n",
    "    # Hyper-parameters for learning\r\n",
    "    self.discount_factor = 0.99\r\n",
    "    self.learning_rate  = 0.005\r\n",
    "    self.epsilon        = 1.0\r\n",
    "    self.epsilon_decay  = 0.999\r\n",
    "    self.epsilon_min    = 0.1\r\n",
    "    self.tau            = 0.005\r\n",
    "    self.start_to_train = self.er_cfg[\"TRAIN_START\"]\r\n",
    "    self.batch_size     = self.er_cfg[\"BATCH_SIZE\"]\r\n",
    "    self.buffer_size    = self.er_cfg[\"MEMORY_SIZE\"]\r\n",
    "    self.update_freq    = self.rl_cfg['UPDATE_FREQ']\r\n",
    "    self.train_freq     = self.rl_cfg['TRAIN_FREQ']\r\n",
    "\r\n",
    "    # DQN Architecture\r\n",
    "    self.model        = get_q_network(self.state_size, self.action_size)\r\n",
    "    self.target_model = get_q_network(self.state_size, self.action_size)\r\n",
    "    self.optimizer    = Adam(learning_rate=self.learning_rate, clipnorm=1.0)\r\n",
    "    self.model.summary()\r\n",
    "\r\n",
    "    # Experience Replay\r\n",
    "    if self.er_type == \"ER\":\r\n",
    "      self.memory = ReplayMemory(capacity=self.buffer_size)\r\n",
    "    elif self.er_type == \"PER\":\r\n",
    "      self.memory = ProportionalPrioritizedMemory(capacity=self.buffer_size)\r\n",
    "\r\n",
    "    # Miscellaneous\r\n",
    "    self.show_media_info = False\r\n",
    "    self.steps = 0\r\n",
    "    \r\n",
    "  def get_actions(self, state):\r\n",
    "    self.steps += 1\r\n",
    "    # Exploration and Exploitation\r\n",
    "    if ((np.random.rand() <= self.epsilon) or ()):\r\n",
    "      return random.randrange(self.action_size)\r\n",
    "    else:\r\n",
    "      state = tf.convert_to_tensor([state], dtype=tf.float32)\r\n",
    "      return np.argmax(self.model(state))\r\n",
    "\r\n",
    "  def remember(self, state, action, reward, next_state, done):\r\n",
    "    state       = np.array(state,       dtype=np.float32)\r\n",
    "    action      = np.array([action])\r\n",
    "    reward      = np.array([reward],    dtype=np.float32)\r\n",
    "    done        = np.array([done],      dtype=np.float32)\r\n",
    "    next_state  = np.array(next_state,  dtype=np.float32)\r\n",
    "    transition  = (state, action, reward, next_state, done)\r\n",
    "    self.memory.append(transition)\r\n",
    "    return\r\n",
    "\r\n",
    "  def train(self):\r\n",
    "    if self.steps < self.start_to_train:\r\n",
    "      return 0.0\r\n",
    "    # Sampling from the memory\r\n",
    "    if self.steps % self.train_freq == 0:\r\n",
    "      return 0.0\r\n",
    "    # Decaying Exploration Ratio\r\n",
    "    if self.epsilon > self.epsilon_min:\r\n",
    "        self.epsilon *= self.epsilon_decay\r\n",
    "    if self.er_type == \"ER\":\r\n",
    "      mini_batch = self.memory.sample(self.batch_size)\r\n",
    "    elif self.er_type == \"PER\":\r\n",
    "      mini_batch, idxs, is_weights = self.memory.sample(self.batch_size)\r\n",
    "\r\n",
    "    states      = tf.convert_to_tensor(np.array([sample[0] for sample in mini_batch]))\r\n",
    "    actions     = tf.convert_to_tensor(np.array([sample[1][0] for sample in mini_batch]))\r\n",
    "    rewards     = tf.convert_to_tensor(np.array([sample[2] for sample in mini_batch]))\r\n",
    "    next_states = tf.convert_to_tensor(np.array([sample[3] for sample in mini_batch]))\r\n",
    "    dones       = tf.convert_to_tensor(np.array([sample[4] for sample in mini_batch]))\r\n",
    "    \r\n",
    "    if self.show_media_info == False:\r\n",
    "      self.show_media_info = True\r\n",
    "      print('Start to train, check batch shapes')\r\n",
    "      print('**** shape of mini_batch', np.shape(mini_batch),type(mini_batch))\r\n",
    "      print('**** shape of states', np.shape(states),type(states))\r\n",
    "      print('**** shape of actions', np.shape(actions),type(actions))\r\n",
    "      print('**** shape of rewards', np.shape(rewards),type(rewards))\r\n",
    "      print('**** shape of next_states', np.shape(next_states),type(next_states))\r\n",
    "      print('**** shape of dones', np.shape(dones),type(dones))\r\n",
    "\r\n",
    "    model_params = self.model.trainable_variables\r\n",
    "    with tf.GradientTape() as tape:\r\n",
    "      # get q value\r\n",
    "      q = self.model(states)\r\n",
    "      one_hot_action = tf.one_hot(actions, self.action_size)\r\n",
    "      q = tf.reduce_sum(one_hot_action * q, axis=1)\r\n",
    "      q = tf.expand_dims(q,axis=1)\r\n",
    "      # Target q and maximum target q\r\n",
    "      target_q = tf.stop_gradient(self.target_model(next_states))\r\n",
    "      max_q = tf.reduce_max(target_q,axis=1)\r\n",
    "      max_q = tf.expand_dims(max_q,axis=1)\r\n",
    "      \r\n",
    "      targets = rewards + (1 - dones) * self.discount_factor * max_q\r\n",
    "      td_error = targets - q\r\n",
    "      if self.er_type == \"PER\":\r\n",
    "        loss = tf.reduce_mean(is_weights * tf.square(targets - q))\r\n",
    "      else:\r\n",
    "        loss = tf.reduce_mean(tf.square(targets - q))\r\n",
    "        \r\n",
    "    grads = tape.gradient(loss, model_params)\r\n",
    "    self.optimizer.apply_gradients(zip(grads, model_params))\r\n",
    "\r\n",
    "    if self.er_type == \"PER\":\r\n",
    "      sample_importance = td_error.numpy()\r\n",
    "      for i in range(self.batch_size):\r\n",
    "        self.memory.update(idxs[i], sample_importance[i])\r\n",
    "\r\n",
    "    return loss\r\n",
    "\r\n",
    "  def update_target_net(self):\r\n",
    "    if self.steps % self.update_freq == 0:\r\n",
    "      self.target_model.set_weights(self.model.get_weights())\r\n",
    "    return\r\n",
    "\r\n",
    "  def load_model(self,at):\r\n",
    "    self.model.load_weights( at + self.filename + \"_TF\")\r\n",
    "    self.target_model.load_weights(at + self.filename + \"_TF\")\r\n",
    "    return\r\n",
    "\r\n",
    "  def save_model(self,at):\r\n",
    "    self.model.save_weights( at + self.filename + \"_TF\", save_format=\"tf\")\r\n",
    "    self.target_model.save_weights(at + self.filename + \"_TF\", save_format=\"tf\")\r\n",
    "    return"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import cv2\r\n",
    "class Featurization():\r\n",
    "  def __init__(self, observation_size):\r\n",
    "    self.obs_size = observation_size\r\n",
    "    self.is_first = True\r\n",
    "    self.feature = np.zeros(observation_size)\r\n",
    "\r\n",
    "  def preprocessing(self, img):\r\n",
    "    img_rgb_resize = cv2.resize(img, self.obs_size[0:2], interpolation=cv2.INTER_CUBIC)\r\n",
    "    img_rgb_resize = np.transpose(img_rgb_resize,axes=(1,0,2))\r\n",
    "    img_k_resize = cv2.cvtColor(img_rgb_resize,cv2.COLOR_RGB2GRAY)\r\n",
    "    img_k_resize = img_k_resize / 255.0 # scaling 0 ~ 1\r\n",
    "    state = np.array(img_k_resize,dtype=np.float32)\r\n",
    "    state = np.expand_dims(state,axis=2)\r\n",
    "    if self.is_first == True:\r\n",
    "      for i in range(self.obs_size[2]):\r\n",
    "        self.feature = np.append(self.feature, state, axis=2)\r\n",
    "        self.feature = np.delete(self.feature, obj=0, axis=2)\r\n",
    "      self.is_first = False\r\n",
    "    else:\r\n",
    "      self.feature = np.append(self.feature, state, axis=2)\r\n",
    "      self.feature = np.delete(self.feature, obj=0, axis=2)\r\n",
    "    return self.feature"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scores_avg, scores_raw, epsilons, losses, loss_list, score_avg, end = [], [], [], [], [], 0, False\r\n",
    "FILENAME = \"BreakoutDeterministic-v4_DQN\"\r\n",
    "def save_statistics():\r\n",
    "    # View data\r\n",
    "    plt.clf()\r\n",
    "    plt.subplot(311)\r\n",
    "    plt.plot(scores_avg, 'b')\r\n",
    "    plt.plot(scores_raw, 'b', alpha=0.8, linewidth=0.5)\r\n",
    "    plt.xlabel('Episodes'); plt.ylabel('average score'); plt.grid()\r\n",
    "    plt.title(FILENAME)\r\n",
    "    plt.subplot(312)\r\n",
    "    plt.plot(epsilons, 'b')\r\n",
    "    plt.xlabel('Episodes'); plt.ylabel('epsilon'); plt.grid()\r\n",
    "    plt.subplot(313)\r\n",
    "    plt.plot(losses, 'b')\r\n",
    "    plt.xlabel('Episodes'); plt.ylabel('losses') ;plt.grid()\r\n",
    "    plt.savefig(FILENAME + \"_TF.jpg\", dpi=100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib tk\r\n",
    "\r\n",
    "EPISODES = 10000\r\n",
    "MAX_STEP_PER_EPISODE = 10000\r\n",
    "END_SCORE = 40\r\n",
    "SAVE_FREQ = 10\r\n",
    "cfg = {\\\r\n",
    "  \"ENV\":{\r\n",
    "    \"NAME\":\"BreakoutDeterministic-v4\",\r\n",
    "    \"IMG_SIZE\":(84,84,4)\r\n",
    "  },\r\n",
    "  \"RL\":{\r\n",
    "    \"ALGORITHM\":'DQN',\r\n",
    "    \"ER\":{\r\n",
    "      \"ALGORITHM\":'ER',\r\n",
    "      \"BATCH_SIZE\":64,\r\n",
    "      \"TRAIN_START\":20000,\r\n",
    "      \"MEMORY_SIZE\":200000,\r\n",
    "    },\r\n",
    "    \"TRAIN_FREQ\":4,\r\n",
    "    \"UPDATE_FREQ\":1000,\r\n",
    "  },\r\n",
    "}\r\n",
    "ENV_NAME = cfg['ENV']['NAME']\r\n",
    "if __name__ == \"__main__\":\r\n",
    "  env = gym.make(ENV_NAME)\r\n",
    "  print('States ',env.observation_space, env.observation_space.shape,', Actions ', env.action_space, env.action_space.n)\r\n",
    "  agent = DQNAgent(env, cfg)\r\n",
    "  featurization = Featurization(cfg['ENV']['IMG_SIZE'])\r\n",
    "  global_steps = 0\r\n",
    "  for e in range(EPISODES):\r\n",
    "    observe = env.reset()\r\n",
    "    feature = featurization.preprocessing(observe)\r\n",
    "    episode_score = 0\r\n",
    "    episode_step = 0\r\n",
    "    loss_list = []\r\n",
    "    while True:\r\n",
    "      # obs = env.render(mode='human')\r\n",
    "      # action = env.action_space.sample()\r\n",
    "      action = agent.get_actions(feature)\r\n",
    "      observe, reward, done, info = env.step(action=action)\r\n",
    "      next_feature = featurization.preprocessing(observe)\r\n",
    "      agent.remember(feature, action, reward, next_feature, done)\r\n",
    "      loss = agent.train()\r\n",
    "      agent.update_target_net()\r\n",
    "\r\n",
    "      episode_score += reward\r\n",
    "      episode_step += 1\r\n",
    "      global_steps += 1\r\n",
    "      feature = next_feature\r\n",
    "      loss_list.append(loss)\r\n",
    "      # break\r\n",
    "      if (done == True) or (episode_step > MAX_STEP_PER_EPISODE):\r\n",
    "        score_avg = 0.9 * score_avg + 0.1 * episode_score if score_avg != 0 else episode_score\r\n",
    "        print('{} epi with {} steps, epi score {}, score_avg {}'.format(e+1,global_steps,episode_score, score_avg))\r\n",
    "        scores_avg.append(score_avg)\r\n",
    "        scores_raw.append(episode_score)\r\n",
    "        losses.append(np.mean(loss_list))\r\n",
    "        epsilons.append(agent.epsilon)\r\n",
    "        if e % SAVE_FREQ == 0:\r\n",
    "          save_statistics()\r\n",
    "        if score_avg > END_SCORE:\r\n",
    "          agent.save_model(\"\")\r\n",
    "          save_statistics()\r\n",
    "          end = True\r\n",
    "        break\r\n",
    "    if end == True:\r\n",
    "      env.close()\r\n",
    "      print(\"End\")\r\n",
    "      break"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('tf240': conda)"
  },
  "interpreter": {
   "hash": "fbba320975a9114d2433fba427f26c389728c846a7c4900c481dce2a1a9f6231"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}