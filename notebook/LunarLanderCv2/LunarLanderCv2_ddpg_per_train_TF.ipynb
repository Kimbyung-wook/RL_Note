{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('tf240': conda)"
  },
  "interpreter": {
   "hash": "fbba320975a9114d2433fba427f26c389728c846a7c4900c481dce2a1a9f6231"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import gym\r\n",
    "import sys\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, concatenate, Lambda\r\n",
    "# from collections import deque\r\n",
    "from utils.prioritized_memory import PrioritizedMemory\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# https://keras.io/examples/rl/ddpg_pendulum/\r\n",
    "class OUActionNoise:\r\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\r\n",
    "        self.theta = theta\r\n",
    "        self.mean = mean\r\n",
    "        self.std_dev = std_deviation\r\n",
    "        self.dt = dt\r\n",
    "        self.x_initial = x_initial\r\n",
    "        self.reset()\r\n",
    "\r\n",
    "    def __call__(self):\r\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\r\n",
    "        x = (\r\n",
    "            self.x_prev\r\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\r\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\r\n",
    "        )\r\n",
    "        # Store x into x_prev\r\n",
    "        # Makes next noise dependent on current one\r\n",
    "        self.x_prev = x\r\n",
    "        return x\r\n",
    "\r\n",
    "    def reset(self):\r\n",
    "        if self.x_initial is not None:\r\n",
    "            self.x_prev = self.x_initial\r\n",
    "        else:\r\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class Actor(tf.keras.Model):\r\n",
    "    def __init__(self, state_size, action_size, action_min, action_max):\r\n",
    "        super(Actor, self).__init__()\r\n",
    "        self.action_min = action_min\r\n",
    "        self.action_max = action_max\r\n",
    "\r\n",
    "        self.fc1 = Dense(64, activation='relu')\r\n",
    "        self.fc2 = Dense(64, activation='relu')\r\n",
    "        self.fc3 = Dense(16, activation='relu')\r\n",
    "        self.out= Dense(action_size, activation='tanh',kernel_initializer = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)) # -1 ~ +1\r\n",
    "\r\n",
    "    def call(self, x):\r\n",
    "        x       = self.fc1(x)\r\n",
    "        x       = self.fc2(x)\r\n",
    "        x       = self.fc3(x)\r\n",
    "        action  = self.out(x)\r\n",
    "        # return self.projected_to_action_space(action)\r\n",
    "        a = Lambda(lambda x: x*self.action_max)(action)\r\n",
    "        return a\r\n",
    "\r\n",
    "class Critic(tf.keras.Model):\r\n",
    "    def __init__(self, state_size, action_size):\r\n",
    "        super(Critic, self).__init__()\r\n",
    "        self.s1 = Dense(32, activation='relu')\r\n",
    "        self.s2 = Dense(32, activation='relu')\r\n",
    "        self.a1 = Dense(16, activation='relu')\r\n",
    "        self.a2 = Dense(32, activation='relu')\r\n",
    "        self.fc1= Dense(64, activation='relu')\r\n",
    "        self.fc2= Dense(64, activation='relu')\r\n",
    "        self.out= Dense(1,  activation='linear')\r\n",
    "\r\n",
    "    def call(self,state,action):\r\n",
    "        # state  = state_action[0]\r\n",
    "        # action = state_action[1]\r\n",
    "        s = self.s1(state)\r\n",
    "        s = self.s2(s)\r\n",
    "        a = self.a1(action)\r\n",
    "        a = self.a2(a)\r\n",
    "        c = concatenate([s,a],axis=-1)\r\n",
    "        x = self.fc1(c)\r\n",
    "        x = self.fc2(x)\r\n",
    "        q = self.out(x)\r\n",
    "        return q"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class DDPGAgent:\r\n",
    "    def __init__(self, state_size, action_size, action_min, action_max):\r\n",
    "        self.state_size = state_size\r\n",
    "        self.action_size= action_size\r\n",
    "        self.action_min = action_min\r\n",
    "        self.action_max = action_max\r\n",
    "\r\n",
    "        # Hyper params for learning\r\n",
    "        self.discount_factor = 0.99\r\n",
    "        self.actor_learning_rate  = 0.001\r\n",
    "        self.critic_learning_rate = 0.002\r\n",
    "        self.tau = 0.005\r\n",
    "\r\n",
    "        # Experience Replay\r\n",
    "        self.batch_size = 64\r\n",
    "        self.train_start = 500\r\n",
    "        self.memory_size = 100000\r\n",
    "        # ER\r\n",
    "        # self.memory = deque(maxlen=self.memory_size)\r\n",
    "        # PER\r\n",
    "        self.memory = PrioritizedMemory(capacity=self.memory_size)\r\n",
    "        # HER\r\n",
    "        \r\n",
    "        self.critic_optimizer   = tf.keras.optimizers.Adam(lr=self.critic_learning_rate)\r\n",
    "        self.actor_optimizer    = tf.keras.optimizers.Adam(lr=self.actor_learning_rate)\r\n",
    "\r\n",
    "        self.critic         = Critic(self.state_size, self.action_size)\r\n",
    "        self.target_critic  = Critic(self.state_size, self.action_size)\r\n",
    "        self.actor          = Actor(self.state_size, self.action_size, self.action_min, self.action_max)\r\n",
    "        self.target_actor   = Actor(self.state_size, self.action_size, self.action_min, self.action_max)\r\n",
    "        self.actor.build(input_shape=(None, self.state_size))\r\n",
    "        self.target_actor.build(input_shape=(None, self.state_size))\r\n",
    "        state_in = Input((self.state_size,))\r\n",
    "        action_in = Input((self.action_size,))\r\n",
    "        self.actor(state_in)\r\n",
    "        self.target_actor(state_in)\r\n",
    "        self.critic(state_in, action_in)\r\n",
    "        self.target_critic(state_in, action_in)\r\n",
    "        self.actor.summary()\r\n",
    "        self.critic.summary()\r\n",
    "        \r\n",
    "        self.target_actor.set_weights(self.actor.get_weights())\r\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\r\n",
    "\r\n",
    "        std_dev = 0.1\r\n",
    "        self.ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\r\n",
    "        \r\n",
    "        self.show_media_info = False\r\n",
    "\r\n",
    "    def remember(self, state, action, reward, next_state, done):\r\n",
    "        transition = (state, action, reward, next_state, done)\r\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\r\n",
    "        action = tf.convert_to_tensor([action], dtype=tf.float32)\r\n",
    "        next_state = tf.convert_to_tensor([next_state], dtype=tf.float32)\r\n",
    "        # ER\r\n",
    "        # self.memory.append(transition)\r\n",
    "        # PER\r\n",
    "        target_action = self.target_actor(next_state)\r\n",
    "        target_q = self.target_critic(next_state,target_action)\r\n",
    "        target_value = reward + (1 - done) * self.discount_factor * target_q\r\n",
    "        q = self.critic(state, action)\r\n",
    "        td_error = target_value - q\r\n",
    "        self.memory.add(td_error[0][0], transition)\r\n",
    "\r\n",
    "    def update_target_model(self):\r\n",
    "        tau = self.tau\r\n",
    "        for (net, target_net) in zip(   self.actor.trainable_variables,\r\n",
    "                                        self.target_actor.trainable_variables):\r\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\r\n",
    "        for (net, target_net) in zip(   self.critic.trainable_variables,\r\n",
    "                                        self.target_critic.trainable_variables):\r\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\r\n",
    "\r\n",
    "    def get_action(self,state):\r\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\r\n",
    "        action = self.actor(state)\r\n",
    "        # Exploration and Exploitation\r\n",
    "        action_from_net = action.numpy()[0]\r\n",
    "        action_from_noise = self.ou_noise()\r\n",
    "        return np.clip(action_from_net+action_from_noise,self.action_min,self.action_max)\r\n",
    "\r\n",
    "    def train_model(self):\r\n",
    "        # Train from Experience Replay\r\n",
    "        # Training Condition - Memory Size\r\n",
    "        if len(self.memory) < self.train_start:\r\n",
    "            return\r\n",
    "        # Sampling from the memory\r\n",
    "        # ER\r\n",
    "        # mini_batch = random.sample(self.memory, self.batch_size)\r\n",
    "        # PER\r\n",
    "        mini_batch, idxs, is_weights = self.memory.sample(self.batch_size)\r\n",
    "\r\n",
    "        states      = tf.convert_to_tensor(np.array([sample[0] for sample in mini_batch]))\r\n",
    "        actions     = tf.convert_to_tensor(np.array([sample[1] for sample in mini_batch]))\r\n",
    "        rewards     = tf.convert_to_tensor(np.array([sample[2] for sample in mini_batch]),dtype=tf.float32)\r\n",
    "        rewards     = tf.expand_dims(rewards, axis = 1)\r\n",
    "        next_states = tf.convert_to_tensor(np.array([sample[3] for sample in mini_batch]))\r\n",
    "        dones       = tf.convert_to_tensor(np.array([sample[4] for sample in mini_batch]),dtype=tf.float32)\r\n",
    "        dones       = tf.expand_dims(dones, axis = 1)\r\n",
    "        \r\n",
    "        if self.show_media_info == False:\r\n",
    "            self.show_media_info = True\r\n",
    "            print('Start to train, check batch shapes')\r\n",
    "            print('shape of states', np.shape(states),type(states))\r\n",
    "            print('shape of actions', np.shape(actions),type(actions))\r\n",
    "            print('shape of next_states', np.shape(next_states),type(next_states)) \r\n",
    "            print('shape of dones', np.shape(dones),type(dones))\r\n",
    "\r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            target_actions = self.target_actor(next_states,training=True)\r\n",
    "            target_q = self.target_critic(next_states,target_actions,training=True)\r\n",
    "            target_value = rewards + (1 - dones) * self.discount_factor * target_q\r\n",
    "            q = self.critic(states, actions,training=True)\r\n",
    "            td_error = target_value - q\r\n",
    "            critic_loss = tf.math.reduce_mean(is_weights*tf.math.square(td_error))\r\n",
    "        critic_params = self.critic.trainable_variables\r\n",
    "        critic_grads = tape.gradient(critic_loss, critic_params)\r\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grads, critic_params))\r\n",
    "\r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            new_actions = self.actor(states,training=True)\r\n",
    "            new_q = self.critic(states, new_actions,training=True)\r\n",
    "            actor_loss = -tf.reduce_mean(new_q)\r\n",
    "        actor_params = self.actor.trainable_variables\r\n",
    "        actor_grads = tape.gradient(actor_loss, actor_params)\r\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, actor_params))\r\n",
    "        \r\n",
    "        self.update_target_model()\r\n",
    "        # td_error = tf.transpose(td_error)[0].numpy()\r\n",
    "        # # td_error = tf.expand_dims(td_error, axis = 1)\r\n",
    "        # print('idxs ',idxs,np.shape(idxs),type(idxs))\r\n",
    "        # print('td_error ',td_error,np.shape(td_error),type(td_error))\r\n",
    "        for i in range(self.batch_size):\r\n",
    "            self.memory.update(idxs[i],td_error[i].numpy())\r\n",
    "        # self.memory.batch_update(idxs,tf.transpose(td_error).numpy())\r\n",
    "        return\r\n",
    "\r\n",
    "    def save_model(self):\r\n",
    "        self.actor.save_weights(\"./save_model/LunarLanderContinuous_ddpg_per_TF_actor\", save_format=\"tf\")\r\n",
    "        self.critic.save_weights(\"./save_model/LunarLanderContinuous_ddpg_per_TF_critic\", save_format=\"tf\")\r\n",
    "        return\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "%matplotlib tk\r\n",
    "\r\n",
    "ENV_NAME = 'LunarLanderContinuous-v2'\r\n",
    "EPISODES = 5000\r\n",
    "END_SCORE = 250\r\n",
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "    env = gym.make(ENV_NAME)\r\n",
    "    state_size  = env.observation_space.shape[0]\r\n",
    "    action_size = env.action_space.shape[0]\r\n",
    "    action_min  = env.action_space.low[0]\r\n",
    "    action_max  = env.action_space.high[0]\r\n",
    "\r\n",
    "    agent = DDPGAgent(state_size, action_size, action_min, action_max)\r\n",
    "    print('Env Name : ',ENV_NAME)\r\n",
    "    print('States {0}, Actions {1}'.format(state_size, action_size))\r\n",
    "    for i in range(0,action_size):\r\n",
    "        print('Action{0:d} space {1:.2f} ~ {2:.2f}'.format(i, env.action_space.low[i], env.action_space.high[i]))\r\n",
    "    scores, episodes = [], []\r\n",
    "    score_avg = 0\r\n",
    "\r\n",
    "    end = False\r\n",
    "    show_media_info = True\r\n",
    "    \r\n",
    "    for e in range(EPISODES):\r\n",
    "        done = False\r\n",
    "        score = 0\r\n",
    "        state = env.reset()\r\n",
    "        while not done:\r\n",
    "            # if e%100 == 0:\r\n",
    "            #     env.render()\r\n",
    "\r\n",
    "            # Interact with env.\r\n",
    "            action = agent.get_action(state)\r\n",
    "            next_state, reward, done, info = env.step(action)\r\n",
    "            agent.remember(state, action, reward, next_state, done)\r\n",
    "            agent.train_model()\r\n",
    "            state = next_state\r\n",
    "            # \r\n",
    "            score += reward\r\n",
    "            if show_media_info:\r\n",
    "                print(\"State Shape : \", np.shape(state))\r\n",
    "                print(\"Action Shape : \", np.shape(action))\r\n",
    "                print(\"Reward Shape : \", np.shape(reward))\r\n",
    "                print(\"done Shape : \", np.shape(done))\r\n",
    "                show_media_info = False\r\n",
    "            if done:\r\n",
    "                tau = 0.95\r\n",
    "                score_avg = tau * score_avg + (1.0 - tau) * score if score_avg != 0 else score\r\n",
    "                print(\"episode: {0:3d} | score avg: {1:3.2f} | mem size {2:6d} |\"\r\n",
    "                    .format(e, score_avg, len(agent.memory)))\r\n",
    "\r\n",
    "                episodes.append(e)\r\n",
    "                scores.append(score_avg)\r\n",
    "\r\n",
    "                plt.plot(episodes, scores, 'b')\r\n",
    "                plt.xlabel('episode')\r\n",
    "                plt.ylabel('average score')\r\n",
    "                plt.title('LunarLanderContinuous DDPG PER TF')\r\n",
    "                plt.grid()\r\n",
    "                plt.savefig(\"./save_model/LunarLanderContinuous_ddpg_per_TF.png\")\r\n",
    "\r\n",
    "                # 이동 평균이 0 이상일 때 종료\r\n",
    "                if score_avg > END_SCORE:\r\n",
    "                    agent.save_model()\r\n",
    "                    end = True\r\n",
    "                    break\r\n",
    "        if end == True:\r\n",
    "            env.close()\r\n",
    "            np.save('./save_model/data/LunarLanderContinuous_ddpg_per_TF_epi',  episodes)\r\n",
    "            np.save('./save_model/data/LunarLanderContinuous_ddpg_per_TF_score',scores)\r\n",
    "            print(\"End\")\r\n",
    "            break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"actor\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             multiple                  576       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             multiple                  1040      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             multiple                  34        \n",
      "=================================================================\n",
      "Total params: 5,810\n",
      "Trainable params: 5,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"critic\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  288       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  1056      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  48        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  544       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  65        \n",
      "=================================================================\n",
      "Total params: 10,321\n",
      "Trainable params: 10,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Env Name :  LunarLanderContinuous-v2\n",
      "States 8, Actions 2\n",
      "Action0 space -1.00 ~ 1.00\n",
      "Action1 space -1.00 ~ 1.00\n",
      "State Shape :  (8,)\n",
      "Action Shape :  (2,)\n",
      "Reward Shape :  ()\n",
      "done Shape :  ()\n",
      "episode:   0 | score avg: -145.98 | mem size     68 |\n",
      "episode:   1 | score avg: -145.35 | mem size    144 |\n",
      "episode:   2 | score avg: -144.91 | mem size    224 |\n",
      "episode:   3 | score avg: -136.56 | mem size    313 |\n",
      "episode:   4 | score avg: -137.70 | mem size    373 |\n",
      "episode:   5 | score avg: -136.59 | mem size    446 |\n",
      "episode:   6 | score avg: -136.01 | mem size    501 |\n",
      "episode:   7 | score avg: -136.54 | mem size    569 |\n",
      "episode:   8 | score avg: -136.46 | mem size    619 |\n",
      "episode:   9 | score avg: -139.00 | mem size    696 |\n",
      "episode:  10 | score avg: -137.87 | mem size    751 |\n",
      "episode:  11 | score avg: -138.18 | mem size    806 |\n",
      "episode:  12 | score avg: -155.49 | mem size    912 |\n",
      "episode:  13 | score avg: -149.68 | mem size   1206 |\n",
      "episode:  14 | score avg: -145.13 | mem size   1286 |\n",
      "episode:  15 | score avg: -143.09 | mem size   1392 |\n",
      "episode:  16 | score avg: -142.42 | mem size   1472 |\n",
      "episode:  17 | score avg: -145.36 | mem size   1554 |\n",
      "episode:  18 | score avg: -141.33 | mem size   1659 |\n",
      "episode:  19 | score avg: -139.57 | mem size   1722 |\n",
      "episode:  20 | score avg: -139.04 | mem size   1777 |\n",
      "episode:  21 | score avg: -141.40 | mem size   1864 |\n",
      "episode:  22 | score avg: -142.23 | mem size   1934 |\n",
      "Start to train, check batch shapes\n",
      "shape of states (64, 8) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "shape of actions (64, 2) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "shape of next_states (64, 8) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "shape of dones (64, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7836/1586262690.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7836/2765122219.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;31m# for i in range(self.batch_size):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;31m#     self.memory.update(idxs[i],td_error[i].numpy())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtd_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\RL_Note\\notebook\\LunarLanderCv2\\utils\\prioritized_memory.py\u001b[0m in \u001b[0;36mbatch_update\u001b[1;34m(self, idxs, errors)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbatch_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_priority\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\user\\.conda\\envs\\tf240\\lib\\site-packages\\ipykernel\\eventloops.py:256: RuntimeWarning: coroutine 'Kernel.do_one_iteration' was never awaited\n",
      "  self.func()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env.close()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}