{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('tf240': conda)"
  },
  "interpreter": {
   "hash": "fbba320975a9114d2433fba427f26c389728c846a7c4900c481dce2a1a9f6231"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import gym\r\n",
    "import sys\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, concatenate, Lambda\r\n",
    "from collections import deque\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# https://keras.io/examples/rl/ddpg_pendulum/\r\n",
    "class OUActionNoise:\r\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\r\n",
    "        self.theta = theta\r\n",
    "        self.mean = mean\r\n",
    "        self.std_dev = std_deviation\r\n",
    "        self.dt = dt\r\n",
    "        self.x_initial = x_initial\r\n",
    "        self.reset()\r\n",
    "\r\n",
    "    def __call__(self):\r\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\r\n",
    "        x = (\r\n",
    "            self.x_prev\r\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\r\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\r\n",
    "        )\r\n",
    "        # Store x into x_prev\r\n",
    "        # Makes next noise dependent on current one\r\n",
    "        self.x_prev = x\r\n",
    "        return x\r\n",
    "\r\n",
    "    def reset(self):\r\n",
    "        if self.x_initial is not None:\r\n",
    "            self.x_prev = self.x_initial\r\n",
    "        else:\r\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, state_size, action_size, action_min, action_max):\n",
    "        super(Actor, self).__init__()\n",
    "        self.action_min = action_min\n",
    "        self.action_max = action_max\n",
    "\n",
    "        self.fc1 = Dense(64, activation='relu')\n",
    "        self.fc2 = Dense(64, activation='relu')\n",
    "        self.fc3 = Dense(16, activation='relu')\n",
    "        self.out= Dense(action_size, activation='tanh',kernel_initializer = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)) # -1 ~ +1\n",
    "\n",
    "    def call(self, x):\n",
    "        x       = self.fc1(x)\n",
    "        x       = self.fc2(x)\n",
    "        x       = self.fc3(x)\n",
    "        action  = self.out(x)\n",
    "        # return self.projected_to_action_space(action)\n",
    "        a = Lambda(lambda x: x*self.action_max)(action)\n",
    "        return a\n",
    "\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.s1 = Dense(32, activation='relu')\n",
    "        self.s2 = Dense(32, activation='relu')\n",
    "        self.a1 = Dense(16, activation='relu')\n",
    "        self.a2 = Dense(32, activation='relu')\n",
    "        self.fc1= Dense(64, activation='relu')\n",
    "        self.fc2= Dense(64, activation='relu')\n",
    "        self.out= Dense(1,  activation='linear')\n",
    "\n",
    "    def call(self,state_action):\n",
    "        state  = state_action[0]\n",
    "        action = state_action[1]\n",
    "        s = self.s1(state)\n",
    "        s = self.s2(s)\n",
    "        a = self.a1(action)\n",
    "        a = self.a2(a)\n",
    "        c = concatenate([s,a],axis=-1)\n",
    "        x = self.fc1(c)\n",
    "        x = self.fc2(x)\n",
    "        q = self.out(x)\n",
    "        return q"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class DDPGAgent:\n",
    "    def __init__(self, state_size, action_size, action_min, action_max):\n",
    "        self.state_size = state_size\n",
    "        self.action_size= action_size\n",
    "        self.action_min = action_min\n",
    "        self.action_max = action_max\n",
    "\n",
    "        # Hyper params for learning\n",
    "        self.discount_factor = 0.99\n",
    "        self.actor_learning_rate  = 0.001\n",
    "        self.critic_learning_rate = 0.002\n",
    "        self.tau = 0.005\n",
    "\n",
    "        # Experience Replay\n",
    "        self.batch_size = 256\n",
    "        self.train_start = 5000\n",
    "        self.memory = deque(maxlen=100000)\n",
    "\n",
    "        self.critic         = Critic(self.state_size, self.action_size)\n",
    "        self.target_critic  = Critic(self.state_size, self.action_size)\n",
    "        self.actor          = Actor(self.state_size, self.action_size, self.action_min, self.action_max)\n",
    "        self.target_actor   = Actor(self.state_size, self.action_size, self.action_min, self.action_max)\n",
    "        self.critic_optimizer   = tf.keras.optimizers.Adam(lr=self.critic_learning_rate)\n",
    "        self.actor_optimizer    = tf.keras.optimizers.Adam(lr=self.actor_learning_rate)\n",
    "        self.actor.build(input_shape=(None, self.state_size))\n",
    "        self.target_actor.build(input_shape=(None, self.state_size))\n",
    "        state_in = Input((self.state_size,))\n",
    "        action_in = Input((self.action_size,))\n",
    "        self.actor(state_in)\n",
    "        self.target_actor(state_in)\n",
    "        self.critic([state_in, action_in])\n",
    "        self.target_critic([state_in, action_in])\n",
    "        self.actor.summary()\n",
    "        self.critic.summary()\n",
    "        \n",
    "        self.target_actor.set_weights(self.actor.get_weights())\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\n",
    "\n",
    "        std_dev = 0.1\n",
    "        self.ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "        \n",
    "        self.show_media_info = False\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def update_target_model(self):\n",
    "        tau = self.tau\n",
    "        for (net, target_net) in zip(   self.actor.trainable_variables,\n",
    "                                        self.target_actor.trainable_variables):\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\n",
    "        for (net, target_net) in zip(   self.critic.trainable_variables,\n",
    "                                        self.target_critic.trainable_variables):\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\n",
    "\n",
    "    def get_action(self,state):\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        action = self.actor(state)\n",
    "        # Exploration and Exploitation\n",
    "        action_from_net = action.numpy()[0]\n",
    "        action_from_noise = self.ou_noise()\n",
    "        return np.clip(action_from_net+action_from_noise,self.action_min,self.action_max)\n",
    "\n",
    "    def train_model(self):\n",
    "        # Train from Experience Replay\n",
    "        # Training Condition - Memory Size\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        # Sampling from the memory\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        states      = tf.convert_to_tensor(np.array([sample[0] for sample in mini_batch]))\n",
    "        actions     = tf.convert_to_tensor(np.array([sample[1] for sample in mini_batch]))\n",
    "        rewards     = tf.convert_to_tensor(np.array([sample[2] for sample in mini_batch]),dtype=tf.float32)\n",
    "        rewards     = tf.expand_dims(rewards, axis = 1)\n",
    "        next_states = tf.convert_to_tensor(np.array([sample[3] for sample in mini_batch]))\n",
    "        dones       = tf.convert_to_tensor(np.array([sample[4] for sample in mini_batch]),dtype=tf.float32)\n",
    "        dones       = tf.expand_dims(dones, axis = 1)\n",
    "        \n",
    "        if self.show_media_info == False:\n",
    "            self.show_media_info = True\n",
    "            print('Start to train, check batch shapes')\n",
    "            print('shape of states', np.shape(states),type(states))\n",
    "            print('shape of actions', np.shape(actions),type(actions))\n",
    "            print('shape of rewards', np.shape(rewards),type(rewards))\n",
    "            print('shape of next_states', np.shape(next_states),type(next_states))\n",
    "            print('shape of dones', np.shape(dones),type(dones))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = self.target_actor(next_states,training=True)\n",
    "            target_q = self.target_critic([next_states,target_actions],training=True)\n",
    "            target_value = rewards + (1 - dones) * self.discount_factor * target_q\n",
    "            q = self.critic([states, actions],training=True)\n",
    "            td_error = target_value - q\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(target_value - q))\n",
    "        critic_params = self.critic.trainable_variables\n",
    "        critic_grads = tape.gradient(critic_loss, critic_params)\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grads, critic_params))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            new_actions = self.actor(states,training=True)\n",
    "            new_q = self.critic([states, new_actions],training=True)\n",
    "            actor_loss = -tf.reduce_mean(new_q)\n",
    "        actor_params = self.actor.trainable_variables\n",
    "        actor_grads = tape.gradient(actor_loss, actor_params)\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, actor_params))\n",
    "        \n",
    "        self.update_target_model()\n",
    "        return\n",
    "\n",
    "    def save_model(self):\n",
    "        self.actor.save_weights(\"./save_model/LunarLanderContinuous_ddpg_TF_actor\", save_format=\"tf\")\n",
    "        self.critic.save_weights(\"./save_model/LunarLanderContinuous_ddpg_TF_critic\", save_format=\"tf\")\n",
    "        return\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "%matplotlib tk\n",
    "\n",
    "ENV_NAME = 'LunarLanderContinuous-v2'\n",
    "EPISODES = 2000\n",
    "END_SCORE = 250\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(ENV_NAME)\n",
    "    state_size  = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.shape[0]\n",
    "    action_min  = env.action_space.low[0]\n",
    "    action_max  = env.action_space.high[0]\n",
    "\n",
    "    agent = DDPGAgent(state_size, action_size, action_min, action_max)\n",
    "    print('Env Name : ',ENV_NAME)\n",
    "    print('States {0}, Actions {1}'.format(state_size, action_size))\n",
    "    for i in range(0,action_size):\n",
    "        print('Action{0:d} space {1:.2f} ~ {2:.2f}'.format(i, env.action_space.low[i], env.action_space.high[i]))\n",
    "    scores, episodes = [], []\n",
    "    score_avg = 0\n",
    "\n",
    "    end = False\n",
    "    show_media_info = True\n",
    "    \n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        while not done:\n",
    "            # env.render()\n",
    "\n",
    "            # Interact with env.\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            agent.train_model()\n",
    "            state = next_state\n",
    "            # \n",
    "            score += reward\n",
    "            if show_media_info:\n",
    "                print(\"State Shape : \", np.shape(state))\n",
    "                print(\"Action Shape : \", np.shape(action))\n",
    "                print(\"Reward Shape : \", np.shape(reward))\n",
    "                print(\"done Shape : \", np.shape(done))\n",
    "                show_media_info = False\n",
    "            if done:\n",
    "                tau = 0.9\n",
    "                score_avg = tau * score_avg + (1.0 - tau) * score if score_avg != 0 else score\n",
    "                print(\"episode: {0:3d} | score avg: {1:3.2f} | mem size {2:6d} |\"\n",
    "                    .format(e, score_avg, len(agent.memory)))\n",
    "\n",
    "                episodes.append(e)\n",
    "                scores.append(score_avg)\n",
    "\n",
    "                plt.plot(episodes, scores, 'b')\n",
    "                plt.xlabel('episode')\n",
    "                plt.ylabel('average score')\n",
    "                plt.title('LunarLanderContinuous DDPG TF')\n",
    "                plt.grid()\n",
    "                plt.savefig(\"./save_model/LunarLanderContinuous_ddpg_TF.png\")\n",
    "\n",
    "                # 이동 평균이 0 이상일 때 종료\n",
    "                if score_avg > END_SCORE:\n",
    "                    agent.save_model()\n",
    "                    end = True\n",
    "                    break\n",
    "        if end == True:\n",
    "            env.close()\n",
    "            np.save('./save_model/data/LunarLanderContinuous_ddpg_TF_epi',  episodes)\n",
    "            np.save('./save_model/data/LunarLanderContinuous_ddpg_TF_score',scores)\n",
    "            print(\"End\")\n",
    "            break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"actor\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             multiple                  576       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             multiple                  1040      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             multiple                  34        \n",
      "=================================================================\n",
      "Total params: 5,810\n",
      "Trainable params: 5,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"critic\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  288       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  1056      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  48        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  544       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  65        \n",
      "=================================================================\n",
      "Total params: 10,321\n",
      "Trainable params: 10,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Env Name :  LunarLanderContinuous-v2\n",
      "States 8, Actions 2\n",
      "Action space -1.00 ~ 1.00\n",
      "State Shape :  (8,)\n",
      "Action Shape :  (2,)\n",
      "Reward Shape :  ()\n",
      "done Shape :  ()\n",
      "episode:   0 | score avg: -213.54 | mem size    103 |\n",
      "episode:   1 | score avg: -186.64 | mem size    196 |\n",
      "episode:   2 | score avg: -197.18 | mem size    294 |\n",
      "episode:   3 | score avg: -219.48 | mem size    388 |\n",
      "episode:   4 | score avg: -217.32 | mem size    480 |\n",
      "episode:   5 | score avg: -231.31 | mem size    579 |\n",
      "episode:   6 | score avg: -222.65 | mem size    704 |\n",
      "episode:   7 | score avg: -212.13 | mem size    776 |\n",
      "episode:   8 | score avg: -203.44 | mem size    829 |\n",
      "episode:   9 | score avg: -197.42 | mem size    914 |\n",
      "episode:  10 | score avg: -193.71 | mem size    980 |\n",
      "episode:  11 | score avg: -184.40 | mem size   1048 |\n",
      "episode:  12 | score avg: -202.20 | mem size   1279 |\n",
      "episode:  13 | score avg: -215.56 | mem size   1365 |\n",
      "episode:  14 | score avg: -244.29 | mem size   1550 |\n",
      "episode:  15 | score avg: -251.11 | mem size   1687 |\n",
      "episode:  16 | score avg: -252.45 | mem size   1930 |\n",
      "episode:  17 | score avg: -258.00 | mem size   2033 |\n",
      "episode:  18 | score avg: -238.94 | mem size   2145 |\n",
      "episode:  19 | score avg: -251.18 | mem size   2255 |\n",
      "episode:  20 | score avg: -253.36 | mem size   2420 |\n",
      "episode:  21 | score avg: -251.20 | mem size   2516 |\n",
      "episode:  22 | score avg: -255.48 | mem size   2612 |\n",
      "episode:  23 | score avg: -236.70 | mem size   2719 |\n",
      "episode:  24 | score avg: -241.05 | mem size   2840 |\n",
      "episode:  25 | score avg: -255.36 | mem size   2930 |\n",
      "episode:  26 | score avg: -266.32 | mem size   3040 |\n",
      "episode:  27 | score avg: -263.91 | mem size   3149 |\n",
      "episode:  28 | score avg: -277.44 | mem size   3361 |\n",
      "episode:  29 | score avg: -282.46 | mem size   3451 |\n",
      "episode:  30 | score avg: -298.64 | mem size   3637 |\n",
      "episode:  31 | score avg: -293.62 | mem size   3788 |\n",
      "episode:  32 | score avg: -293.78 | mem size   3929 |\n",
      "episode:  33 | score avg: -274.46 | mem size   4020 |\n",
      "episode:  34 | score avg: -266.77 | mem size   4101 |\n",
      "episode:  35 | score avg: -254.34 | mem size   4158 |\n",
      "episode:  36 | score avg: -245.07 | mem size   4240 |\n",
      "episode:  37 | score avg: -234.42 | mem size   4311 |\n",
      "episode:  38 | score avg: -225.45 | mem size   4375 |\n",
      "episode:  39 | score avg: -215.26 | mem size   4462 |\n",
      "episode:  40 | score avg: -209.49 | mem size   4553 |\n",
      "episode:  41 | score avg: -197.64 | mem size   4609 |\n",
      "episode:  42 | score avg: -175.51 | mem size   4702 |\n",
      "episode:  43 | score avg: -176.04 | mem size   4785 |\n",
      "episode:  44 | score avg: -170.64 | mem size   4852 |\n",
      "episode:  45 | score avg: -169.29 | mem size   4916 |\n",
      "episode:  46 | score avg: -155.37 | mem size   4976 |\n",
      "Start to train, check batch shapes\n",
      "shape of states (256, 8) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "shape of actions (256, 2) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "shape of rewards (256, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "shape of next_states (256, 8) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "shape of dones (256, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "episode:  47 | score avg: -168.15 | mem size   5074 |\n",
      "episode:  48 | score avg: -199.60 | mem size   5169 |\n",
      "episode:  49 | score avg: -273.06 | mem size   5249 |\n",
      "episode:  50 | score avg: -342.28 | mem size   5328 |\n",
      "episode:  51 | score avg: -429.50 | mem size   5428 |\n",
      "episode:  52 | score avg: -567.15 | mem size   5725 |\n",
      "episode:  53 | score avg: -548.39 | mem size   5815 |\n",
      "episode:  54 | score avg: -536.16 | mem size   5918 |\n",
      "episode:  55 | score avg: -506.88 | mem size   6258 |\n",
      "episode:  56 | score avg: -497.68 | mem size   6332 |\n",
      "episode:  57 | score avg: -493.61 | mem size   6388 |\n",
      "episode:  58 | score avg: -450.57 | mem size   6530 |\n",
      "episode:  59 | score avg: -427.94 | mem size   6693 |\n",
      "episode:  60 | score avg: -429.17 | mem size   6825 |\n",
      "episode:  61 | score avg: -431.44 | mem size   7037 |\n",
      "episode:  62 | score avg: -393.42 | mem size   7146 |\n",
      "episode:  63 | score avg: -381.60 | mem size   7343 |\n",
      "episode:  64 | score avg: -357.05 | mem size   7410 |\n",
      "episode:  65 | score avg: -338.62 | mem size   7567 |\n",
      "episode:  66 | score avg: -317.38 | mem size   7927 |\n",
      "episode:  67 | score avg: -309.16 | mem size   8020 |\n",
      "episode:  68 | score avg: -315.51 | mem size   8092 |\n",
      "episode:  69 | score avg: -322.92 | mem size   8168 |\n",
      "episode:  70 | score avg: -346.48 | mem size   8253 |\n",
      "episode:  71 | score avg: -345.22 | mem size   8539 |\n",
      "episode:  72 | score avg: -340.56 | mem size   8649 |\n",
      "episode:  73 | score avg: -355.54 | mem size   8935 |\n",
      "episode:  74 | score avg: -358.99 | mem size   9052 |\n",
      "episode:  75 | score avg: -349.70 | mem size   9200 |\n",
      "episode:  76 | score avg: -342.70 | mem size   9289 |\n",
      "episode:  77 | score avg: -341.08 | mem size   9405 |\n",
      "episode:  78 | score avg: -358.98 | mem size   9725 |\n",
      "episode:  79 | score avg: -365.67 | mem size   9845 |\n",
      "episode:  80 | score avg: -370.58 | mem size  10007 |\n",
      "episode:  81 | score avg: -389.48 | mem size  10128 |\n",
      "episode:  82 | score avg: -387.17 | mem size  10206 |\n",
      "episode:  83 | score avg: -394.52 | mem size  10342 |\n",
      "episode:  84 | score avg: -395.21 | mem size  10430 |\n",
      "episode:  85 | score avg: -417.37 | mem size  10607 |\n",
      "episode:  86 | score avg: -423.45 | mem size  10718 |\n",
      "episode:  87 | score avg: -408.08 | mem size  10806 |\n",
      "episode:  88 | score avg: -405.08 | mem size  10908 |\n",
      "episode:  89 | score avg: -408.58 | mem size  11024 |\n",
      "episode:  90 | score avg: -408.66 | mem size  11122 |\n",
      "episode:  91 | score avg: -407.49 | mem size  11235 |\n",
      "episode:  92 | score avg: -417.44 | mem size  11381 |\n",
      "episode:  93 | score avg: -424.95 | mem size  11510 |\n",
      "episode:  94 | score avg: -448.19 | mem size  11637 |\n",
      "episode:  95 | score avg: -470.94 | mem size  11765 |\n",
      "episode:  96 | score avg: -452.88 | mem size  11864 |\n",
      "episode:  97 | score avg: -473.67 | mem size  11946 |\n",
      "episode:  98 | score avg: -453.82 | mem size  12021 |\n",
      "episode:  99 | score avg: -443.81 | mem size  12159 |\n",
      "episode: 100 | score avg: -417.41 | mem size  12232 |\n",
      "episode: 101 | score avg: -440.58 | mem size  12299 |\n",
      "episode: 102 | score avg: -421.12 | mem size  12385 |\n",
      "episode: 103 | score avg: -459.48 | mem size  12476 |\n",
      "episode: 104 | score avg: -550.99 | mem size  12592 |\n",
      "episode: 105 | score avg: -582.22 | mem size  12720 |\n",
      "episode: 106 | score avg: -550.10 | mem size  12858 |\n",
      "episode: 107 | score avg: -549.09 | mem size  12967 |\n",
      "episode: 108 | score avg: -519.46 | mem size  13069 |\n",
      "episode: 109 | score avg: -509.88 | mem size  13227 |\n",
      "episode: 110 | score avg: -480.45 | mem size  13309 |\n",
      "episode: 111 | score avg: -446.01 | mem size  13376 |\n",
      "episode: 112 | score avg: -413.70 | mem size  13445 |\n",
      "episode: 113 | score avg: -379.32 | mem size  13515 |\n",
      "episode: 114 | score avg: -366.06 | mem size  13692 |\n",
      "episode: 115 | score avg: -366.35 | mem size  13823 |\n",
      "episode: 116 | score avg: -408.11 | mem size  13923 |\n",
      "episode: 117 | score avg: -387.77 | mem size  13994 |\n",
      "episode: 118 | score avg: -375.25 | mem size  14090 |\n",
      "episode: 119 | score avg: -344.45 | mem size  14174 |\n",
      "episode: 120 | score avg: -332.33 | mem size  14286 |\n",
      "episode: 121 | score avg: -311.25 | mem size  14338 |\n",
      "episode: 122 | score avg: -291.72 | mem size  14409 |\n",
      "episode: 123 | score avg: -274.74 | mem size  14469 |\n",
      "episode: 124 | score avg: -250.54 | mem size  14545 |\n",
      "episode: 125 | score avg: -233.86 | mem size  14649 |\n",
      "episode: 126 | score avg: -211.51 | mem size  14775 |\n",
      "episode: 127 | score avg: -191.32 | mem size  14876 |\n",
      "episode: 128 | score avg: -182.56 | mem size  14993 |\n",
      "episode: 129 | score avg: -174.61 | mem size  15131 |\n",
      "episode: 130 | score avg: -167.45 | mem size  15207 |\n",
      "episode: 131 | score avg: -166.65 | mem size  15269 |\n",
      "episode: 132 | score avg: -161.09 | mem size  15340 |\n",
      "episode: 133 | score avg: -160.15 | mem size  15417 |\n",
      "episode: 134 | score avg: -156.98 | mem size  15570 |\n",
      "episode: 135 | score avg: -152.50 | mem size  15662 |\n",
      "episode: 136 | score avg: -139.48 | mem size  15735 |\n",
      "episode: 137 | score avg: -133.94 | mem size  15816 |\n",
      "episode: 138 | score avg: -149.55 | mem size  15914 |\n",
      "episode: 139 | score avg: -144.60 | mem size  15975 |\n",
      "episode: 140 | score avg: -143.95 | mem size  16045 |\n",
      "episode: 141 | score avg: -140.43 | mem size  16117 |\n",
      "episode: 142 | score avg: -151.36 | mem size  16297 |\n",
      "episode: 143 | score avg: -152.14 | mem size  16393 |\n",
      "episode: 144 | score avg: -155.69 | mem size  17220 |\n",
      "episode: 145 | score avg: -151.53 | mem size  17333 |\n",
      "episode: 146 | score avg: -143.15 | mem size  17413 |\n",
      "episode: 147 | score avg: -143.31 | mem size  17477 |\n",
      "episode: 148 | score avg: -128.25 | mem size  17720 |\n",
      "episode: 149 | score avg: -130.56 | mem size  17830 |\n",
      "episode: 150 | score avg: -146.63 | mem size  17966 |\n",
      "episode: 151 | score avg: -142.25 | mem size  18083 |\n",
      "episode: 152 | score avg: -139.27 | mem size  18168 |\n",
      "episode: 153 | score avg: -132.57 | mem size  18422 |\n",
      "episode: 154 | score avg: -141.32 | mem size  18513 |\n",
      "episode: 155 | score avg: -137.59 | mem size  18664 |\n",
      "episode: 156 | score avg: -151.85 | mem size  18769 |\n",
      "episode: 157 | score avg: -168.31 | mem size  19080 |\n",
      "episode: 158 | score avg: -171.93 | mem size  19236 |\n",
      "episode: 159 | score avg: -164.50 | mem size  19404 |\n",
      "episode: 160 | score avg: -175.81 | mem size  19503 |\n",
      "episode: 161 | score avg: -187.16 | mem size  19658 |\n",
      "episode: 162 | score avg: -182.39 | mem size  19773 |\n",
      "episode: 163 | score avg: -188.55 | mem size  20059 |\n",
      "episode: 164 | score avg: -172.60 | mem size  20178 |\n",
      "episode: 165 | score avg: -183.34 | mem size  20288 |\n",
      "episode: 166 | score avg: -187.42 | mem size  20553 |\n",
      "episode: 167 | score avg: -195.62 | mem size  20898 |\n",
      "episode: 168 | score avg: -194.90 | mem size  20974 |\n",
      "episode: 169 | score avg: -185.62 | mem size  21056 |\n",
      "episode: 170 | score avg: -202.68 | mem size  21226 |\n",
      "episode: 171 | score avg: -186.96 | mem size  21339 |\n",
      "episode: 172 | score avg: -182.17 | mem size  21428 |\n",
      "episode: 173 | score avg: -170.61 | mem size  21535 |\n",
      "episode: 174 | score avg: -132.85 | mem size  22004 |\n",
      "episode: 175 | score avg: -137.16 | mem size  22068 |\n",
      "episode: 176 | score avg: -136.93 | mem size  22144 |\n",
      "episode: 177 | score avg: -139.08 | mem size  22261 |\n",
      "episode: 178 | score avg: -145.67 | mem size  22374 |\n",
      "episode: 179 | score avg: -148.38 | mem size  22494 |\n",
      "episode: 180 | score avg: -160.62 | mem size  22582 |\n",
      "episode: 181 | score avg: -162.48 | mem size  22672 |\n",
      "episode: 182 | score avg: -164.76 | mem size  22753 |\n",
      "episode: 183 | score avg: -155.63 | mem size  22864 |\n",
      "episode: 184 | score avg: -149.07 | mem size  22968 |\n",
      "episode: 185 | score avg: -134.90 | mem size  23150 |\n",
      "episode: 186 | score avg: -123.42 | mem size  23241 |\n",
      "episode: 187 | score avg: -145.81 | mem size  23343 |\n",
      "episode: 188 | score avg: -150.61 | mem size  23411 |\n",
      "episode: 189 | score avg: -166.39 | mem size  23555 |\n",
      "episode: 190 | score avg: -175.87 | mem size  23730 |\n",
      "episode: 191 | score avg: -181.97 | mem size  23896 |\n",
      "episode: 192 | score avg: -178.30 | mem size  23961 |\n",
      "episode: 193 | score avg: -177.73 | mem size  24037 |\n",
      "episode: 194 | score avg: -178.91 | mem size  24126 |\n",
      "episode: 195 | score avg: -170.50 | mem size  24294 |\n",
      "episode: 196 | score avg: -168.45 | mem size  24371 |\n",
      "episode: 197 | score avg: -164.03 | mem size  24436 |\n",
      "episode: 198 | score avg: -173.29 | mem size  24540 |\n",
      "episode: 199 | score avg: -193.29 | mem size  24632 |\n",
      "episode: 200 | score avg: -205.70 | mem size  24738 |\n",
      "episode: 201 | score avg: -236.14 | mem size  24851 |\n",
      "episode: 202 | score avg: -255.36 | mem size  25007 |\n",
      "episode: 203 | score avg: -259.30 | mem size  25121 |\n",
      "episode: 204 | score avg: -294.96 | mem size  25262 |\n",
      "episode: 205 | score avg: -301.23 | mem size  25421 |\n",
      "episode: 206 | score avg: -297.11 | mem size  25541 |\n",
      "episode: 207 | score avg: -277.48 | mem size  25594 |\n",
      "episode: 208 | score avg: -262.34 | mem size  25674 |\n",
      "episode: 209 | score avg: -246.58 | mem size  25755 |\n",
      "episode: 210 | score avg: -243.32 | mem size  25871 |\n",
      "episode: 211 | score avg: -230.25 | mem size  25987 |\n",
      "episode: 212 | score avg: -224.09 | mem size  26073 |\n",
      "episode: 213 | score avg: -215.97 | mem size  26127 |\n",
      "episode: 214 | score avg: -196.17 | mem size  26230 |\n",
      "episode: 215 | score avg: -190.40 | mem size  26305 |\n",
      "episode: 216 | score avg: -176.71 | mem size  26377 |\n",
      "episode: 217 | score avg: -176.91 | mem size  26475 |\n",
      "episode: 218 | score avg: -191.34 | mem size  26579 |\n",
      "episode: 219 | score avg: -184.00 | mem size  26672 |\n",
      "episode: 220 | score avg: -178.60 | mem size  26755 |\n",
      "episode: 221 | score avg: -174.27 | mem size  26895 |\n",
      "episode: 222 | score avg: -170.61 | mem size  26983 |\n",
      "episode: 223 | score avg: -180.11 | mem size  27500 |\n",
      "episode: 224 | score avg: -185.06 | mem size  27637 |\n",
      "episode: 225 | score avg: -190.24 | mem size  27829 |\n",
      "episode: 226 | score avg: -188.73 | mem size  27957 |\n",
      "episode: 227 | score avg: -199.35 | mem size  28375 |\n",
      "episode: 228 | score avg: -211.55 | mem size  28457 |\n",
      "episode: 229 | score avg: -209.26 | mem size  28550 |\n",
      "episode: 230 | score avg: -212.32 | mem size  28662 |\n",
      "episode: 231 | score avg: -227.92 | mem size  28746 |\n",
      "episode: 232 | score avg: -238.32 | mem size  28821 |\n",
      "episode: 233 | score avg: -247.91 | mem size  28903 |\n",
      "episode: 234 | score avg: -276.13 | mem size  28972 |\n",
      "episode: 235 | score avg: -282.69 | mem size  29061 |\n",
      "episode: 236 | score avg: -304.00 | mem size  29151 |\n",
      "episode: 237 | score avg: -311.30 | mem size  29243 |\n",
      "episode: 238 | score avg: -306.23 | mem size  29357 |\n",
      "episode: 239 | score avg: -306.23 | mem size  29450 |\n",
      "episode: 240 | score avg: -307.85 | mem size  29542 |\n",
      "episode: 241 | score avg: -287.78 | mem size  29631 |\n",
      "episode: 242 | score avg: -288.52 | mem size  29741 |\n",
      "episode: 243 | score avg: -292.14 | mem size  29875 |\n",
      "episode: 244 | score avg: -268.00 | mem size  29994 |\n",
      "episode: 245 | score avg: -266.10 | mem size  30097 |\n",
      "episode: 246 | score avg: -273.26 | mem size  30236 |\n",
      "episode: 247 | score avg: -280.95 | mem size  30402 |\n",
      "episode: 248 | score avg: -286.06 | mem size  30534 |\n",
      "episode: 249 | score avg: -277.65 | mem size  30669 |\n",
      "episode: 250 | score avg: -283.38 | mem size  30761 |\n",
      "episode: 251 | score avg: -274.29 | mem size  30872 |\n",
      "episode: 252 | score avg: -289.67 | mem size  31150 |\n",
      "episode: 253 | score avg: -293.47 | mem size  31547 |\n",
      "episode: 254 | score avg: -290.90 | mem size  31773 |\n",
      "episode: 255 | score avg: -289.31 | mem size  31869 |\n",
      "episode: 256 | score avg: -283.63 | mem size  32072 |\n",
      "episode: 257 | score avg: -285.26 | mem size  32536 |\n",
      "episode: 258 | score avg: -276.28 | mem size  32739 |\n",
      "episode: 259 | score avg: -265.41 | mem size  33001 |\n",
      "episode: 260 | score avg: -255.79 | mem size  33144 |\n",
      "episode: 261 | score avg: -247.36 | mem size  33449 |\n",
      "episode: 262 | score avg: -238.43 | mem size  33842 |\n",
      "episode: 263 | score avg: -227.38 | mem size  34033 |\n",
      "episode: 264 | score avg: -227.35 | mem size  34136 |\n",
      "episode: 265 | score avg: -221.03 | mem size  34305 |\n",
      "episode: 266 | score avg: -207.34 | mem size  34456 |\n",
      "episode: 267 | score avg: -205.97 | mem size  34875 |\n",
      "episode: 268 | score avg: -201.91 | mem size  34958 |\n",
      "episode: 269 | score avg: -196.89 | mem size  35170 |\n",
      "episode: 270 | score avg: -194.15 | mem size  35252 |\n",
      "episode: 271 | score avg: -203.34 | mem size  35542 |\n",
      "episode: 272 | score avg: -196.86 | mem size  35659 |\n",
      "episode: 273 | score avg: -206.29 | mem size  35922 |\n",
      "episode: 274 | score avg: -193.98 | mem size  36106 |\n",
      "episode: 275 | score avg: -188.75 | mem size  36233 |\n",
      "episode: 276 | score avg: -190.42 | mem size  36335 |\n",
      "episode: 277 | score avg: -186.97 | mem size  36416 |\n",
      "episode: 278 | score avg: -192.56 | mem size  36494 |\n",
      "episode: 279 | score avg: -179.09 | mem size  36620 |\n",
      "episode: 280 | score avg: -178.57 | mem size  36729 |\n",
      "episode: 281 | score avg: -182.46 | mem size  36866 |\n",
      "episode: 282 | score avg: -169.15 | mem size  36970 |\n",
      "episode: 283 | score avg: -160.80 | mem size  37137 |\n",
      "episode: 284 | score avg: -151.71 | mem size  37250 |\n",
      "episode: 285 | score avg: -204.84 | mem size  37931 |\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env.close()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}