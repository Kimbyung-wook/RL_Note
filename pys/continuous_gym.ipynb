{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('tf240': conda)"
  },
  "interpreter": {
   "hash": "fbba320975a9114d2433fba427f26c389728c846a7c4900c481dce2a1a9f6231"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Find RL_Note path and append sys path\r\n",
    "import os, sys\r\n",
    "cwd = os.getcwd()\r\n",
    "pos = cwd.find('RL_Note')\r\n",
    "root_path = cwd[0:pos] + 'RL_Note'\r\n",
    "sys.path.append(root_path)\r\n",
    "print(root_path)\r\n",
    "workspace_path = root_path + \"\\\\pys\""
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "d:\\RL_Note\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import gym\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from env_config  import env_configs\r\n",
    "from pys.agent.ddpg_agent   import DDPGAgent\r\n",
    "from pys.agent.td3_agent    import TD3Agent\r\n",
    "from pys.agent.sac_agent    import SACAgent\r\n",
    "from pys.gyms.functions import lunarlandercontinuous_done as done_function\r\n",
    "from pys.gyms.functions import lunarlandercontinuous_reward as reward_function"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set Environment and Agent"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "cfg = {\\\r\n",
    "        \"ENV\":\"Pendulum-v0\",\\\r\n",
    "        # \"ENV\":\"LunarLanderContinuous-v2\",\\\r\n",
    "        # \"ENV\":\"MountainCarContinuous-v0\",\\\r\n",
    "        \"RL\":{\r\n",
    "            \"ALGORITHM\":\"DDPG\",\\\r\n",
    "            \"NETWORK\":{\r\n",
    "                \"ACTOR\":[64,64],\\\r\n",
    "                \"CRITIC\":\r\n",
    "                {\r\n",
    "                    \"STATE\":[16,32],\\\r\n",
    "                    \"ACTION\":[32,32],\\\r\n",
    "                    \"CONCAT\":[64,64]\r\n",
    "                }\r\n",
    "            }\r\n",
    "        },\\\r\n",
    "        \"ER\":\\\r\n",
    "            {\r\n",
    "                \"ALGORITHM\":\"PER\",\\\r\n",
    "                \"REPLAY_N\":8,\\\r\n",
    "                \"STRATEGY\":\"EPISODE\",\\\r\n",
    "                \"REWARD_FUNC\":reward_function,\\\r\n",
    "                \"DONE_FUNC\":done_function,\\\r\n",
    "            },\\\r\n",
    "        \"BATCH_SIZE\":32,\\\r\n",
    "        \"TRAIN_START\":2000,\\\r\n",
    "        \"MEMORY_SIZE\":50000,\\\r\n",
    "        \"ADD_NAME\":\"\",\r\n",
    "        }\r\n",
    "env_config = env_configs[cfg[\"ENV\"]]\r\n",
    "if cfg[\"ER\"] == \"HER\":\r\n",
    "    FILENAME = cfg[\"ENV\"] + '_' + cfg[\"RL\"][\"ALGORITHM\"] + '_' + cfg[\"ER\"][\"ALGORITHM\"] + '_' + cfg[\"HER\"][\"STRATEGY\"]\r\n",
    "else:\r\n",
    "    FILENAME = cfg[\"ENV\"] + '_' + cfg[\"RL\"][\"ALGORITHM\"] + '_' + cfg[\"ER\"][\"ALGORITHM\"]\r\n",
    "EPISODES = env_config[\"EPISODES\"]\r\n",
    "END_SCORE = env_config[\"END_SCORE\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "%matplotlib tk\r\n",
    "\r\n",
    "figure = plt.gcf()\r\n",
    "figure.set_size_inches(8,6)\r\n",
    "env = gym.make(cfg[\"ENV\"])\r\n",
    "if cfg[\"RL\"][\"ALGORITHM\"] == \"DDPG\":\r\n",
    "    agent = DDPGAgent(env, cfg)\r\n",
    "elif cfg[\"RL\"][\"ALGORITHM\"] == \"TD3\":\r\n",
    "    agent = TD3Agent(env, cfg)\r\n",
    "elif cfg[\"RL\"][\"ALGORITHM\"] == \"SAC\":\r\n",
    "    agent = SACAgent(env, cfg)\r\n",
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "    scores_avg, scores_raw, episodes, losses = [], [], [], []\r\n",
    "    critic_mean, actor_mean = [], []\r\n",
    "    score_avg = 0\r\n",
    "    end = False\r\n",
    "    show_media_info = True\r\n",
    "    goal = np.array([1.0,0.0,0.0])\r\n",
    "    \r\n",
    "    for e in range(EPISODES):\r\n",
    "        done = False\r\n",
    "        score = 0\r\n",
    "        state = env.reset()\r\n",
    "        critic_losses = []\r\n",
    "        actor_losses = []\r\n",
    "        while not done:\r\n",
    "            # if e%100 == 0:\r\n",
    "            #     env.render()\r\n",
    "            # Interact with env.\r\n",
    "            action = agent.get_action(state)\r\n",
    "            next_state, reward, done, info = env.step(action)\r\n",
    "            agent.remember(state, action, reward, next_state, done, goal)\r\n",
    "            critic_loss, actor_loss = agent.train_model()\r\n",
    "            state = next_state\r\n",
    "            # \r\n",
    "            score += reward\r\n",
    "            critic_losses.append(critic_loss)\r\n",
    "            actor_losses.append(actor_loss)\r\n",
    "            if show_media_info:\r\n",
    "                print(\"-------------- Variable shapes --------------\")\r\n",
    "                print(\"State Shape : \", np.shape(state))\r\n",
    "                print(\"Action Shape : \", np.shape(action))\r\n",
    "                print(\"Reward Shape : \", np.shape(reward))\r\n",
    "                print(\"done Shape : \", np.shape(done))\r\n",
    "                print(\"---------------------------------------------\")\r\n",
    "                show_media_info = False\r\n",
    "            if done:\r\n",
    "                score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\r\n",
    "                print(\"episode: {0:3d} | score avg: {1:3.2f} | mem size {2:6d} |\"\r\n",
    "                    .format(e, score_avg, len(agent.memory)))\r\n",
    "\r\n",
    "                episodes.append(e)\r\n",
    "                scores_avg.append(score_avg)\r\n",
    "                scores_raw.append(score)\r\n",
    "                critic_mean.append(np.mean(critic_losses))\r\n",
    "                actor_mean.append(np.mean(actor_losses))\r\n",
    "                # View data\r\n",
    "                plt.clf()\r\n",
    "                plt.subplot(311)\r\n",
    "                plt.plot(episodes, scores_avg, 'b')\r\n",
    "                plt.plot(episodes, scores_raw, 'b', alpha=0.8, linewidth=0.5)\r\n",
    "                plt.xlabel('episode'); plt.ylabel('average score'); plt.grid()\r\n",
    "                plt.title(cfg[\"ENV\"] +'_' + cfg[\"RL\"] +'_' + cfg[\"ER\"])\r\n",
    "                plt.subplot(312)\r\n",
    "                plt.plot(episodes, critic_mean, 'b.',markersize=3)\r\n",
    "                plt.xlabel('episode'); plt.ylabel('critic loss'); plt.grid()\r\n",
    "                plt.subplot(313)\r\n",
    "                plt.plot(episodes, actor_mean, 'b.',markersize=3)\r\n",
    "                plt.xlabel('episode'); plt.ylabel('actor loss'); plt.grid()\r\n",
    "                # plt.savefig(workspace_path + \"\\\\result\\\\img\\\\\" + FILENAME + \"_TF.jpg\", dpi=100)\r\n",
    "\r\n",
    "                # 이동 평균이 0 이상일 때 종료\r\n",
    "                if score_avg > END_SCORE:\r\n",
    "                    # agent.save_model(workspace_path + \"\\\\result\\\\save_model\\\\\")\r\n",
    "                    end = True\r\n",
    "                    break\r\n",
    "        if end == True:\r\n",
    "            env.close()\r\n",
    "            # np.save(workspace_path + \"\\\\result\\\\data\\\\\" + FILENAME + \"_TF_epi\",  episodes)\r\n",
    "            # np.save(workspace_path + \"\\\\result\\\\data\\\\\" + FILENAME + \"_TF_scores_avg\",scores_avg)\r\n",
    "            # np.save(workspace_path + \"\\\\result\\\\data\\\\\" + FILENAME + \"_TF_scores_raw\",scores_raw)\r\n",
    "            # np.save(workspace_path + \"\\\\result\\\\data\\\\\" + FILENAME + \"_TF_critic_mean\",critic_mean)\r\n",
    "            # np.save(workspace_path + \"\\\\result\\\\data\\\\\" + FILENAME + \"_TF_actor_mean\",actor_mean)\r\n",
    "            print(\"End\")\r\n",
    "            break"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'ADD_NAME'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11876/3256993725.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ENV\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"RL\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ALGORITHM\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"DDPG\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDDPGAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"RL\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ALGORITHM\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"TD3\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTD3Agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\RL_Note\\pys\\agent\\ddpg_agent.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, env, cfg)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ER\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ALGORITHM\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"HER\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ER\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"STRATEGY\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ADD_NAME\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m# Experience Replay\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ADD_NAME'"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\user\\.conda\\envs\\tf240\\lib\\site-packages\\ipykernel\\eventloops.py:256: RuntimeWarning: coroutine 'Kernel.do_one_iteration' was never awaited\n",
      "  self.func()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env = gym.make(cfg[\"ENV\"])\r\n",
    "if cfg[\"RL\"] == \"DDPG\":\r\n",
    "    agent = DDPGAgent(env, cfg)\r\n",
    "elif cfg[\"RL\"] == \"TD3\":\r\n",
    "    agent = TD3Agent(env, cfg)\r\n",
    "elif cfg[\"RL\"] == \"SAC\":\r\n",
    "    agent = SACAgent(env, cfg)\r\n",
    "agent.load_model(workspace_path + \"\\\\result\\\\save_model\\\\\")\r\n",
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "    score_avg = 0\r\n",
    "    for e in range(10):\r\n",
    "        done = False\r\n",
    "        score = 0\r\n",
    "        state = env.reset()\r\n",
    "        while not done:\r\n",
    "            env.render()\r\n",
    "            action = agent.get_action(state)\r\n",
    "            next_state, reward, done, info = env.step(action)\r\n",
    "            state = next_state\r\n",
    "            # \r\n",
    "            score += reward\r\n",
    "            if done:\r\n",
    "                score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\r\n",
    "                print(\"episode: {0:3d} | score avg: {1:3.2f} |\"\r\n",
    "                    .format(e+1, score_avg))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}