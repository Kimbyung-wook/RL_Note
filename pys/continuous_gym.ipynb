{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('TF240': conda)",
   "metadata": {
    "interpreter": {
     "hash": "8f237aa33f5a133d3a67a1e00bf3cb9d47b6c38bcc6ab493273f5d4df41b8866"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "e:\\MyNote\\RL_Note\n"
     ]
    }
   ],
   "source": [
    "# Find RL_Note path and append sys path\n",
    "import os, sys\n",
    "cwd = os.getcwd()\n",
    "pos = cwd.find('RL_Note')\n",
    "root_path = cwd[0:pos] + 'RL_Note'\n",
    "sys.path.append(root_path)\n",
    "print(root_path)\n",
    "workspace_path = root_path + \"\\\\pys\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from env_config  import env_configs\n",
    "from pys.agent.ddpg_agent   import DDPGAgent\n",
    "from pys.agent.td3_agent    import TD3Agent\n",
    "from pys.agent.sac_agent    import SACAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def done_function(state):\n",
    "    return False\n",
    "\n",
    "def reward_function(state, action, next_state, done):\n",
    "    costh   = state[0]\n",
    "    sinth   = state[1]\n",
    "    th      = np.arctan2(sinth,costh)\n",
    "    thdot   = state[2]\n",
    "    u       = action[0]\n",
    "    costs   = th ** 2 + 0.1 * thdot + 0.001 * (u ** 2)\n",
    "    return -costs"
   ]
  },
  {
   "source": [
    "Set Environment and Agent"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\\\n",
    "        \"ENV\":\"Pendulum-v0\",\\\n",
    "        # \"ENV\":\"LunarLanderContinuous-v2\",\\\n",
    "        \"RL\":\"DDPG\",\\\n",
    "        \"ER\":\"ER\",\\\n",
    "        \"HER\":\\\n",
    "            {\n",
    "                \"REPLAY_N\":8,\\\n",
    "                \"STRATEGY\":\"EPISODE\",\\\n",
    "                \"REWARD_FUNC\":reward_function,\\\n",
    "                \"DONE_FUNC\":done_function,\\\n",
    "            },\\\n",
    "        \"BATCH_SIZE\":32,\\\n",
    "        }\n",
    "env_config = env_configs[cfg[\"ENV\"]]\n",
    "if cfg[\"ER\"] == \"HER\":\n",
    "    FILENAME = cfg[\"ENV\"] + '_' + cfg[\"RL\"] + '_' + cfg[\"ER\"] + '_' + cfg[\"HER\"][\"STRATEGY\"]\n",
    "else:\n",
    "    FILENAME = cfg[\"ENV\"] + '_' + cfg[\"RL\"] + '_' + cfg[\"ER\"]\n",
    "EPISODES = env_config[\"EPISODES\"]\n",
    "END_SCORE = env_config[\"END_SCORE\"]"
   ]
  },
  {
   "source": [
    "Train Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pendulum-v0_DDPG_ER\n",
      "States 3, Actions 1\n",
      "1 th Action space -2.00 ~ 2.00\n",
      "-------------- Variable shapes --------------\n",
      "State Shape :  (3,)\n",
      "Action Shape :  (1,)\n",
      "Reward Shape :  ()\n",
      "done Shape :  ()\n",
      "---------------------------------------------\n",
      "episode:   0 | score avg: -1790.96 | mem size    200 |\n",
      "episode:   1 | score avg: -1798.01 | mem size    400 |\n",
      "episode:   2 | score avg: -1744.76 | mem size    600 |\n",
      "episode:   3 | score avg: -1676.79 | mem size    800 |\n",
      "episode:   4 | score avg: -1624.78 | mem size   1000 |\n",
      "episode:   5 | score avg: -1563.71 | mem size   1200 |\n",
      "episode:   6 | score avg: -1533.00 | mem size   1400 |\n",
      "episode:   7 | score avg: -1501.95 | mem size   1600 |\n",
      "episode:   8 | score avg: -1508.92 | mem size   1800 |\n",
      "Start to train, check batch shapes\n",
      "**** shape of mini_batch (32, 5) <class 'list'>\n",
      "**** shape of states (32, 3) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of actions (32, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of rewards (32, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of next_states (32, 3) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of dones (32, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "episode:   9 | score avg: -1464.24 | mem size   2000 |\n",
      "episode:  10 | score avg: -1462.31 | mem size   2200 |\n",
      "episode:  11 | score avg: -1461.89 | mem size   2400 |\n",
      "episode:  12 | score avg: -1456.01 | mem size   2600 |\n",
      "episode:  13 | score avg: -1434.90 | mem size   2800 |\n",
      "episode:  14 | score avg: -1427.00 | mem size   3000 |\n",
      "episode:  15 | score avg: -1427.60 | mem size   3200 |\n",
      "episode:  16 | score avg: -1399.35 | mem size   3400 |\n",
      "episode:  17 | score avg: -1400.88 | mem size   3600 |\n",
      "episode:  18 | score avg: -1405.27 | mem size   3800 |\n",
      "episode:  19 | score avg: -1384.05 | mem size   4000 |\n",
      "episode:  20 | score avg: -1360.63 | mem size   4200 |\n",
      "episode:  21 | score avg: -1336.61 | mem size   4400 |\n",
      "episode:  22 | score avg: -1309.50 | mem size   4600 |\n",
      "episode:  23 | score avg: -1273.70 | mem size   4800 |\n",
      "episode:  24 | score avg: -1244.66 | mem size   5000 |\n",
      "episode:  25 | score avg: -1170.52 | mem size   5200 |\n",
      "episode:  26 | score avg: -1090.66 | mem size   5400 |\n",
      "episode:  27 | score avg: -1018.39 | mem size   5600 |\n",
      "episode:  28 | score avg: -1013.72 | mem size   5800 |\n",
      "episode:  29 | score avg: -975.14 | mem size   6000 |\n",
      "episode:  30 | score avg: -890.04 | mem size   6200 |\n",
      "episode:  31 | score avg: -813.67 | mem size   6400 |\n",
      "episode:  32 | score avg: -781.23 | mem size   6600 |\n",
      "episode:  33 | score avg: -763.55 | mem size   6800 |\n",
      "episode:  34 | score avg: -698.92 | mem size   7000 |\n",
      "episode:  35 | score avg: -713.60 | mem size   7200 |\n",
      "episode:  36 | score avg: -795.78 | mem size   7400 |\n",
      "episode:  37 | score avg: -740.27 | mem size   7600 |\n",
      "episode:  38 | score avg: -678.53 | mem size   7800 |\n",
      "episode:  39 | score avg: -635.80 | mem size   8000 |\n",
      "episode:  40 | score avg: -648.05 | mem size   8200 |\n",
      "episode:  41 | score avg: -716.37 | mem size   8400 |\n",
      "episode:  42 | score avg: -718.39 | mem size   8600 |\n",
      "episode:  43 | score avg: -659.02 | mem size   8800 |\n",
      "episode:  44 | score avg: -642.81 | mem size   9000 |\n",
      "episode:  45 | score avg: -590.79 | mem size   9200 |\n",
      "episode:  46 | score avg: -531.90 | mem size   9400 |\n",
      "episode:  47 | score avg: -491.13 | mem size   9600 |\n",
      "episode:  48 | score avg: -466.30 | mem size   9800 |\n",
      "episode:  49 | score avg: -419.90 | mem size  10000 |\n",
      "episode:  50 | score avg: -401.63 | mem size  10200 |\n",
      "episode:  51 | score avg: -385.51 | mem size  10400 |\n",
      "episode:  52 | score avg: -358.63 | mem size  10600 |\n",
      "episode:  53 | score avg: -367.07 | mem size  10800 |\n",
      "episode:  54 | score avg: -380.55 | mem size  11000 |\n",
      "episode:  55 | score avg: -393.14 | mem size  11200 |\n",
      "episode:  56 | score avg: -432.73 | mem size  11400 |\n",
      "episode:  57 | score avg: -424.65 | mem size  11600 |\n",
      "episode:  58 | score avg: -407.95 | mem size  11800 |\n",
      "episode:  59 | score avg: -403.20 | mem size  12000 |\n",
      "episode:  60 | score avg: -413.51 | mem size  12200 |\n",
      "episode:  61 | score avg: -384.44 | mem size  12400 |\n",
      "episode:  62 | score avg: -358.04 | mem size  12600 |\n",
      "episode:  63 | score avg: -359.67 | mem size  12800 |\n",
      "episode:  64 | score avg: -348.54 | mem size  13000 |\n",
      "episode:  65 | score avg: -409.80 | mem size  13200 |\n",
      "episode:  66 | score avg: -393.53 | mem size  13400 |\n",
      "episode:  67 | score avg: -475.44 | mem size  13600 |\n",
      "episode:  68 | score avg: -543.42 | mem size  13800 |\n",
      "episode:  69 | score avg: -512.96 | mem size  14000 |\n",
      "episode:  70 | score avg: -486.40 | mem size  14200 |\n",
      "episode:  71 | score avg: -438.31 | mem size  14400 |\n",
      "episode:  72 | score avg: -407.90 | mem size  14600 |\n",
      "episode:  73 | score avg: -368.10 | mem size  14800 |\n",
      "episode:  74 | score avg: -367.36 | mem size  15000 |\n",
      "episode:  75 | score avg: -331.36 | mem size  15200 |\n",
      "episode:  76 | score avg: -299.15 | mem size  15400 |\n",
      "episode:  77 | score avg: -269.96 | mem size  15600 |\n",
      "episode:  78 | score avg: -255.57 | mem size  15800 |\n",
      "episode:  79 | score avg: -266.81 | mem size  16000 |\n",
      "episode:  80 | score avg: -275.59 | mem size  16200 |\n",
      "episode:  81 | score avg: -248.59 | mem size  16400 |\n",
      "episode:  82 | score avg: -249.05 | mem size  16600 |\n",
      "episode:  83 | score avg: -224.75 | mem size  16800 |\n",
      "episode:  84 | score avg: -250.27 | mem size  17000 |\n",
      "episode:  85 | score avg: -273.63 | mem size  17200 |\n",
      "episode:  86 | score avg: -270.12 | mem size  17400 |\n",
      "episode:  87 | score avg: -279.45 | mem size  17600 |\n",
      "episode:  88 | score avg: -251.75 | mem size  17800 |\n",
      "episode:  89 | score avg: -250.05 | mem size  18000 |\n",
      "episode:  90 | score avg: -249.27 | mem size  18200 |\n",
      "episode:  91 | score avg: -236.75 | mem size  18400 |\n",
      "episode:  92 | score avg: -323.29 | mem size  18600 |\n",
      "episode:  93 | score avg: -315.08 | mem size  18800 |\n",
      "episode:  94 | score avg: -295.83 | mem size  19000 |\n",
      "episode:  95 | score avg: -278.84 | mem size  19200 |\n",
      "episode:  96 | score avg: -262.75 | mem size  19400 |\n",
      "episode:  97 | score avg: -261.15 | mem size  19600 |\n",
      "episode:  98 | score avg: -235.60 | mem size  19800 |\n",
      "episode:  99 | score avg: -224.39 | mem size  20000 |\n",
      "episode: 100 | score avg: -225.74 | mem size  20200 |\n",
      "episode: 101 | score avg: -215.56 | mem size  20400 |\n",
      "episode: 102 | score avg: -207.14 | mem size  20600 |\n",
      "episode: 103 | score avg: -199.20 | mem size  20800 |\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "%matplotlib tk\n",
    "\n",
    "figure = plt.gcf()\n",
    "figure.set_size_inches(8,6)\n",
    "env = gym.make(cfg[\"ENV\"])\n",
    "if cfg[\"RL\"] == \"DDPG\":\n",
    "    agent = DDPGAgent(env, cfg)\n",
    "elif cfg[\"RL\"] == \"TD3\":\n",
    "    agent = TD3Agent(env, cfg)\n",
    "elif cfg[\"RL\"] == \"SAC\":\n",
    "    agent = SACAgent(env, cfg)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scores_avg, scores_raw, episodes, losses = [], [], [], []\n",
    "    critic_mean, actor_mean = [], []\n",
    "    score_avg = 0\n",
    "    end = False\n",
    "    show_media_info = True\n",
    "    goal = np.array([1.0,0.0,0.0])\n",
    "    \n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        critic_losses = []\n",
    "        actor_losses = []\n",
    "        while not done:\n",
    "            # if e%100 == 0:\n",
    "            #     env.render()\n",
    "            # Interact with env.\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done, goal)\n",
    "            critic_loss, actor_loss = agent.train_model()\n",
    "            state = next_state\n",
    "            # \n",
    "            score += reward\n",
    "            critic_losses.append(critic_loss)\n",
    "            actor_losses.append(actor_loss)\n",
    "            if show_media_info:\n",
    "                print(\"-------------- Variable shapes --------------\")\n",
    "                print(\"State Shape : \", np.shape(state))\n",
    "                print(\"Action Shape : \", np.shape(action))\n",
    "                print(\"Reward Shape : \", np.shape(reward))\n",
    "                print(\"done Shape : \", np.shape(done))\n",
    "                print(\"---------------------------------------------\")\n",
    "                show_media_info = False\n",
    "            if done:\n",
    "                score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\n",
    "                print(\"episode: {0:3d} | score avg: {1:3.2f} | mem size {2:6d} |\"\n",
    "                    .format(e, score_avg, len(agent.memory)))\n",
    "\n",
    "                episodes.append(e)\n",
    "                scores_avg.append(score_avg)\n",
    "                scores_raw.append(score)\n",
    "                critic_mean.append(np.mean(critic_losses))\n",
    "                actor_mean.append(np.mean(actor_losses))\n",
    "                # View data\n",
    "                plt.clf()\n",
    "                plt.subplot(311)\n",
    "                plt.plot(episodes, scores_avg, 'b')\n",
    "                plt.plot(episodes, scores_raw, 'b', alpha=0.8, linewidth=0.5)\n",
    "                plt.xlabel('episode'); plt.ylabel('average score'); plt.grid()\n",
    "                plt.title(cfg[\"ENV\"] +'_' + cfg[\"RL\"] +'_' + cfg[\"ER\"])\n",
    "                plt.subplot(312)\n",
    "                plt.plot(episodes, critic_mean, 'b.',markersize=3)\n",
    "                plt.xlabel('episode'); plt.ylabel('critic loss'); plt.grid()\n",
    "                plt.subplot(313)\n",
    "                plt.plot(episodes, actor_mean, 'b.',markersize=3)\n",
    "                plt.xlabel('episode'); plt.ylabel('actor loss'); plt.grid()\n",
    "                plt.savefig(workspace_path + \"\\\\result\\\\img\\\\\" + FILENAME + \"_TF.jpg\", dpi=100)\n",
    "\n",
    "                # 이동 평균이 0 이상일 때 종료\n",
    "                if score_avg > END_SCORE:\n",
    "                    agent.save_model(workspace_path + \"\\\\result\\\\save_model\\\\\")\n",
    "                    end = True\n",
    "                    break\n",
    "        if end == True:\n",
    "            env.close()\n",
    "            np.save(workspace_path + \"\\\\result\\\\data\\\\\" + FILENAME + \"_TF_epi\",  episodes)\n",
    "            np.save(workspace_path + \"\\\\result\\\\data\\\\\" + FILENAME + \"_TF_scores_avg\",scores_avg)\n",
    "            np.save(workspace_path + \"\\\\result\\\\data\\\\\" + FILENAME + \"_TF_scores_raw\",scores_raw)\n",
    "            np.save(workspace_path + \"\\\\result\\\\data\\\\\" + FILENAME + \"_TF_critic_mean\",critic_mean)\n",
    "            np.save(workspace_path + \"\\\\result\\\\data\\\\\" + FILENAME + \"_TF_actor_mean\",actor_mean)\n",
    "            print(\"End\")\n",
    "            break"
   ]
  },
  {
   "source": [
    "Test Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Pendulum-v0_DDPG_ER\n",
      "States 3, Actions 1\n",
      "1 th Action space -2.00 ~ 2.00\n",
      "episode:   1 | score avg: -8.88 |\n",
      "episode:   2 | score avg: -45.43 |\n",
      "episode:   3 | score avg: -65.77 |\n",
      "episode:   4 | score avg: -84.52 |\n",
      "episode:   5 | score avg: -89.37 |\n",
      "episode:   6 | score avg: -145.62 |\n",
      "episode:   7 | score avg: -260.45 |\n",
      "episode:   8 | score avg: -370.85 |\n",
      "episode:   9 | score avg: -463.26 |\n",
      "episode:  10 | score avg: -543.44 |\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(cfg[\"ENV\"])\n",
    "if cfg[\"RL\"] == \"DDPG\":\n",
    "    agent = DDPGAgent(env, cfg)\n",
    "elif cfg[\"RL\"] == \"TD3\":\n",
    "    agent = TD3Agent(env, cfg)\n",
    "elif cfg[\"RL\"] == \"SAC\":\n",
    "    agent = SACAgent(env, cfg)\n",
    "agent.load_model(workspace_path + \"\\\\result\\\\save_model\\\\\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    score_avg = 0\n",
    "    for e in range(10):\n",
    "        done = False\n",
    "        score = 0\n",
    "        state = env.reset()\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            state = next_state\n",
    "            # \n",
    "            score += reward\n",
    "            if done:\n",
    "                score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\n",
    "                print(\"episode: {0:3d} | score avg: {1:3.2f} |\"\n",
    "                    .format(e+1, score_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}