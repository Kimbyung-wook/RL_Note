{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3710jvsc74a57bd0d58761fb0507901c753af48a6b8c3f78eaa64b9d21040326b9f6cfad30dd9f9c",
   "display_name": "Python 3.7.10 64-bit ('tf240_gpu': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import sys\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow_probability import distributions as tfd\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "dist = tfd.Normal(loc=0., scale=3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC(tf.keras.Model):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(SAC, self).__init__()\n",
    "        self.actor_fc1 = Dense(24,activation='tanh')\n",
    "        self.actor_mu  = Dense(action_size,kernel_initializer=RandomUniform(-1e-3,1e-3))\n",
    "        self.actor_sig = Dense(action_size, activation='sigmoid',kernel_initializer=RandomUniform(-1e-3,1e-3))\n",
    "\n",
    "        self.critic_fc1 = Dense(24,activation='tanh')\n",
    "        self.critic_fc2 = Dense(24,activation='tanh')\n",
    "        self.critic_out = Dense(1,kernel_initializer=tf.keras.initializers.RandomUniform(-1e-3,1e-3))\n",
    "\n",
    "    def call(self,x):\n",
    "        x1  = self.actor_fc1(x)\n",
    "        mu  = self.actor_mu(x1)\n",
    "        sig = self.actor_sig(x1)\n",
    "        sig+= 1e-5\n",
    "\n",
    "        x1  = self.critic_fc1(x)\n",
    "        x2  = self.critic_fc2(x1)\n",
    "        val = self.critic_out(x2)\n",
    "\n",
    "        return mu, sig, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACAgent:\n",
    "    def __init__(self, state_size, action_size, act_min, act_max):\n",
    "        self.state_size = state_size\n",
    "        self.action_size= action_size\n",
    "        self.act_min = act_min\n",
    "        self.act_max = act_max\n",
    "\n",
    "        # Hyper params for learning\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "\n",
    "        self.model     = SAC(self.state_size, self.action_size)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr=self.learning_rate, clipnorm=1.0)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        mu, sigma, _ = self.model(state)\n",
    "        distrib = tfd.Normal(loc=mu[0], scale=sigma[0])\n",
    "        action = distrib.sample([1])[0]\n",
    "        return np.clip(action, self.act_min, self.act_max)\n",
    "\n",
    "    def train_model(self, state, action, reward, next_state, done):\n",
    "        model_params = self.model.trainable_variables\n",
    "        with tf.GradientTape() as tape:\n",
    "            mu, sigma, value = self.model(state)\n",
    "            _, _, next_value = self.model(next_state)\n",
    "            target = reward + (1 - done) * self.discount_factor * next_value[0]\n",
    "\n",
    "            # Policy Network\n",
    "            advantage = tf.stop_gradient(target - value[0])\n",
    "            distrib = tfd.Normal(loc=mu[0], scale=sigma[0])\n",
    "            action_prob = distrib.prob([action])[0]\n",
    "            cross_entropy = - tf.math.log(action_prob + 1e-5)\n",
    "            actor_loss = tf.reduce_mean(cross_entropy * advantage)\n",
    "\n",
    "            # Value Network\n",
    "            critic_loss = 0.5 * tf.square(tf.stop_gradient(target) - value[0])\n",
    "            critic_loss = tf.reduce_mean(critic_loss)\n",
    "\n",
    "            loss = 0.1 * actor_loss + critic_loss\n",
    "        \n",
    "        # Update weights\n",
    "        grads = tape.gradient(loss, model_params)\n",
    "        self.optimizer.apply_gradients(zip(grads, model_params))\n",
    "        return loss, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "episode:   0 | score avg: -1454.82 | loss: 0.088 | sigma: 0.495\n",
      "episode:   1 | score avg: -1395.70 | loss: 0.143 | sigma: 0.378\n",
      "episode:   2 | score avg: -1430.40 | loss: 0.157 | sigma: 0.513\n",
      "episode:   3 | score avg: -1438.18 | loss: 0.200 | sigma: 0.410\n",
      "episode:   4 | score avg: -1389.85 | loss: 0.156 | sigma: 0.371\n",
      "episode:   5 | score avg: -1340.41 | loss: 0.076 | sigma: 0.457\n",
      "episode:   6 | score avg: -1385.89 | loss: 0.151 | sigma: 0.452\n",
      "episode:   7 | score avg: -1417.44 | loss: 0.212 | sigma: 0.447\n",
      "episode:   8 | score avg: -1388.36 | loss: 0.265 | sigma: 0.478\n",
      "episode:   9 | score avg: -1437.53 | loss: 0.277 | sigma: 0.480\n",
      "episode:  10 | score avg: -1469.69 | loss: 0.284 | sigma: 0.488\n",
      "episode:  11 | score avg: -1503.70 | loss: 0.276 | sigma: 0.495\n",
      "episode:  12 | score avg: -1533.00 | loss: 0.290 | sigma: 0.477\n",
      "episode:  13 | score avg: -1553.38 | loss: 0.301 | sigma: 0.503\n",
      "episode:  14 | score avg: -1527.89 | loss: 0.339 | sigma: 0.498\n",
      "episode:  15 | score avg: -1524.05 | loss: 0.329 | sigma: 0.547\n",
      "episode:  16 | score avg: -1543.98 | loss: 0.316 | sigma: 0.572\n",
      "episode:  17 | score avg: -1547.04 | loss: 0.312 | sigma: 0.574\n",
      "episode:  18 | score avg: -1556.40 | loss: 0.307 | sigma: 0.519\n",
      "episode:  19 | score avg: -1583.21 | loss: 0.295 | sigma: 0.537\n",
      "episode:  20 | score avg: -1608.36 | loss: 0.295 | sigma: 0.550\n",
      "episode:  21 | score avg: -1629.10 | loss: 0.297 | sigma: 0.549\n",
      "episode:  22 | score avg: -1644.73 | loss: 0.289 | sigma: 0.585\n",
      "episode:  23 | score avg: -1645.62 | loss: 0.300 | sigma: 0.551\n",
      "episode:  24 | score avg: -1634.32 | loss: 0.319 | sigma: 0.554\n",
      "episode:  25 | score avg: -1642.26 | loss: 0.315 | sigma: 0.581\n",
      "episode:  26 | score avg: -1667.26 | loss: 0.295 | sigma: 0.599\n",
      "episode:  27 | score avg: -1682.82 | loss: 0.304 | sigma: 0.603\n",
      "episode:  28 | score avg: -1703.59 | loss: 0.298 | sigma: 0.611\n",
      "episode:  29 | score avg: -1715.16 | loss: 0.300 | sigma: 0.618\n",
      "episode:  30 | score avg: -1731.80 | loss: 0.296 | sigma: 0.639\n",
      "episode:  31 | score avg: -1735.23 | loss: 0.296 | sigma: 0.646\n",
      "episode:  32 | score avg: -1741.62 | loss: 0.295 | sigma: 0.646\n",
      "episode:  33 | score avg: -1741.59 | loss: 0.297 | sigma: 0.667\n",
      "episode:  34 | score avg: -1734.94 | loss: 0.293 | sigma: 0.715\n",
      "episode:  35 | score avg: -1747.81 | loss: 0.289 | sigma: 0.725\n",
      "episode:  36 | score avg: -1757.38 | loss: 0.288 | sigma: 0.697\n",
      "episode:  37 | score avg: -1758.46 | loss: 0.299 | sigma: 0.680\n",
      "episode:  38 | score avg: -1765.49 | loss: 0.291 | sigma: 0.711\n",
      "episode:  39 | score avg: -1757.11 | loss: 0.293 | sigma: 0.694\n",
      "episode:  40 | score avg: -1756.95 | loss: 0.294 | sigma: 0.713\n",
      "episode:  41 | score avg: -1750.73 | loss: 0.290 | sigma: 0.760\n",
      "episode:  42 | score avg: -1737.92 | loss: 0.305 | sigma: 0.727\n",
      "episode:  43 | score avg: -1741.84 | loss: 0.295 | sigma: 0.760\n",
      "episode:  44 | score avg: -1739.62 | loss: 0.290 | sigma: 0.786\n",
      "episode:  45 | score avg: -1734.98 | loss: 0.295 | sigma: 0.724\n",
      "episode:  46 | score avg: -1731.64 | loss: 0.287 | sigma: 0.759\n",
      "episode:  47 | score avg: -1726.83 | loss: 0.291 | sigma: 0.740\n",
      "episode:  48 | score avg: -1724.67 | loss: 0.294 | sigma: 0.751\n",
      "episode:  49 | score avg: -1716.99 | loss: 0.294 | sigma: 0.754\n",
      "episode:  50 | score avg: -1696.23 | loss: 0.299 | sigma: 0.775\n",
      "episode:  51 | score avg: -1703.24 | loss: 0.294 | sigma: 0.755\n",
      "episode:  52 | score avg: -1699.76 | loss: 0.294 | sigma: 0.788\n",
      "episode:  53 | score avg: -1677.89 | loss: 0.295 | sigma: 0.827\n",
      "episode:  54 | score avg: -1657.51 | loss: 0.289 | sigma: 0.829\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-51b666513a1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mloss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0msigma_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-dd308a7d5562>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf240_gpu/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf240_gpu/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf240_gpu/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf240_gpu/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_RealDivGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1422\u001b[0m           array_ops.reshape(\n\u001b[1;32m   1423\u001b[0m               math_ops.reduce_sum(\n\u001b[0;32m-> 1424\u001b[0;31m                   grad * math_ops.realdiv(math_ops.realdiv(-x, y), y), ry), sy))\n\u001b[0m\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf240_gpu/lib/python3.7/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mneg\u001b[0;34m(x, name)\u001b[0m\n\u001b[1;32m   6235\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6236\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 6237\u001b[0;31m         _ctx, \"Neg\", name, x)\n\u001b[0m\u001b[1;32m   6238\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6239\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib tk\n",
    "\n",
    "ENV_NAME = 'Pendulum-v0'\n",
    "EPISODES = 1000\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(ENV_NAME)\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.shape[0]\n",
    "\n",
    "    agent = SACAgent(state_size, action_size,\n",
    "                        env.action_space.low[0],\n",
    "                        env.action_space.high[0])\n",
    "    scores, episodes, losses, sigmas = [], [], [], []\n",
    "    score_avg = 0\n",
    "\n",
    "    end = False\n",
    "    \n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        loss_list, sigma_list = [], []\n",
    "\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            env.render()\n",
    "\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "            score += reward\n",
    "            reward = 0.1 if not done or score == 500 else -1\n",
    "\n",
    "            loss, sigma = agent.train_model(state, action, reward, next_state, done)\n",
    "            loss_list.append(loss)\n",
    "            sigma_list.append(sigma)\n",
    "            \n",
    "            state = next_state\n",
    "            if done:\n",
    "                score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\n",
    "                print(\"episode: {:3d} | score avg: {:3.2f} | loss: {:.3f} | sigma: {:.3f}\".format(\n",
    "                      e, score_avg, np.mean(loss_list), np.mean(sigma)))\n",
    "\n",
    "                episodes.append(e)\n",
    "                scores.append(score_avg)\n",
    "                losses.append(np.mean(loss_list))\n",
    "                sigmas.append(np.mean(sigma))\n",
    "\n",
    "                plt.subplot(311)\n",
    "                plt.plot(episodes, scores, 'b')\n",
    "                plt.xlabel('episode')\n",
    "                plt.ylabel('average score')\n",
    "                plt.title('pendulum SAC')\n",
    "                plt.grid()\n",
    "                \n",
    "                plt.subplot(312)\n",
    "                plt.plot(episodes, sigmas, 'b')\n",
    "                plt.xlabel('episode')\n",
    "                plt.ylabel('sigma')\n",
    "                plt.grid()\n",
    "                \n",
    "                plt.subplot(313)\n",
    "                plt.plot(episodes, losses, 'b')\n",
    "                plt.xlabel('episode')\n",
    "                plt.ylabel('losses')\n",
    "                plt.grid()\n",
    "                plt.savefig(\"./save_model/pendulum_sac.png\")\n",
    "\n",
    "                # 이동 평균이 400 이상일 때 종료\n",
    "                if score_avg > 400:\n",
    "                    agent.model.save_weights(\"./save_model/pendulum_sac\", save_format=\"tf\")\n",
    "                    end = True\n",
    "                    break\n",
    "        if end == True:\n",
    "            env.close()\n",
    "            np.save('./save_model/pendulum_sac_epi',  episodes)\n",
    "            np.save('./save_model/pendulum_sac_score',scores)\n",
    "            np.save('./save_model/pendulum_sac_loss', losses)\n",
    "            np.save('./save_model/pendulum_sac_sigmas', sigmas)\n",
    "            print(\"End\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ]
}