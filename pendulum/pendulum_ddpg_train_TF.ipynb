{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('tf240': conda)"
  },
  "interpreter": {
   "hash": "fbba320975a9114d2433fba427f26c389728c846a7c4900c481dce2a1a9f6231"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import gym\r\n",
    "import sys\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras.layers import Dense, Lambda, concatenate\r\n",
    "from collections import deque\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class Actor(tf.keras.Model):\r\n",
    "    def __init__(self, state_size, action_size, action_max):\r\n",
    "        super(Actor, self).__init__()\r\n",
    "\r\n",
    "        self.action_max = action_max\r\n",
    "\r\n",
    "        self.a1 = Dense(64, activation='relu')\r\n",
    "        self.a2 = Dense(32, activation='relu')\r\n",
    "        self.a3 = Dense(12, activation='relu')\r\n",
    "        self.out= Dense(action_size, activation='tanh')\r\n",
    "\r\n",
    "    def call(self, x):\r\n",
    "        x = self.a1(x)\r\n",
    "        x = self.a2(x)\r\n",
    "        x = self.a3(x)\r\n",
    "        a = self.out(x)\r\n",
    "        a = Lambda(lambda x: x*self.action_max)(a)\r\n",
    "        return a\r\n",
    "\r\n",
    "class Critic(tf.keras.Model):\r\n",
    "    def __init__(self, state_size, action_size):\r\n",
    "        super(Critic, self).__init__()\r\n",
    "        self.s1= Dense(32, activation='relu')\r\n",
    "        self.a1= Dense(32, activation='relu')\r\n",
    "        self.c1= Dense(32, activation='relu')\r\n",
    "        self.c2= Dense(16, activation='relu')\r\n",
    "        self.out= Dense(1, activation='linear')\r\n",
    "\r\n",
    "    def call(self, state, action):\r\n",
    "        # state = state_action[0]\r\n",
    "        # action = state_action[1]\r\n",
    "\r\n",
    "        state = self.s1(state)\r\n",
    "        action = self.a1(action)\r\n",
    "        c = concatenate([state,action],axis=-1)\r\n",
    "        c = self.c1(c)\r\n",
    "        c = self.c2(c)\r\n",
    "        q = self.out(c)\r\n",
    "        return q\r\n",
    "        \r\n",
    "class OUActionNoise:\r\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\r\n",
    "        self.theta = theta\r\n",
    "        self.mean = mean\r\n",
    "        self.std_dev = std_deviation\r\n",
    "        self.dt = dt\r\n",
    "        self.x_initial = x_initial\r\n",
    "        self.reset()\r\n",
    "\r\n",
    "    def __call__(self):\r\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\r\n",
    "        x = (\r\n",
    "            self.x_prev\r\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\r\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\r\n",
    "        )\r\n",
    "        # Store x into x_prev\r\n",
    "        # Makes next noise dependent on current one\r\n",
    "        self.x_prev = x\r\n",
    "        return x\r\n",
    "\r\n",
    "    def reset(self):\r\n",
    "        if self.x_initial is not None:\r\n",
    "            self.x_prev = self.x_initial\r\n",
    "        else:\r\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class DDPGAgent(tf.keras.Model):\r\n",
    "    def __init__(self, state_size, action_size, action_max):\r\n",
    "        super(DDPGAgent, self).__init__()\r\n",
    "        self.state_size = state_size\r\n",
    "        self.action_size= action_size\r\n",
    "        self.action_max = action_max\r\n",
    "\r\n",
    "        # Hyper params for learning\r\n",
    "        self.discount_factor = 0.99\r\n",
    "        self.learning_rate_actor = 0.001\r\n",
    "        self.learning_rate_critic = 0.01\r\n",
    "        self.tau = 0.001\r\n",
    "        \r\n",
    "        # Experience Replay\r\n",
    "        self.batch_size = 256\r\n",
    "        self.train_start = 400\r\n",
    "        self.memory = deque(maxlen=20000)\r\n",
    "        \r\n",
    "        # Neural Network Architecture\r\n",
    "        self.actor        = Actor(self.state_size, self.action_size, self.action_max)\r\n",
    "        self.target_actor = Actor(self.state_size, self.action_size, self.action_max)\r\n",
    "        self.critic       = Critic(self.state_size, self.action_size)\r\n",
    "        self.target_critic= Critic(self.state_size, self.action_size)\r\n",
    "        self.optimizer_actor    = tf.keras.optimizers.Adam(lr=self.learning_rate_actor)\r\n",
    "        self.optimizer_critic   = tf.keras.optimizers.Adam(lr=self.learning_rate_critic)\r\n",
    "        self.ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=0.2 * np.ones(1))\r\n",
    "        \r\n",
    "        self.target_actor.set_weights(self.actor.get_weights())\r\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\r\n",
    "        self.update_target_model()\r\n",
    "\r\n",
    "    def remember(self, state, action, reward, next_state, done):\r\n",
    "        self.memory.append((state, action, reward, next_state, done))\r\n",
    "\r\n",
    "    def update_target_model(self):\r\n",
    "        weight          = self.actor.get_weights()\r\n",
    "        target_weight   = self.target_actor.get_weights()\r\n",
    "        print('Len of weight : ',len(weight))\r\n",
    "        print(weight)\r\n",
    "        for i in range(1,len(weight)):\r\n",
    "            target_weight[i] = self.tau * weight[i] + (1. - self.tau) * target_weight[i]\r\n",
    "        self.target_actor.set_weights(target_weight)\r\n",
    "\r\n",
    "        weight          = self.critic.get_weights()\r\n",
    "        target_weight   = self.target_critic.get_weights()\r\n",
    "        for i in range(1,len(weight)):\r\n",
    "            target_weight[i] = self.tau * weight[i] + (1. - self.tau) * target_weight[i]\r\n",
    "        self.target_critic.set_weights(target_weight)\r\n",
    "        return \r\n",
    "        \r\n",
    "    def get_action(self,state):\r\n",
    "        # Exploration and Exploitation\r\n",
    "        action = self.actor(state)\r\n",
    "        action = action.numpy()[0]\r\n",
    "        noise = self.ou_noise()\r\n",
    "        out = np.clip(action + noise, -self.action_max, self.action_max)\r\n",
    "        return out\r\n",
    "        # if (np.random.rand() <= self.epsilon):\r\n",
    "        #     return random.randrange(self.action_size)\r\n",
    "        # else:\r\n",
    "        #     return np.argmax(self.model.predict(state))\r\n",
    "        \r\n",
    "    def train_model(self):\r\n",
    "        # Train from Experience Replay\r\n",
    "        # Training Condition - Memory Size\r\n",
    "        if len(self.memory) < self.train_start:\r\n",
    "            return 0.0\r\n",
    "        # Sampling from the memory\r\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\r\n",
    "        \r\n",
    "        states      = np.array([sample[0][0] for sample in mini_batch])\r\n",
    "        actions     = np.array([sample[1] for sample in mini_batch])\r\n",
    "        rewards     = np.array([sample[2] for sample in mini_batch])\r\n",
    "        next_states = np.array([sample[3][0] for sample in mini_batch])\r\n",
    "        dones       = np.array([sample[4] for sample in mini_batch])\r\n",
    "\r\n",
    "        critic_params = self.critic.trainable_variables\r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            q               = self.critic(states,actions)\r\n",
    "            target_action   = self.target_actor(next_states)\r\n",
    "            target_q        = self.target_critic(next_states, target_action)\r\n",
    "            y = rewards + (1 - dones) * self.discount_factor * target_q\r\n",
    "            td_error = y - q\r\n",
    "            critic_loss = tf.reduce_mean(tf.square(y - q))\r\n",
    "            # critic_loss = tf.keras.losses.MSE(y,q)\r\n",
    "            \r\n",
    "        critic_grads = tape.gradient(critic_loss, critic_params)\r\n",
    "        self.optimizer_critic.apply_gradients(zip(critic_grads, critic_params))\r\n",
    "         \r\n",
    "        actor_params = self.actor.trainable_variables\r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            now_action  = self.actor(states)\r\n",
    "            now_q       = self.critic(states,now_action)\r\n",
    "            actor_loss  = -tf.reduce_mean(now_q)\r\n",
    "            \r\n",
    "        actor_grads = tape.gradient(actor_loss, actor_params)\r\n",
    "        self.optimizer_actor.apply_gradients(zip(actor_grads, actor_params))\r\n",
    "\r\n",
    "        # agent.update_target_model()\r\n",
    "        # return critic_loss, actor_loss\r\n",
    "        return"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "%matplotlib tk\r\n",
    "\r\n",
    "ENV_NAME = 'Pendulum-v0'\r\n",
    "EPISODES = 1000\r\n",
    "END_SCORE = 200\r\n",
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "    env = gym.make(ENV_NAME)\r\n",
    "    state_size = env.observation_space.shape[0]\r\n",
    "    action_size = env.action_space.shape[0]\r\n",
    "\r\n",
    "    agent = DDPGAgent(state_size, action_size,env.action_space.high[0])\r\n",
    "    # agent.get_act_lim(  env.action_space.low[0],\r\n",
    "    #                     env.action_space.high[0])\r\n",
    "    print('Env Name : ',ENV_NAME)\r\n",
    "    print('States {}, Actions {} in {} ~ {}'\r\n",
    "            .format(state_size, action_size,env.\r\n",
    "                    action_space.low[0],env.action_space.high[0]))\r\n",
    "    \r\n",
    "    scores, episodes = [], []\r\n",
    "    score_avg = 0\r\n",
    "    \r\n",
    "    end = False\r\n",
    "\r\n",
    "    for e in range(EPISODES):\r\n",
    "        # Episode initialization\r\n",
    "        done = False\r\n",
    "        score = 0\r\n",
    "\r\n",
    "        state = env.reset()\r\n",
    "        state = np.reshape(state, [1, state_size])\r\n",
    "\r\n",
    "        while not done:\r\n",
    "            # env.render()\r\n",
    "\r\n",
    "            # Interact with env.\r\n",
    "            action = agent.get_action(state)\r\n",
    "            next_state, reward, done, info = env.step(action)\r\n",
    "            next_state = np.reshape(next_state, [1, state_size])\r\n",
    "            agent.remember(state, action, reward, next_state, done)\r\n",
    "            agent.train_model()\r\n",
    "            state = next_state\r\n",
    "\r\n",
    "            score += reward\r\n",
    "            if done:\r\n",
    "\r\n",
    "                score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\r\n",
    "                print('epi: {:3d} | score avg {:3.2f} | mem length: {:4d}'\r\n",
    "                      .format(e, score_avg, len(agent.memory)))\r\n",
    "\r\n",
    "                # Save data for plot\r\n",
    "                episodes.append(e)\r\n",
    "                scores.append(score_avg)\r\n",
    "\r\n",
    "                # View data\r\n",
    "                plt.plot(episodes, scores, 'b')\r\n",
    "                plt.xlabel('episode')\r\n",
    "                plt.ylabel('average score')\r\n",
    "                plt.title('Pendulum DDPG')\r\n",
    "                plt.grid()\r\n",
    "\r\n",
    "                plt.savefig('./save_model/pendulum_ddpg_TF.png')\r\n",
    "\r\n",
    "                if score_avg > END_SCORE:\r\n",
    "                    agent.model.save_weights('./save_model/pendulum_ddpg_TF', save_format='tf')\r\n",
    "                    end = True\r\n",
    "                    break\r\n",
    "        if end == True:\r\n",
    "            env.close()\r\n",
    "            np.save('./save_model/pendulum_ddpg_TF_epi',  episodes)\r\n",
    "            np.save('./save_model/pendulum_ddpg_TF_score',scores)\r\n",
    "            print(\"End\")\r\n",
    "            break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Len of weight :  0\n",
      "[]\n",
      "Env Name :  Pendulum-v0\n",
      "States 3, Actions 1 in -2.0 ~ 2.0\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2096/2141174623.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2096/2605315062.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[1;31m# Sampling from the memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mmini_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mstates\u001b[0m      \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf240\\lib\\random.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, population, k)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Sample larger than population or is negative\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[0msetsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m21\u001b[0m        \u001b[1;31m# size of a small set minus size of an empty list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\user\\.conda\\envs\\tf240\\lib\\site-packages\\ipykernel\\eventloops.py:256: RuntimeWarning: coroutine 'Kernel.do_one_iteration' was never awaited\n",
      "  self.func()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env.close()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}