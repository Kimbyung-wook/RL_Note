{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('tf240': conda)"
  },
  "interpreter": {
   "hash": "fbba320975a9114d2433fba427f26c389728c846a7c4900c481dce2a1a9f6231"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Refer from\r\n",
    "#  https://pasus.tistory.com/138\r\n",
    "#  https://horomary.hatenablog.com/entry/2020/06/26/003806\r\n",
    "#  https://keras.io/examples/rl/ddpg_pendulum/\r\n",
    "#\r\n",
    "import gym\r\n",
    "import sys\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, concatenate, Lambda\r\n",
    "from collections import deque\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class Actor(tf.keras.Model):\r\n",
    "    def __init__(self, state_size, action_size, action_min, action_max):\r\n",
    "        super(Actor, self).__init__()\r\n",
    "        self.action_min = action_min\r\n",
    "        self.action_max = action_max\r\n",
    "\r\n",
    "        self.fc1 = Dense(64, activation='relu')\r\n",
    "        self.fc2 = Dense(64, activation='relu')\r\n",
    "        # self.fc3 = Dense(16, activation='relu')\r\n",
    "        self.out= Dense(action_size, activation='tanh',kernel_initializer = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)) # -1 ~ +1\r\n",
    "\r\n",
    "    def call(self, x):\r\n",
    "        x       = self.fc1(x)\r\n",
    "        x       = self.fc2(x)\r\n",
    "        # x       = self.fc3(x)\r\n",
    "        action  = self.out(x)\r\n",
    "        # return self.projected_to_action_space(action)\r\n",
    "        a = Lambda(lambda x: x*self.action_max)(action)\r\n",
    "        return a\r\n",
    "\r\n",
    "class Critic(tf.keras.Model):\r\n",
    "    def __init__(self, state_size, action_size):\r\n",
    "        super(Critic, self).__init__()\r\n",
    "        self.s1 = Dense(16, activation='relu')\r\n",
    "        self.s2 = Dense(32, activation='relu')\r\n",
    "        self.a1 = Dense(32, activation='relu')\r\n",
    "        self.a2 = Dense(32, activation='relu')\r\n",
    "        self.fc1= Dense(64, activation='relu')\r\n",
    "        self.fc2= Dense(64, activation='relu')\r\n",
    "        self.out= Dense(1,  activation='linear')\r\n",
    "\r\n",
    "    def call(self,state,action):\r\n",
    "        # state  = state_action[0]\r\n",
    "        # action = state_action[1]\r\n",
    "        s = self.s1(state)\r\n",
    "        s = self.s2(s)\r\n",
    "        a = self.a1(action)\r\n",
    "        a = self.a2(a)\r\n",
    "        c = concatenate([s,a],axis=-1)\r\n",
    "        x = self.fc1(c)\r\n",
    "        x = self.fc2(x)\r\n",
    "        q = self.out(x)\r\n",
    "        return q"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class TD3Agent:\r\n",
    "    def __init__(self, state_size, action_size, action_min, action_max):\r\n",
    "        self.state_size = state_size\r\n",
    "        self.action_size= action_size\r\n",
    "        self.action_min = action_min\r\n",
    "        self.action_max = action_max\r\n",
    "\r\n",
    "        # Hyper params for learning\r\n",
    "        self.discount_factor = 0.99\r\n",
    "        self.actor_learning_rate  = 0.001\r\n",
    "        self.critic_learning_rate = 0.002\r\n",
    "        self.tau = 0.005\r\n",
    "\r\n",
    "        # Experience Replay\r\n",
    "        self.batch_size = 64\r\n",
    "        self.train_start = 2000\r\n",
    "        self.memory = deque(maxlen=50000)\r\n",
    "\r\n",
    "        # self.critic         = get_critic(self.state_size, self.action_size)\r\n",
    "        # self.target_critic  = get_critic(self.state_size, self.action_size)\r\n",
    "        # self.actor          = get_actor(self.state_size, self.action_size, self.action_max)\r\n",
    "        # self.target_actor   = get_actor(self.state_size, self.action_size, self.action_max)\r\n",
    "        self.critic1_optimizer   = tf.keras.optimizers.Adam(lr=self.critic_learning_rate)\r\n",
    "        self.critic2_optimizer   = tf.keras.optimizers.Adam(lr=self.critic_learning_rate)\r\n",
    "        self.actor_optimizer    = tf.keras.optimizers.Adam(lr=self.actor_learning_rate)\r\n",
    "\r\n",
    "        self.critic1        = Critic(self.state_size, self.action_size)\r\n",
    "        self.critic2        = Critic(self.state_size, self.action_size)\r\n",
    "        self.target_critic1 = Critic(self.state_size, self.action_size)\r\n",
    "        self.target_critic2 = Critic(self.state_size, self.action_size)\r\n",
    "        self.actor          = Actor(self.state_size, self.action_size, self.action_min, self.action_max)\r\n",
    "        self.target_actor   = Actor(self.state_size, self.action_size, self.action_min, self.action_max)\r\n",
    "\r\n",
    "        self.actor.build(input_shape=(None, self.state_size))\r\n",
    "        self.target_actor.build(input_shape=(None, self.state_size))\r\n",
    "        state_in = Input((self.state_size,))\r\n",
    "        action_in = Input((self.action_size,))\r\n",
    "        self.actor(state_in)\r\n",
    "        self.target_actor(state_in)\r\n",
    "        self.critic1(state_in, action_in)\r\n",
    "        self.critic2(state_in, action_in)\r\n",
    "        self.target_critic1(state_in, action_in)\r\n",
    "        self.target_critic2(state_in, action_in)\r\n",
    "\r\n",
    "        self.actor.summary()\r\n",
    "        self.critic1.summary()\r\n",
    "        self.critic2.summary()\r\n",
    "        \r\n",
    "        self.target_actor.set_weights(self.actor.get_weights())\r\n",
    "        self.target_critic1.set_weights(self.critic1.get_weights())\r\n",
    "        self.target_critic2.set_weights(self.critic2.get_weights())\r\n",
    "\r\n",
    "        self.update_freq = 2\r\n",
    "        self.train_idx = 0\r\n",
    "        self.show_media_info = False\r\n",
    "\r\n",
    "    def remember(self, state, action, reward, next_state, done):\r\n",
    "        self.memory.append((state, action, reward, next_state, done))\r\n",
    "\r\n",
    "    def update_target_model(self):\r\n",
    "        tau = self.tau\r\n",
    "        for (net, target_net) in zip(   self.actor.trainable_variables,\r\n",
    "                                        self.target_actor.trainable_variables):\r\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\r\n",
    "        for (net, target_net) in zip(   self.critic1.trainable_variables,\r\n",
    "                                        self.target_critic1.trainable_variables):\r\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\r\n",
    "        for (net, target_net) in zip(   self.critic2.trainable_variables,\r\n",
    "                                        self.target_critic2.trainable_variables):\r\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\r\n",
    "\r\n",
    "    def get_action(self,state):\r\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\r\n",
    "        action = self.actor(state)\r\n",
    "        noise = np.random.random(self.action_size)\r\n",
    "        # Exploration and Exploitation\r\n",
    "        return np.clip(action.numpy()[0]+noise,self.action_min,self.action_max)\r\n",
    "\r\n",
    "    def train_model(self):\r\n",
    "        # Train from Experience Replay\r\n",
    "        # Training Condition - Memory Size\r\n",
    "        if len(self.memory) < self.train_start:\r\n",
    "            return\r\n",
    "        # Sampling from the memory\r\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\r\n",
    "        \r\n",
    "        states      = tf.convert_to_tensor(np.array([sample[0] for sample in mini_batch]))\r\n",
    "        actions     = tf.convert_to_tensor(np.array([sample[1] for sample in mini_batch]))\r\n",
    "        rewards     = tf.convert_to_tensor(np.array([sample[2] for sample in mini_batch]),dtype=tf.float32)\r\n",
    "        rewards     = tf.expand_dims(rewards, axis = 1)\r\n",
    "        next_states = tf.convert_to_tensor(np.array([sample[3] for sample in mini_batch]))\r\n",
    "        dones       = tf.convert_to_tensor(np.array([sample[4] for sample in mini_batch]),dtype=tf.float32)\r\n",
    "        dones       = tf.expand_dims(dones, axis = 1)\r\n",
    "        \r\n",
    "        if self.show_media_info == False:\r\n",
    "            self.show_media_info = True\r\n",
    "            print('Start to train, check batch shapes')\r\n",
    "            print('**** shape of states', np.shape(states),type(states))\r\n",
    "            print('**** shape of actions', np.shape(actions),type(actions))\r\n",
    "            print('**** shape of rewards', np.shape(rewards),type(rewards))\r\n",
    "            print('**** shape of next_states', np.shape(next_states),type(next_states))\r\n",
    "            print('**** shape of dones', np.shape(dones),type(dones))\r\n",
    "\r\n",
    "        target_actions = self.target_actor(next_states,training=True)\r\n",
    "        target_q1 = self.target_critic1(next_states,target_actions,training=True)\r\n",
    "        target_q2 = self.target_critic2(next_states,target_actions,training=True)\r\n",
    "        target_q = tf.minimum(target_q1,target_q2)\r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            target_value = rewards + (1.0 - dones) * self.discount_factor * target_q\r\n",
    "            q = self.critic1(states, actions, training=True)\r\n",
    "            critic1_loss = tf.math.reduce_mean(tf.math.square(target_value - q))\r\n",
    "        critic1_params = self.critic1.trainable_variables\r\n",
    "        critic1_grads = tape.gradient(critic1_loss, critic1_params)\r\n",
    "        self.critic1_optimizer.apply_gradients(zip(critic1_grads, critic1_params))\r\n",
    "\r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            target_value = rewards + (1.0 - dones) * self.discount_factor * target_q\r\n",
    "            q = self.critic2(states, actions, training=True)\r\n",
    "            critic2_loss = tf.math.reduce_mean(tf.math.square(target_value - q))\r\n",
    "        critic2_params = self.critic2.trainable_variables\r\n",
    "        critic2_grads = tape.gradient(critic2_loss, critic2_params)\r\n",
    "        self.critic2_optimizer.apply_gradients(zip(critic2_grads, critic2_params))\r\n",
    "\r\n",
    "        self.train_idx = self.train_idx + 1\r\n",
    "        if self.train_idx % self.update_freq == 0:\r\n",
    "          with tf.GradientTape() as tape:\r\n",
    "              new_actions = self.actor(states,training=True)\r\n",
    "              new_q = self.critic1(states, new_actions,training=True)\r\n",
    "              actor_loss = -tf.reduce_mean(new_q)\r\n",
    "          actor_params = self.actor.trainable_variables\r\n",
    "          actor_grads = tape.gradient(actor_loss, actor_params)\r\n",
    "          self.actor_optimizer.apply_gradients(zip(actor_grads, actor_params))\r\n",
    "\r\n",
    "          self.update_target_model()\r\n",
    "        return\r\n",
    "\r\n",
    "    def save_model(self):\r\n",
    "        self.actor.save_weights(\"./save_model/pendulum_td3_TF_actor\", save_format=\"tf\")\r\n",
    "        self.critic1.save_weights(\"./save_model/pendulum_td3_TF_critic1\", save_format=\"tf\")\r\n",
    "        self.critic2.save_weights(\"./save_model/pendulum_td3_TF_critic2\", save_format=\"tf\")\r\n",
    "        return\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# %matplotlib tk\r\n",
    "\r\n",
    "ENV_NAME = 'Pendulum-v0'\r\n",
    "EPISODES = 300\r\n",
    "END_SCORE = -200\r\n",
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "    env = gym.make(ENV_NAME)\r\n",
    "    state_size  = env.observation_space.shape[0]\r\n",
    "    action_size = env.action_space.shape[0]\r\n",
    "    action_min  = env.action_space.low[0]\r\n",
    "    action_max  = env.action_space.high[0]\r\n",
    "\r\n",
    "    agent = TD3Agent(state_size, action_size, action_min, action_max)\r\n",
    "    print('Env Name : ',ENV_NAME)\r\n",
    "    print('States {0}, Actions {1}'.format(state_size, action_size))\r\n",
    "    print('Action space {0:.2f} ~ {1:.2f}'.format(action_min, action_max))\r\n",
    "    scores, episodes = [], []\r\n",
    "    score_avg = 0\r\n",
    "\r\n",
    "    end = False\r\n",
    "    show_media_info = True\r\n",
    "    \r\n",
    "    fig = plt.figure(1)\r\n",
    "    fig.clf()\r\n",
    "    \r\n",
    "    for e in range(EPISODES):\r\n",
    "        done = False\r\n",
    "        score = 0\r\n",
    "        state = env.reset()\r\n",
    "        while not done:\r\n",
    "            # env.render()\r\n",
    "\r\n",
    "            # Interact with env.\r\n",
    "            action = agent.get_action(state)\r\n",
    "            next_state, reward, done, info = env.step(action)\r\n",
    "            agent.remember(state, action, reward, next_state, done)\r\n",
    "            agent.train_model()\r\n",
    "            state = next_state\r\n",
    "\r\n",
    "            # \r\n",
    "            score += reward\r\n",
    "            if show_media_info:\r\n",
    "                print(\"State Shape : \", np.shape(state))\r\n",
    "                print(\"Action Shape : \", np.shape(action))\r\n",
    "                print(\"Reward Shape : \", np.shape(reward))\r\n",
    "                print(\"done Shape : \", np.shape(done))\r\n",
    "                show_media_info = False\r\n",
    "            if done:\r\n",
    "                score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\r\n",
    "                print(\"episode: {0:3d} | score avg: {1:3.2f} | mem size {2:6d} |\"\r\n",
    "                    .format(e, score_avg, len(agent.memory)))\r\n",
    "\r\n",
    "                episodes.append(e)\r\n",
    "                scores.append(score_avg)\r\n",
    "\r\n",
    "                plt.plot(episodes, scores, 'b')\r\n",
    "                plt.xlabel('episode')\r\n",
    "                plt.ylabel('average score')\r\n",
    "                plt.title('pendulum DDPG')\r\n",
    "                plt.grid()\r\n",
    "                plt.savefig(\"./save_model/pendulum_td3_TF.png\")\r\n",
    "\r\n",
    "                # 이동 평균이 0 이상일 때 종료\r\n",
    "                if score_avg > END_SCORE:\r\n",
    "                    agent.save_model()\r\n",
    "                    end = True\r\n",
    "                    break\r\n",
    "        if end == True:\r\n",
    "            env.close()\r\n",
    "            np.save('./save_model/data/pendulum_td3_TF_epi',  episodes)\r\n",
    "            np.save('./save_model/data/pendulum_td3_TF_score',scores)\r\n",
    "            print(\"End\")\r\n",
    "            break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"actor\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             multiple                  256       \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             multiple                  65        \n",
      "=================================================================\n",
      "Total params: 4,481\n",
      "Trainable params: 4,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"critic\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  64        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  544       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  64        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  1056      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  65        \n",
      "=================================================================\n",
      "Total params: 10,113\n",
      "Trainable params: 10,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"critic_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              multiple                  64        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              multiple                  544       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              multiple                  64        \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             multiple                  1056      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             multiple                  65        \n",
      "=================================================================\n",
      "Total params: 10,113\n",
      "Trainable params: 10,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Env Name :  Pendulum-v0\n",
      "States 3, Actions 1\n",
      "Action space -2.00 ~ 2.00\n",
      "State Shape :  (3,)\n",
      "Action Shape :  (1,)\n",
      "Reward Shape :  ()\n",
      "done Shape :  ()\n",
      "episode:   0 | score avg: -1458.12 | mem size    200 |\n",
      "episode:   1 | score avg: -1430.67 | mem size    400 |\n",
      "episode:   2 | score avg: -1431.06 | mem size    600 |\n",
      "episode:   3 | score avg: -1455.63 | mem size    800 |\n",
      "episode:   4 | score avg: -1420.55 | mem size   1000 |\n",
      "episode:   5 | score avg: -1427.55 | mem size   1200 |\n",
      "episode:   6 | score avg: -1429.57 | mem size   1400 |\n",
      "episode:   7 | score avg: -1405.21 | mem size   1600 |\n",
      "episode:   8 | score avg: -1379.55 | mem size   1800 |\n",
      "Start to train, check batch shapes\n",
      "**** shape of states (64, 3) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of actions (64, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of rewards (64, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of next_states (64, 3) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of dones (64, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "episode:   9 | score avg: -1399.57 | mem size   2000 |\n",
      "episode:  10 | score avg: -1428.74 | mem size   2200 |\n",
      "episode:  11 | score avg: -1437.54 | mem size   2400 |\n",
      "episode:  12 | score avg: -1406.13 | mem size   2600 |\n",
      "episode:  13 | score avg: -1420.84 | mem size   2800 |\n",
      "episode:  14 | score avg: -1403.89 | mem size   3000 |\n",
      "episode:  15 | score avg: -1396.34 | mem size   3200 |\n",
      "episode:  16 | score avg: -1401.28 | mem size   3400 |\n",
      "episode:  17 | score avg: -1413.11 | mem size   3600 |\n",
      "episode:  18 | score avg: -1397.47 | mem size   3800 |\n",
      "episode:  19 | score avg: -1376.29 | mem size   4000 |\n",
      "episode:  20 | score avg: -1367.10 | mem size   4200 |\n",
      "episode:  21 | score avg: -1334.76 | mem size   4400 |\n",
      "episode:  22 | score avg: -1319.30 | mem size   4600 |\n",
      "episode:  23 | score avg: -1313.53 | mem size   4800 |\n",
      "episode:  24 | score avg: -1280.57 | mem size   5000 |\n",
      "episode:  25 | score avg: -1292.84 | mem size   5200 |\n",
      "episode:  26 | score avg: -1319.41 | mem size   5400 |\n",
      "episode:  27 | score avg: -1307.02 | mem size   5600 |\n",
      "episode:  28 | score avg: -1294.17 | mem size   5800 |\n",
      "episode:  29 | score avg: -1238.29 | mem size   6000 |\n",
      "episode:  30 | score avg: -1250.95 | mem size   6200 |\n",
      "episode:  31 | score avg: -1249.27 | mem size   6400 |\n",
      "episode:  32 | score avg: -1198.23 | mem size   6600 |\n",
      "episode:  33 | score avg: -1201.08 | mem size   6800 |\n",
      "episode:  34 | score avg: -1190.88 | mem size   7000 |\n",
      "episode:  35 | score avg: -1209.19 | mem size   7200 |\n",
      "episode:  36 | score avg: -1190.68 | mem size   7400 |\n",
      "episode:  37 | score avg: -1201.80 | mem size   7600 |\n",
      "episode:  38 | score avg: -1154.76 | mem size   7800 |\n",
      "episode:  39 | score avg: -1167.22 | mem size   8000 |\n",
      "episode:  40 | score avg: -1168.87 | mem size   8200 |\n",
      "episode:  41 | score avg: -1140.20 | mem size   8400 |\n",
      "episode:  42 | score avg: -1122.90 | mem size   8600 |\n",
      "episode:  43 | score avg: -1058.96 | mem size   8800 |\n",
      "episode:  44 | score avg: -1087.10 | mem size   9000 |\n",
      "episode:  45 | score avg: -1002.94 | mem size   9200 |\n",
      "episode:  46 | score avg: -926.96 | mem size   9400 |\n",
      "episode:  47 | score avg: -834.90 | mem size   9600 |\n",
      "episode:  48 | score avg: -811.94 | mem size   9800 |\n",
      "episode:  49 | score avg: -742.89 | mem size  10000 |\n",
      "episode:  50 | score avg: -669.06 | mem size  10200 |\n",
      "episode:  51 | score avg: -650.48 | mem size  10400 |\n",
      "episode:  52 | score avg: -734.55 | mem size  10600 |\n",
      "episode:  53 | score avg: -661.35 | mem size  10800 |\n",
      "episode:  54 | score avg: -608.11 | mem size  11000 |\n",
      "episode:  55 | score avg: -559.41 | mem size  11200 |\n",
      "episode:  56 | score avg: -600.21 | mem size  11400 |\n",
      "episode:  57 | score avg: -552.56 | mem size  11600 |\n",
      "episode:  58 | score avg: -509.91 | mem size  11800 |\n",
      "episode:  59 | score avg: -570.16 | mem size  12000 |\n",
      "episode:  60 | score avg: -525.89 | mem size  12200 |\n",
      "episode:  61 | score avg: -511.41 | mem size  12400 |\n",
      "episode:  62 | score avg: -460.51 | mem size  12600 |\n",
      "episode:  63 | score avg: -486.94 | mem size  12800 |\n",
      "episode:  64 | score avg: -438.55 | mem size  13000 |\n",
      "episode:  65 | score avg: -394.93 | mem size  13200 |\n",
      "episode:  66 | score avg: -355.57 | mem size  13400 |\n",
      "episode:  67 | score avg: -458.94 | mem size  13600 |\n",
      "episode:  68 | score avg: -425.25 | mem size  13800 |\n",
      "episode:  69 | score avg: -418.27 | mem size  14000 |\n",
      "episode:  70 | score avg: -400.60 | mem size  14200 |\n",
      "episode:  71 | score avg: -384.60 | mem size  14400 |\n",
      "episode:  72 | score avg: -393.95 | mem size  14600 |\n",
      "episode:  73 | score avg: -402.15 | mem size  14800 |\n",
      "episode:  74 | score avg: -423.11 | mem size  15000 |\n",
      "episode:  75 | score avg: -392.92 | mem size  15200 |\n",
      "episode:  76 | score avg: -378.86 | mem size  15400 |\n",
      "episode:  77 | score avg: -440.26 | mem size  15600 |\n",
      "episode:  78 | score avg: -493.43 | mem size  15800 |\n",
      "episode:  79 | score avg: -468.39 | mem size  16000 |\n",
      "episode:  80 | score avg: -533.33 | mem size  16200 |\n",
      "episode:  81 | score avg: -503.85 | mem size  16400 |\n",
      "episode:  82 | score avg: -596.32 | mem size  16600 |\n",
      "episode:  83 | score avg: -548.26 | mem size  16800 |\n",
      "episode:  84 | score avg: -528.72 | mem size  17000 |\n",
      "episode:  85 | score avg: -499.44 | mem size  17200 |\n",
      "episode:  86 | score avg: -461.94 | mem size  17400 |\n",
      "episode:  87 | score avg: -452.59 | mem size  17600 |\n",
      "episode:  88 | score avg: -460.95 | mem size  17800 |\n",
      "episode:  89 | score avg: -454.15 | mem size  18000 |\n",
      "episode:  90 | score avg: -470.29 | mem size  18200 |\n",
      "episode:  91 | score avg: -495.21 | mem size  18400 |\n",
      "episode:  92 | score avg: -482.89 | mem size  18600 |\n",
      "episode:  93 | score avg: -534.89 | mem size  18800 |\n",
      "episode:  94 | score avg: -566.21 | mem size  19000 |\n",
      "episode:  95 | score avg: -522.67 | mem size  19200 |\n",
      "episode:  96 | score avg: -494.34 | mem size  19400 |\n",
      "episode:  97 | score avg: -457.01 | mem size  19600 |\n",
      "episode:  98 | score avg: -423.30 | mem size  19800 |\n",
      "episode:  99 | score avg: -392.93 | mem size  20000 |\n",
      "episode: 100 | score avg: -365.75 | mem size  20200 |\n",
      "episode: 101 | score avg: -341.95 | mem size  20400 |\n",
      "episode: 102 | score avg: -399.36 | mem size  20600 |\n",
      "episode: 103 | score avg: -455.31 | mem size  20800 |\n",
      "episode: 104 | score avg: -434.67 | mem size  21000 |\n",
      "episode: 105 | score avg: -451.20 | mem size  21200 |\n",
      "episode: 106 | score avg: -490.78 | mem size  21400 |\n",
      "episode: 107 | score avg: -454.23 | mem size  21600 |\n",
      "episode: 108 | score avg: -420.84 | mem size  21800 |\n",
      "episode: 109 | score avg: -416.31 | mem size  22000 |\n",
      "episode: 110 | score avg: -436.41 | mem size  22200 |\n",
      "episode: 111 | score avg: -393.43 | mem size  22400 |\n",
      "episode: 112 | score avg: -426.27 | mem size  22600 |\n",
      "episode: 113 | score avg: -407.23 | mem size  22800 |\n",
      "episode: 114 | score avg: -391.44 | mem size  23000 |\n",
      "episode: 115 | score avg: -410.67 | mem size  23200 |\n",
      "episode: 116 | score avg: -381.93 | mem size  23400 |\n",
      "episode: 117 | score avg: -343.95 | mem size  23600 |\n",
      "episode: 118 | score avg: -333.00 | mem size  23800 |\n",
      "episode: 119 | score avg: -311.70 | mem size  24000 |\n",
      "episode: 120 | score avg: -317.18 | mem size  24200 |\n",
      "episode: 121 | score avg: -309.28 | mem size  24400 |\n",
      "episode: 122 | score avg: -338.01 | mem size  24600 |\n",
      "episode: 123 | score avg: -304.72 | mem size  24800 |\n",
      "episode: 124 | score avg: -334.98 | mem size  25000 |\n",
      "episode: 125 | score avg: -325.18 | mem size  25200 |\n",
      "episode: 126 | score avg: -292.80 | mem size  25400 |\n",
      "episode: 127 | score avg: -323.83 | mem size  25600 |\n",
      "episode: 128 | score avg: -291.60 | mem size  25800 |\n",
      "episode: 129 | score avg: -285.84 | mem size  26000 |\n",
      "episode: 130 | score avg: -281.25 | mem size  26200 |\n",
      "episode: 131 | score avg: -276.52 | mem size  26400 |\n",
      "episode: 132 | score avg: -260.83 | mem size  26600 |\n",
      "episode: 133 | score avg: -333.94 | mem size  26800 |\n",
      "episode: 134 | score avg: -385.30 | mem size  27000 |\n",
      "episode: 135 | score avg: -370.96 | mem size  27200 |\n",
      "episode: 136 | score avg: -369.48 | mem size  27400 |\n",
      "episode: 137 | score avg: -344.80 | mem size  27600 |\n",
      "episode: 138 | score avg: -347.05 | mem size  27800 |\n",
      "episode: 139 | score avg: -348.58 | mem size  28000 |\n",
      "episode: 140 | score avg: -464.52 | mem size  28200 |\n",
      "episode: 141 | score avg: -568.26 | mem size  28400 |\n",
      "episode: 142 | score avg: -534.89 | mem size  28600 |\n",
      "episode: 143 | score avg: -493.46 | mem size  28800 |\n",
      "episode: 144 | score avg: -456.91 | mem size  29000 |\n",
      "episode: 145 | score avg: -424.35 | mem size  29200 |\n",
      "episode: 146 | score avg: -418.35 | mem size  29400 |\n",
      "episode: 147 | score avg: -400.93 | mem size  29600 |\n",
      "episode: 148 | score avg: -372.87 | mem size  29800 |\n",
      "episode: 149 | score avg: -335.73 | mem size  30000 |\n",
      "episode: 150 | score avg: -452.06 | mem size  30200 |\n",
      "episode: 151 | score avg: -407.14 | mem size  30400 |\n",
      "episode: 152 | score avg: -366.59 | mem size  30600 |\n",
      "episode: 153 | score avg: -367.42 | mem size  30800 |\n",
      "episode: 154 | score avg: -367.50 | mem size  31000 |\n",
      "episode: 155 | score avg: -342.87 | mem size  31200 |\n",
      "episode: 156 | score avg: -321.51 | mem size  31400 |\n",
      "episode: 157 | score avg: -348.82 | mem size  31600 |\n",
      "episode: 158 | score avg: -327.26 | mem size  31800 |\n",
      "episode: 159 | score avg: -319.13 | mem size  32000 |\n",
      "episode: 160 | score avg: -337.11 | mem size  32200 |\n",
      "episode: 161 | score avg: -339.25 | mem size  32400 |\n",
      "episode: 162 | score avg: -344.47 | mem size  32600 |\n",
      "episode: 163 | score avg: -335.70 | mem size  32800 |\n",
      "episode: 164 | score avg: -327.57 | mem size  33000 |\n",
      "episode: 165 | score avg: -356.87 | mem size  33200 |\n",
      "episode: 166 | score avg: -345.46 | mem size  33400 |\n",
      "episode: 167 | score avg: -385.19 | mem size  33600 |\n",
      "episode: 168 | score avg: -347.14 | mem size  33800 |\n",
      "episode: 169 | score avg: -462.25 | mem size  34000 |\n",
      "episode: 170 | score avg: -428.61 | mem size  34200 |\n",
      "episode: 171 | score avg: -409.07 | mem size  34400 |\n",
      "episode: 172 | score avg: -419.87 | mem size  34600 |\n",
      "episode: 173 | score avg: -401.99 | mem size  34800 |\n",
      "episode: 174 | score avg: -397.41 | mem size  35000 |\n",
      "episode: 175 | score avg: -370.57 | mem size  35200 |\n",
      "episode: 176 | score avg: -358.40 | mem size  35400 |\n",
      "episode: 177 | score avg: -347.68 | mem size  35600 |\n",
      "episode: 178 | score avg: -348.54 | mem size  35800 |\n",
      "episode: 179 | score avg: -336.75 | mem size  36000 |\n",
      "episode: 180 | score avg: -327.55 | mem size  36200 |\n",
      "episode: 181 | score avg: -331.03 | mem size  36400 |\n",
      "episode: 182 | score avg: -322.55 | mem size  36600 |\n",
      "episode: 183 | score avg: -302.96 | mem size  36800 |\n",
      "episode: 184 | score avg: -285.66 | mem size  37000 |\n",
      "episode: 185 | score avg: -283.25 | mem size  37200 |\n",
      "episode: 186 | score avg: -305.58 | mem size  37400 |\n",
      "episode: 187 | score avg: -298.91 | mem size  37600 |\n",
      "episode: 188 | score avg: -294.81 | mem size  37800 |\n",
      "episode: 189 | score avg: -278.26 | mem size  38000 |\n",
      "episode: 190 | score avg: -298.99 | mem size  38200 |\n",
      "episode: 191 | score avg: -307.11 | mem size  38400 |\n",
      "episode: 192 | score avg: -304.53 | mem size  38600 |\n",
      "episode: 193 | score avg: -312.07 | mem size  38800 |\n",
      "episode: 194 | score avg: -304.18 | mem size  39000 |\n",
      "episode: 195 | score avg: -321.60 | mem size  39200 |\n",
      "episode: 196 | score avg: -315.96 | mem size  39400 |\n",
      "episode: 197 | score avg: -296.95 | mem size  39600 |\n",
      "episode: 198 | score avg: -279.01 | mem size  39800 |\n",
      "episode: 199 | score avg: -263.54 | mem size  40000 |\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "env.close()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}