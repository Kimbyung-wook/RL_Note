{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('tf240': conda)"
  },
  "interpreter": {
   "hash": "fbba320975a9114d2433fba427f26c389728c846a7c4900c481dce2a1a9f6231"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Refer from\r\n",
    "#  https://pasus.tistory.com/138\r\n",
    "#  https://horomary.hatenablog.com/entry/2020/06/26/003806\r\n",
    "#  https://keras.io/examples/rl/ddpg_pendulum/\r\n",
    "#\r\n",
    "import gym\r\n",
    "import sys\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, concatenate, Lambda\r\n",
    "from collections import deque\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class Actor(tf.keras.Model):\r\n",
    "    def __init__(self, state_size, action_size, action_min, action_max):\r\n",
    "        super(Actor, self).__init__()\r\n",
    "        self.action_min = action_min\r\n",
    "        self.action_max = action_max\r\n",
    "\r\n",
    "        self.fc1 = Dense(64, activation='relu')\r\n",
    "        self.fc2 = Dense(64, activation='relu')\r\n",
    "        # self.fc3 = Dense(16, activation='relu')\r\n",
    "        self.out= Dense(action_size, activation='tanh',kernel_initializer = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)) # -1 ~ +1\r\n",
    "\r\n",
    "    def call(self, x):\r\n",
    "        x       = self.fc1(x)\r\n",
    "        x       = self.fc2(x)\r\n",
    "        # x       = self.fc3(x)\r\n",
    "        action  = self.out(x)\r\n",
    "        # return self.projected_to_action_space(action)\r\n",
    "        a = Lambda(lambda x: x*self.action_max)(action)\r\n",
    "        return a\r\n",
    "\r\n",
    "class Critic(tf.keras.Model):\r\n",
    "    def __init__(self, state_size, action_size):\r\n",
    "        super(Critic, self).__init__()\r\n",
    "        self.s1 = Dense(16, activation='relu')\r\n",
    "        self.s2 = Dense(32, activation='relu')\r\n",
    "        self.a1 = Dense(32, activation='relu')\r\n",
    "        self.a2 = Dense(32, activation='relu')\r\n",
    "        self.fc1= Dense(64, activation='relu')\r\n",
    "        self.fc2= Dense(64, activation='relu')\r\n",
    "        self.out= Dense(1,  activation='linear')\r\n",
    "\r\n",
    "    def call(self,state,action):\r\n",
    "        # state  = state_action[0]\r\n",
    "        # action = state_action[1]\r\n",
    "        s = self.s1(state)\r\n",
    "        s = self.s2(s)\r\n",
    "        a = self.a1(action)\r\n",
    "        a = self.a2(a)\r\n",
    "        c = concatenate([s,a],axis=-1)\r\n",
    "        x = self.fc1(c)\r\n",
    "        x = self.fc2(x)\r\n",
    "        q = self.out(x)\r\n",
    "        return q"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class TD3Agent:\r\n",
    "    def __init__(self, state_size, action_size, action_min, action_max):\r\n",
    "        self.state_size = state_size\r\n",
    "        self.action_size= action_size\r\n",
    "        self.action_min = action_min\r\n",
    "        self.action_max = action_max\r\n",
    "\r\n",
    "        # Hyper params for learning\r\n",
    "        self.discount_factor = 0.99\r\n",
    "        self.actor_learning_rate  = 0.001\r\n",
    "        self.critic_learning_rate = 0.002\r\n",
    "        self.tau = 0.005\r\n",
    "\r\n",
    "        # Experience Replay\r\n",
    "        self.batch_size = 64\r\n",
    "        self.train_start = 2000\r\n",
    "        self.memory = deque(maxlen=50000)\r\n",
    "\r\n",
    "        # self.critic         = get_critic(self.state_size, self.action_size)\r\n",
    "        # self.target_critic  = get_critic(self.state_size, self.action_size)\r\n",
    "        # self.actor          = get_actor(self.state_size, self.action_size, self.action_max)\r\n",
    "        # self.target_actor   = get_actor(self.state_size, self.action_size, self.action_max)\r\n",
    "        self.critic1_optimizer   = tf.keras.optimizers.Adam(lr=self.critic_learning_rate)\r\n",
    "        self.critic2_optimizer   = tf.keras.optimizers.Adam(lr=self.critic_learning_rate)\r\n",
    "        self.actor_optimizer    = tf.keras.optimizers.Adam(lr=self.actor_learning_rate)\r\n",
    "\r\n",
    "        self.critic1        = Critic(self.state_size, self.action_size)\r\n",
    "        self.critic2        = Critic(self.state_size, self.action_size)\r\n",
    "        self.target_critic1 = Critic(self.state_size, self.action_size)\r\n",
    "        self.target_critic2 = Critic(self.state_size, self.action_size)\r\n",
    "        self.actor          = Actor(self.state_size, self.action_size, self.action_min, self.action_max)\r\n",
    "        self.target_actor   = Actor(self.state_size, self.action_size, self.action_min, self.action_max)\r\n",
    "\r\n",
    "        self.actor.build(input_shape=(None, self.state_size))\r\n",
    "        self.target_actor.build(input_shape=(None, self.state_size))\r\n",
    "        state_in = Input((self.state_size,))\r\n",
    "        action_in = Input((self.action_size,))\r\n",
    "        self.actor(state_in)\r\n",
    "        self.target_actor(state_in)\r\n",
    "        self.critic1(state_in, action_in)\r\n",
    "        self.critic2(state_in, action_in)\r\n",
    "        self.target_critic1(state_in, action_in)\r\n",
    "        self.target_critic2(state_in, action_in)\r\n",
    "\r\n",
    "        self.actor.summary()\r\n",
    "        self.critic1.summary()\r\n",
    "        self.critic2.summary()\r\n",
    "        \r\n",
    "        self.target_actor.set_weights(self.actor.get_weights())\r\n",
    "        self.target_critic1.set_weights(self.critic1.get_weights())\r\n",
    "        self.target_critic2.set_weights(self.critic2.get_weights())\r\n",
    "\r\n",
    "        self.update_freq = 2\r\n",
    "        self.train_idx = 0\r\n",
    "        self.show_media_info = False\r\n",
    "\r\n",
    "    def remember(self, state, action, reward, next_state, done):\r\n",
    "        self.memory.append((state, action, reward, next_state, done))\r\n",
    "\r\n",
    "    def update_target_model(self):\r\n",
    "        tau = self.tau\r\n",
    "        for (net, target_net) in zip(   self.actor.trainable_variables,\r\n",
    "                                        self.target_actor.trainable_variables):\r\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\r\n",
    "        for (net, target_net) in zip(   self.critic1.trainable_variables,\r\n",
    "                                        self.target_critic1.trainable_variables):\r\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\r\n",
    "        for (net, target_net) in zip(   self.critic2.trainable_variables,\r\n",
    "                                        self.target_critic2.trainable_variables):\r\n",
    "            target_net.assign(tau * net + (1.0 - tau) * target_net)\r\n",
    "\r\n",
    "    def get_action(self,state):\r\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\r\n",
    "        action = self.actor(state)\r\n",
    "        noise = np.random.random(self.action_size)\r\n",
    "        # Exploration and Exploitation\r\n",
    "        return np.clip(action.numpy()[0]+noise,self.action_min,self.action_max)\r\n",
    "\r\n",
    "    def train_model(self):\r\n",
    "        # Train from Experience Replay\r\n",
    "        # Training Condition - Memory Size\r\n",
    "        if len(self.memory) < self.train_start:\r\n",
    "            return\r\n",
    "        # Sampling from the memory\r\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\r\n",
    "        \r\n",
    "        states      = tf.convert_to_tensor(np.array([sample[0] for sample in mini_batch]))\r\n",
    "        actions     = tf.convert_to_tensor(np.array([sample[1] for sample in mini_batch]))\r\n",
    "        rewards     = tf.convert_to_tensor(np.array([sample[2] for sample in mini_batch]),dtype=tf.float32)\r\n",
    "        rewards     = tf.expand_dims(rewards, axis = 1)\r\n",
    "        next_states = tf.convert_to_tensor(np.array([sample[3] for sample in mini_batch]))\r\n",
    "        dones       = tf.convert_to_tensor(np.array([sample[4] for sample in mini_batch]),dtype=tf.float32)\r\n",
    "        dones       = tf.expand_dims(dones, axis = 1)\r\n",
    "        \r\n",
    "        if self.show_media_info == False:\r\n",
    "            self.show_media_info = True\r\n",
    "            print('Start to train, check batch shapes')\r\n",
    "            print('**** shape of states', np.shape(states),type(states))\r\n",
    "            print('**** shape of actions', np.shape(actions),type(actions))\r\n",
    "            print('**** shape of rewards', np.shape(rewards),type(rewards))\r\n",
    "            print('**** shape of next_states', np.shape(next_states),type(next_states))\r\n",
    "            print('**** shape of dones', np.shape(dones),type(dones))\r\n",
    "\r\n",
    "        target_actions = self.target_actor(next_states,training=True)\r\n",
    "        target_q1 = self.target_critic1(next_states,target_actions,training=True)\r\n",
    "        target_q2 = self.target_critic2(next_states,target_actions,training=True)\r\n",
    "        target_q_min = tf.minimum(target_q1,target_q2) # Clipping Double Q\r\n",
    "        \r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            target_value = rewards + (1.0 - dones) * self.discount_factor * target_q_min\r\n",
    "            q = self.critic1(states, actions, training=True)\r\n",
    "            critic1_loss = tf.math.reduce_mean(tf.math.square(target_value - q))\r\n",
    "        critic1_params = self.critic1.trainable_variables\r\n",
    "        critic1_grads = tape.gradient(critic1_loss, critic1_params)\r\n",
    "        self.critic1_optimizer.apply_gradients(zip(critic1_grads, critic1_params))\r\n",
    "\r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            target_value = rewards + (1.0 - dones) * self.discount_factor * target_q_min\r\n",
    "            q = self.critic2(states, actions, training=True)\r\n",
    "            critic2_loss = tf.math.reduce_mean(tf.math.square(target_value - q))\r\n",
    "        critic2_params = self.critic2.trainable_variables\r\n",
    "        critic2_grads = tape.gradient(critic2_loss, critic2_params)\r\n",
    "        self.critic2_optimizer.apply_gradients(zip(critic2_grads, critic2_params))\r\n",
    "\r\n",
    "        self.train_idx = self.train_idx + 1\r\n",
    "        if self.train_idx % self.update_freq == 0:\r\n",
    "          with tf.GradientTape() as tape:\r\n",
    "              new_actions = self.actor(states,training=True)\r\n",
    "              new_q = self.critic1(states, new_actions,training=True)\r\n",
    "              actor_loss = -tf.reduce_mean(new_q)\r\n",
    "          actor_params = self.actor.trainable_variables\r\n",
    "          actor_grads = tape.gradient(actor_loss, actor_params)\r\n",
    "          self.actor_optimizer.apply_gradients(zip(actor_grads, actor_params))\r\n",
    "\r\n",
    "          self.update_target_model()\r\n",
    "        return\r\n",
    "\r\n",
    "    def save_model(self):\r\n",
    "        self.actor.save_weights(\"./save_model/pendulum_td3_TF_actor\", save_format=\"tf\")\r\n",
    "        self.critic1.save_weights(\"./save_model/pendulum_td3_TF_critic1\", save_format=\"tf\")\r\n",
    "        self.critic2.save_weights(\"./save_model/pendulum_td3_TF_critic2\", save_format=\"tf\")\r\n",
    "        return\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# %matplotlib tk\r\n",
    "\r\n",
    "ENV_NAME = 'Pendulum-v0'\r\n",
    "EPISODES = 300\r\n",
    "END_SCORE = -200\r\n",
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "    env = gym.make(ENV_NAME)\r\n",
    "    state_size  = env.observation_space.shape[0]\r\n",
    "    action_size = env.action_space.shape[0]\r\n",
    "    action_min  = env.action_space.low[0]\r\n",
    "    action_max  = env.action_space.high[0]\r\n",
    "\r\n",
    "    agent = TD3Agent(state_size, action_size, action_min, action_max)\r\n",
    "    print('Env Name : ',ENV_NAME)\r\n",
    "    print('States {0}, Actions {1}'.format(state_size, action_size))\r\n",
    "    print('Action space {0:.2f} ~ {1:.2f}'.format(action_min, action_max))\r\n",
    "    scores, episodes = [], []\r\n",
    "    score_avg = 0\r\n",
    "\r\n",
    "    end = False\r\n",
    "    show_media_info = True\r\n",
    "    \r\n",
    "    fig = plt.figure(1)\r\n",
    "    fig.clf()\r\n",
    "    \r\n",
    "    for e in range(EPISODES):\r\n",
    "        done = False\r\n",
    "        score = 0\r\n",
    "        state = env.reset()\r\n",
    "        while not done:\r\n",
    "            # env.render()\r\n",
    "\r\n",
    "            # Interact with env.\r\n",
    "            action = agent.get_action(state)\r\n",
    "            next_state, reward, done, info = env.step(action)\r\n",
    "            agent.remember(state, action, reward, next_state, done)\r\n",
    "            agent.train_model()\r\n",
    "            state = next_state\r\n",
    "\r\n",
    "            # \r\n",
    "            score += reward\r\n",
    "            if show_media_info:\r\n",
    "                print(\"State Shape : \", np.shape(state))\r\n",
    "                print(\"Action Shape : \", np.shape(action))\r\n",
    "                print(\"Reward Shape : \", np.shape(reward))\r\n",
    "                print(\"done Shape : \", np.shape(done))\r\n",
    "                show_media_info = False\r\n",
    "            if done:\r\n",
    "                score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\r\n",
    "                print(\"episode: {0:3d} | score avg: {1:3.2f} | mem size {2:6d} |\"\r\n",
    "                    .format(e, score_avg, len(agent.memory)))\r\n",
    "\r\n",
    "                episodes.append(e)\r\n",
    "                scores.append(score_avg)\r\n",
    "\r\n",
    "                plt.plot(episodes, scores, 'b')\r\n",
    "                plt.xlabel('episode')\r\n",
    "                plt.ylabel('average score')\r\n",
    "                plt.title('pendulum DDPG')\r\n",
    "                plt.grid()\r\n",
    "                plt.savefig(\"./save_model/pendulum_td3_TF.png\")\r\n",
    "\r\n",
    "                # 이동 평균이 0 이상일 때 종료\r\n",
    "                if score_avg > END_SCORE:\r\n",
    "                    agent.save_model()\r\n",
    "                    end = True\r\n",
    "                    break\r\n",
    "        if end == True:\r\n",
    "            env.close()\r\n",
    "            np.save('./save_model/data/pendulum_td3_TF_epi',  episodes)\r\n",
    "            np.save('./save_model/data/pendulum_td3_TF_score',scores)\r\n",
    "            print(\"End\")\r\n",
    "            break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"actor\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_28 (Dense)             multiple                  256       \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             multiple                  65        \n",
      "=================================================================\n",
      "Total params: 4,481\n",
      "Trainable params: 4,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"critic\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  64        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  544       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  64        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  1056      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              multiple                  65        \n",
      "=================================================================\n",
      "Total params: 10,113\n",
      "Trainable params: 10,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"critic_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              multiple                  64        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              multiple                  544       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              multiple                  64        \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             multiple                  1056      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             multiple                  4160      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             multiple                  65        \n",
      "=================================================================\n",
      "Total params: 10,113\n",
      "Trainable params: 10,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Env Name :  Pendulum-v0\n",
      "States 3, Actions 1\n",
      "Action space -2.00 ~ 2.00\n",
      "State Shape :  (3,)\n",
      "Action Shape :  (1,)\n",
      "Reward Shape :  ()\n",
      "done Shape :  ()\n",
      "episode:   0 | score avg: -1156.64 | mem size    200 |\n",
      "episode:   1 | score avg: -1220.60 | mem size    400 |\n",
      "episode:   2 | score avg: -1257.27 | mem size    600 |\n",
      "episode:   3 | score avg: -1227.31 | mem size    800 |\n",
      "episode:   4 | score avg: -1285.37 | mem size   1000 |\n",
      "episode:   5 | score avg: -1263.56 | mem size   1200 |\n",
      "episode:   6 | score avg: -1255.43 | mem size   1400 |\n",
      "episode:   7 | score avg: -1247.89 | mem size   1600 |\n",
      "episode:   8 | score avg: -1277.10 | mem size   1800 |\n",
      "Start to train, check batch shapes\n",
      "**** shape of states (64, 3) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of actions (64, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of rewards (64, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of next_states (64, 3) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "**** shape of dones (64, 1) <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "episode:   9 | score avg: -1256.04 | mem size   2000 |\n",
      "episode:  10 | score avg: -1268.53 | mem size   2200 |\n",
      "episode:  11 | score avg: -1259.94 | mem size   2400 |\n",
      "episode:  12 | score avg: -1224.55 | mem size   2600 |\n",
      "episode:  13 | score avg: -1231.79 | mem size   2800 |\n",
      "episode:  14 | score avg: -1283.37 | mem size   3000 |\n",
      "episode:  15 | score avg: -1317.66 | mem size   3200 |\n",
      "episode:  16 | score avg: -1373.06 | mem size   3400 |\n",
      "episode:  17 | score avg: -1362.16 | mem size   3600 |\n",
      "episode:  18 | score avg: -1377.17 | mem size   3800 |\n",
      "episode:  19 | score avg: -1393.71 | mem size   4000 |\n",
      "episode:  20 | score avg: -1402.38 | mem size   4200 |\n",
      "episode:  21 | score avg: -1428.64 | mem size   4400 |\n",
      "episode:  22 | score avg: -1437.59 | mem size   4600 |\n",
      "episode:  23 | score avg: -1438.43 | mem size   4800 |\n",
      "episode:  24 | score avg: -1371.21 | mem size   5000 |\n",
      "episode:  25 | score avg: -1322.93 | mem size   5200 |\n",
      "episode:  26 | score avg: -1319.64 | mem size   5400 |\n",
      "episode:  27 | score avg: -1267.35 | mem size   5600 |\n",
      "episode:  28 | score avg: -1249.07 | mem size   5800 |\n",
      "episode:  29 | score avg: -1187.89 | mem size   6000 |\n",
      "episode:  30 | score avg: -1224.15 | mem size   6200 |\n",
      "episode:  31 | score avg: -1218.38 | mem size   6400 |\n",
      "episode:  32 | score avg: -1197.36 | mem size   6600 |\n",
      "episode:  33 | score avg: -1140.39 | mem size   6800 |\n",
      "episode:  34 | score avg: -1142.55 | mem size   7000 |\n",
      "episode:  35 | score avg: -1155.98 | mem size   7200 |\n",
      "episode:  36 | score avg: -1098.05 | mem size   7400 |\n",
      "episode:  37 | score avg: -1050.59 | mem size   7600 |\n",
      "episode:  38 | score avg: -996.01 | mem size   7800 |\n",
      "episode:  39 | score avg: -945.81 | mem size   8000 |\n",
      "episode:  40 | score avg: -902.04 | mem size   8200 |\n",
      "episode:  41 | score avg: -836.66 | mem size   8400 |\n",
      "episode:  42 | score avg: -753.58 | mem size   8600 |\n",
      "episode:  43 | score avg: -702.17 | mem size   8800 |\n",
      "episode:  44 | score avg: -819.59 | mem size   9000 |\n",
      "episode:  45 | score avg: -823.25 | mem size   9200 |\n",
      "episode:  46 | score avg: -765.01 | mem size   9400 |\n",
      "episode:  47 | score avg: -724.92 | mem size   9600 |\n",
      "episode:  48 | score avg: -688.64 | mem size   9800 |\n",
      "episode:  49 | score avg: -643.99 | mem size  10000 |\n",
      "episode:  50 | score avg: -591.85 | mem size  10200 |\n",
      "episode:  51 | score avg: -557.05 | mem size  10400 |\n",
      "episode:  52 | score avg: -513.79 | mem size  10600 |\n",
      "episode:  53 | score avg: -512.22 | mem size  10800 |\n",
      "episode:  54 | score avg: -485.30 | mem size  11000 |\n",
      "episode:  55 | score avg: -498.44 | mem size  11200 |\n",
      "episode:  56 | score avg: -460.38 | mem size  11400 |\n",
      "episode:  57 | score avg: -427.14 | mem size  11600 |\n",
      "episode:  58 | score avg: -397.86 | mem size  11800 |\n",
      "episode:  59 | score avg: -407.15 | mem size  12000 |\n",
      "episode:  60 | score avg: -378.74 | mem size  12200 |\n",
      "episode:  61 | score avg: -395.48 | mem size  12400 |\n",
      "episode:  62 | score avg: -380.45 | mem size  12600 |\n",
      "episode:  63 | score avg: -379.94 | mem size  12800 |\n",
      "episode:  64 | score avg: -366.95 | mem size  13000 |\n",
      "episode:  65 | score avg: -342.83 | mem size  13200 |\n",
      "episode:  66 | score avg: -345.76 | mem size  13400 |\n",
      "episode:  67 | score avg: -311.64 | mem size  13600 |\n",
      "episode:  68 | score avg: -292.83 | mem size  13800 |\n",
      "episode:  69 | score avg: -275.69 | mem size  14000 |\n",
      "episode:  70 | score avg: -260.75 | mem size  14200 |\n",
      "episode:  71 | score avg: -286.85 | mem size  14400 |\n",
      "episode:  72 | score avg: -399.05 | mem size  14600 |\n",
      "episode:  73 | score avg: -408.75 | mem size  14800 |\n",
      "episode:  74 | score avg: -419.17 | mem size  15000 |\n",
      "episode:  75 | score avg: -427.40 | mem size  15200 |\n",
      "episode:  76 | score avg: -526.54 | mem size  15400 |\n",
      "episode:  77 | score avg: -623.87 | mem size  15600 |\n",
      "episode:  78 | score avg: -586.66 | mem size  15800 |\n",
      "episode:  79 | score avg: -565.17 | mem size  16000 |\n",
      "episode:  80 | score avg: -508.79 | mem size  16200 |\n",
      "episode:  81 | score avg: -507.91 | mem size  16400 |\n",
      "episode:  82 | score avg: -457.21 | mem size  16600 |\n",
      "episode:  83 | score avg: -436.94 | mem size  16800 |\n",
      "episode:  84 | score avg: -405.20 | mem size  17000 |\n",
      "episode:  85 | score avg: -377.11 | mem size  17200 |\n",
      "episode:  86 | score avg: -391.83 | mem size  17400 |\n",
      "episode:  87 | score avg: -365.22 | mem size  17600 |\n",
      "episode:  88 | score avg: -341.48 | mem size  17800 |\n",
      "episode:  89 | score avg: -307.37 | mem size  18000 |\n",
      "episode:  90 | score avg: -276.86 | mem size  18200 |\n",
      "episode:  91 | score avg: -274.37 | mem size  18400 |\n",
      "episode:  92 | score avg: -282.77 | mem size  18600 |\n",
      "episode:  93 | score avg: -266.93 | mem size  18800 |\n",
      "episode:  94 | score avg: -263.96 | mem size  19000 |\n",
      "episode:  95 | score avg: -249.37 | mem size  19200 |\n",
      "episode:  96 | score avg: -260.59 | mem size  19400 |\n",
      "episode:  97 | score avg: -246.63 | mem size  19600 |\n",
      "episode:  98 | score avg: -234.75 | mem size  19800 |\n",
      "episode:  99 | score avg: -224.06 | mem size  20000 |\n",
      "episode: 100 | score avg: -260.45 | mem size  20200 |\n",
      "episode: 101 | score avg: -259.05 | mem size  20400 |\n",
      "episode: 102 | score avg: -245.15 | mem size  20600 |\n",
      "episode: 103 | score avg: -245.64 | mem size  20800 |\n",
      "episode: 104 | score avg: -233.58 | mem size  21000 |\n",
      "episode: 105 | score avg: -222.42 | mem size  21200 |\n",
      "episode: 106 | score avg: -212.86 | mem size  21400 |\n",
      "episode: 107 | score avg: -204.42 | mem size  21600 |\n",
      "episode: 108 | score avg: -208.03 | mem size  21800 |\n",
      "episode: 109 | score avg: -210.83 | mem size  22000 |\n",
      "episode: 110 | score avg: -202.96 | mem size  22200 |\n",
      "episode: 111 | score avg: -195.05 | mem size  22400 |\n",
      "End\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEWCAYAAACjYXoKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAnZUlEQVR4nO3debwcVZn/8c+XhLDIEkIiAmGJEmTYBuGyjToihB0nwCAi8jMwYMQB3EZHcENQZtxGFFGHKCjMoIAbIBEhCbI4CnIDKAkxEDYT1rCEfUvy/P6oartu3743fW93dXX1/b5fr/u6VadOVz11O+mnzzlVpxQRmJmZNWO1ogMwM7PyczIxM7OmOZmYmVnTnEzMzKxpTiZmZtY0JxMzM2uak4lZEySFpK1aXdesbJxMzLqMpGMlrZD0fPpzv6QfSto6U2fLNLlV6jwm6SpJ+9bs6wFJL2Xq/EjSOpnt+0r6raTnJD0p6Q5Jn5K0ZjvP2YrnZGLWnf4QEesA6wNTgJeAuZK2r6k3Nq3398As4JeSjq2p8660zs5AD/BZAEnvBn4G/BjYIiI2BN4DTAQ2y+WsrGM5mVjXSb9NnybpLklPp9/K18xsPyT9Br1M0u8l7Vjz2k9I+rOkZyRdWvPaT0p6RNLDkv6l5rjXSzohs36spN8NEOOgddNWw79Kuif91v9FSW9K431W0mWSxqzqbxERKyLi3oj4V+AG4AsD1Hs0Ir6Vbv+KpH6fDRHxEHA1sL0kAd8AzoyI70fEU2mdhRFxSkTcs6rYrLs4mVi3eh+wP/AmYGuq36bfAlwAfBDYEDgPuFLSGpnXHgkcAEwCdgSOTV97APAJYF9gMsk3/jztD+wC7AH8OzADOIbkW//2wHuHuL9fAG9voM7rgTfXbpC0GXAQcHu6fSLw8yHGYF3KycS61bkRsTj9xnwW1Q/e6cB5EXFL+q39QuAVkg/sinMi4uH0tb8CdkrLjwR+GBHzIuIFBviW30JfjYhnI2I+MA+4NiLui4hnSFoIbxni/h4GxjVQh5p6l0taBvyOpHXzH8D4dNujlUqSLklbey9K+n9DjM1KbnTRAZjlZHFm+UFgk3R5C2CapFMy28dktkPmAxJ4MbNtE2BuzX7z9Fhm+aU6628Y4v42BZ5qoA419Q6NiNnZSpKeTBc3Bu4HiIij0m2/A0YNMTYrObdMrFtlB4A3p/qNezFwVkSMzfysHRE/aWCfj9TZb9YLwNqZ9cE+7IdSt1UOA25qoM7jwMJV1FsIPAQc3oK4rAs4mVi3OknSREnjgM8Al6bl3wdOlLS7Eq+TdLCkdRvY52XAsZK2lbQ2cHrN9juAwyWtnd5Pcvwg+xpK3WGTNErSJEnfBvYCzhig3kaSTiY5p9MiYuVg+023/xtwuqQPSNog/XtOBjZq7VlYGTiZWLf6MXAtcB9wL/AlgIjoBT4AnAs8DSwiHWBflYi4GvgmcF36uutqqpwNvErSHXUhcPEguxtK3eHYU9LzwLPA9cB6wK4RcWdNvWWSXgDuJBlcf3dEXNDIASLiUpJxpGNIWnxPkCTcGcBPW3ESVh7yw7Gs20h6ADihtp/fzPLjlomZmTXNycTMzJrmbi4zM2uaWyZmZta0EXvT4vjx42PLLbcsOgwzs1KZO3fuExExobZ8xCaTLbfckt7e3qLDMDMrFUl1Z35wN5eZmTXNycTMzJrmZGJmZk1zMjEzs6Y5mZiZWdOcTMzMrGkdl0wkfU3SX9JncP9S0tjMttMkLZK0UNL+mfID0rJFkk4tJHAzsxGs45IJMAvYPiJ2BO4GTgOQtC1wFLAdyfO5v5s+q2EU8B3gQGBb4L1pXTMzy5g3Dz72MXjttdbvu+OSSURcGxHL09WbgYnp8lTgkoh4JSLuJ3mexG7pz6L02divApekdc3MRoRHHx1428qV8M1vwr77wg47wIwZ8Oc/tz6GjksmNf4FuDpd3pS+z/VekpYNVN6PpOmSeiX1Ll26NIdwzczaZ/p0WG012HhjkGDMGNhyS/jwh+HGG+EjH4F1101aIzffDF/8Ivz1r7DLLq2PpZDpVCTNpv4zrz8TEVekdT4DLKeFT6CLiBkkT4Gjp6fH0yWbWSkdfzxckHke5mqrQUTSffXgg/Dtbyc/AKuvDu97H5xzDowbl19MhSSTiJgy2HZJxwKHAPtEdY78h4DNMtUmpmUMUm5m1jFeegnWWmv4r/+v/4JPfjJJHACjRsHFF8N73pOsL18OF12UtErmzoWttoKf/hRGt+GTvuOeZyLpAOAbwDsiYmmmfDuS53rvBmwCzAEmAyIZqN+HJIncChwdEfMHO05PT094okcza4eHH4ZN0873iRNh8eLB69d65ZWkVfHii9Wyb3wj6b5qN0lzI6KntrwTZw0+F1gDmCUJ4OaIODEi5ku6DLiLpPvrpIhYASDpZOAaYBRwwaoSiZlZu6y3Hjz3XHV9yZKhvX7atKS1UbHPPjB7dmtia6WOa5m0i1smZpanOXNgygAd+jvuCH/606r3seaaSasEki6tRx6BCf2eJNJeA7VMOv1qLjOzjvbSS/3LNtywbyI56qhknOMd70jW612ae9ttyRVZ2Z9KIjn44GQ8pOhEMphO7OYyMyuFpCc+8fTTMHZs3zKoDpYDXH99dfshh8BVVyXLH/oQ/Pd/1z/G/fcnl/t2OrdMzKxhf/gDnHde0VF0htqkscEGfcu23rpvIsmWA8ycmbxm1KhqIpGS1zzxBMyfn7RGypBIwMnEzBokwT/8A5x4Ivz850VH035z5iQ3BVa6oCoOPrh/3bPPhoUL6+8nW75sWXKHOiQ3HlaWN9wQtt02STRl4WRiZqtU+y38iCOKiaMoO+6YjIHUzmk1e3bSVRWR3NMBSYL46EcH39/hhye/R49OWidf/nJy+XCZeczEzAZVm0iy5d10MWi989x666TL6amnqmWjRyctj5NP7lv3nnsaP1Y3tuycTMxsQPUGk7NlCxbA3/1de2PKw0AJ8+67+653U/JsNXdzmVld2Q/YNdesfpBmP1C37YKHPTz+eHV5992T84tIxkcqRo1yIlkVJxMz6yebSNZaq/+9FHfd1d548rTRRtXlm2+uLr/ySpJAnnsuuarKBudkYmZ91LZIsvNBVXRD1xYkz/moGGgcY5112hJK6TmZmNnf1I4d1Lu7u5tkJ0qsXGFlw+NkYmYATJrUd73RMYI31HsyUQn80z9Vlz0e0jwnEzMD4IEHqstD+XB97LGWh9IWv/pV0RF0FycTM2OTTarLI+Fb+l57VZdHwvm2g5OJmfHII0N/zX77tT6OdrnhhqIj6D5OJmYj0IoVcO21yfL48dXyoXxLv+aa1sbULnvuWV12q6R1fAe82QjUjmeCd6rsvSTWOm6ZmI0wA00d0sy39MsvH/5r2yl77m6VtJaTidkIkr0BrzJFSkT/2XCH6rDDmnt9O6y3XnX5058uLo5u5WRiNoK88EJ1OXtDYrd3e82cmUyLArDaanDWWcXG042cTMxGgB12GNldPIccUl1esaK4OLqZk4lZiVWmhD/uuL7lv/89rL9+9amA8+ZVt51xRuuOv2BB6/bVKpVz/utfk/Xs7L9lfwBVJ+vyxq1Zd1st/Tr4ox/B/ffD9dcPPMAOrW+RbLNNa/fXrO23ry5vsQWcfnp1PGiNNZJH41o+OrZlIunfJIWk8em6JJ0jaZGkP0vaOVN3mqR70p9pxUVt1j614xw33NDeRNKJ5s/vu55thb38cntjGWk6MplI2gzYD/hrpvhAYHL6Mx34Xlp3HHA6sDuwG3C6pA3aGrBZG1S6bypW1fdfuVKr8pO3oid8HGxMaO7c9sYyEnVkMgHOBv4dyP6TmApcFImbgbGSNgb2B2ZFxFMR8TQwCzig7RGb5ejcc6vLtUmlXrIoohXy2GODt4zy9NOfVpe32y75HZE8IfFtb4Odd67/OmudjksmkqYCD0XEn2o2bQoszqwvScsGKq+37+mSeiX1Ll26tIVRm+XrlFNWXSciudy33YnkzDP7rktw0EHtjeHII6vL2YsNli+Hm25qbywjVSHJRNJsSfPq/EwFPg18Po/jRsSMiOiJiJ4JEybkcQizlst+27/llr7bahPHmmvmH0+tz32ufxxXX92+Vso++1SXFy8euJ7lq5CruSJiSr1ySTsAk4A/KfmXOBG4TdJuwEPAZpnqE9Oyh4C9asqvb3nQZh1gt906dyC9Elc2iVSW84z5uuuqyxMn5nccG1xHdXNFxJ0R8fqI2DIitiTpsto5Ih4FrgTen17VtQfwTEQ8AlwD7Cdpg3Tgfb+0zKz0ynijYUTfmwSh/zhPq2yQudSmLH+fblWm+0x+DRwELAJeBI4DiIinJH0RuDWtd2ZEPFVMiGYG1acYvulNcN99+R1n2bL89m1D09HJJG2dVJYDOGmAehcAF7QpLLO2KGOrpNa99ya/82iVdMPfp5t0VDeXmfW31lpFR9B5Vst8cvlams7gZGLWgbLful98sbg4Wi073clwTZrUtyXy+OPN79Oa52Ri1mGyTwLcfPPi4shD7XQnw/HAA9Vld291DicTsw6TfUb5gw8WF0cnWn316rITSWdxMjHrUFtvXXQErTNjRmv2s3x5a/ZjredkYtZBsmMlCxcWF0erfeADrd2fWyWdx8nErEB53czXjfx36mxOJmYFyc5kW28m4G7lS527k5OJWUFuv73oCIrR7EOqujnRlpmTiVkBsq2Q3/++77Zu/bBsppvKXVydz8nErE1Wrkx+77VX3/I996wmkPHj2xpSW91999DqjxrVv/uv9lHF1jn81pi1wUDfrLOtkG5tkVRstdXQ6leSb9Zrr7UmFms9t0zMCjJzZtERFGfRosbrtvM59jZ8bpmY5ex3v6su+wMxMXny4H+L1fw1t3T8lpnl7O1vLzqC8nHSLR8nEzNrm6E+VtdJpTycTMxyNFJuRGzU4sWrrvOmN+Ufh7Wek4mZdZQ8H/Nr+XEyMcvJTTfVX7bEkUcOvv2ii9oTh7WGYoS2vXt6eqK3t7foMKyLuYurvsH+LvvtB7Nm1d9mnUHS3IjoqS13y8QsB9mWyNSpxcVRJvffX00kVj5OJmY5+Md/rC5ffnlhYXSkT3yifvkb31hddqukfDoymUg6RdJfJM2X9NVM+WmSFklaKGn/TPkBadkiSacWE7VZItsqeetbi4ujU33ta/3Lsl1fc+a0LxZrnY67A17SO4GpwN9HxCuSXp+WbwscBWwHbALMllR5sOl3gH2BJcCtkq6MiLvaH71Z31ZJ9u53q+/EE6vLkyfD3nsXF4sNX8clE+BDwJcj4hWAiHg8LZ8KXJKW3y9pEbBbum1RRNwHIOmStK6TiRVqu+2KjqDzrbcePPdcdX2oMwtb5+jEbq6tgbdLukXSDZJ2Tcs3BbK3PC1JywYqN2u7bHfNvHnFxVEW2URy773FxWHNa6hlImktYPOIWNiKg0qaDbyhzqbPpDGNA/YAdgUuk/TGOnWHc9zpwHSAzTffvBW7NLNhkPoPsr+xJf/LrSirbJlIehdwB/CbdH0nSVc2c9CImBIR29f5uYKkZfGLSPwRWAmMBx4CNsvsZmJaNlB5vePOiIieiOiZMGFCM6dgNqgbbig6gs52zz191z/1qWLisNZppJvrCyRjE8sAIuIOYFJuEcHlwDsB0gH2McATwJXAUZLWkDQJmAz8EbgVmCxpkqQxJIP0TSU7s+HIdnFlB+Gtv9r5t7785WLisNZppJvrtYh4Rn0fFZfnVeAXABdImge8CkyL5Db9+ZIuIxlYXw6cFBErACSdDFwDjAIuiIj5OcZnZi20005FR2Ct0EgymS/paGCUpMnAh4Hf5xVQRLwKHDPAtrOAs+qU/xr4dV4xmQ3FF79YdATlMH06XHUV3HZb0ZFYK6xybi5Ja5MMjO+XFl0DfCkiXs45tlx5bi5rJc/DZSPFQHNzDdoykTQKmBkR7yRJKGZmZv0MOgCfjkmslLR+m+IxK7Uzzyw6ArNiNDJm8jxwp6RZwAuVwoj4cG5RmZVItovrc58rLg6zIjWSTH6R/piZmdW1ymQSERem929UJlVcGBGv5RuWWTkcckh12QPvNpKtMplI2gu4EHgAELCZpGkRcWOukZmVwMyZRUdg1hka6eb6L2C/yrxc6V3pPwF2yTMwszLZZpuiIzArViPTqayeneAxIu4GVs8vJLNyyA68L1hQXBxmnaCRlkmvpB8A/5uuvw/w3X7WVVasgFGjio7CrLwaSSYfAk4imUYF4Cbgu7lFZFaA0en/hEYH0X3Hu1lfjSST0cC3IuIb8Le74tfINSqzNuo7h6mZDUcjYyZzgLUy62sBs/MJx6xYsxv4l+1WiVl/jSSTNSPi+cpKurx2fiGZFWfffYuOwKycGkkmL0jaubIiaRfgpfxCMmufoXZx9WTmSnWrxKyqkTGTjwI/lfQwyU2LbwDek2dQZp1q7tyiIzDrTI1Mp3KrpG2AN6dFnk7Fus7RR8OPf9x4/c9/Pr9YzMpold1ckt5NMm4yDzgUuDTb7WVWVtkurosvHlr9M85ofTxmZdbImMnnIuI5SW8D9gHOB76Xb1hmZlYmjSSTFenvg4HvR8RMYEx+IZm117XX9l3/7Gf71/HlwGaDaySZPCTpPJJB919LWqPB15mVQu3lwGedVUwcZmXWSFI4ErgG2D8ilgHjgE/mGZRZ3oZ717tbJWb1NXI114tknrQYEY8Aj+QZlFkn8XQrZqvWcd1VknaSdLOkOyT1StotLZekcyQtkvTnmhspp0m6J/2ZVlz0VmazZhUdgVl5NXLTYrt9FTgjIq6WdFC6vhdwIDA5/dmd5Iqy3SWNA04HeoAA5kq6MiKeLiJ4K5dst9WUKYPXnTMn31jMyqyhlomkLSRNSZfXkrRujjEFsF66vD7wcLo8FbgoEjcDYyVtDOwPzIqIp9IEMgs4IMf4rORqr94azPjx1eW99259LGbdopGbFj8A/Aw4Ly2aCFyeY0wfBb4maTHwdeC0tHxTYHGm3pK0bKDyfiRNT7vOepcuXdrquK0k9t9/1XW22ir5/eST+cZi1i0aaZmcBLwVeBYgIu4BXt/MQSXNljSvzs9UkodxfSwiNgM+RnKTZEtExIyI6ImIngkTJrRqt9aF7r237/qGGxYTh1lZNDJm8kpEvKr0khZJo0m6ooYtIgbsnZZ0EfCRdPWnwA/S5YeAzTJVJ6ZlD5GMqWTLr28mPhsZ1l9/8O3XXVddfuKJfGMxK7tGWiY3SPo0sJakfUk+4H+VY0wPA+9Il/cG7kmXrwTen17VtQfwTHqZ8jXAfpI2kLQBsF9aZjaoZcsG377PPm0Jw6wrNNIyORU4HrgT+CDwa6qthTx8APhW2gJ6GZielv8aOAhYBLwIHAcQEU9J+iJwa1rvzIh4Ksf4rMQ+/enBt0f4vhKz4VCM0Ft6e3p6ore3t+gwrM2GM8eWE4xZlaS5EdFTW77KlomkO+k/RvIM0At8KSJ8vYt1NScSs1VrpJvrapKZgyuPDjqK5BnwjwI/At6VS2RmOfrNb4qOwKy7NJJMpkRE9mFYd0q6LSJ2lnRMXoGZtdo1mcsyGrnXxMwa18jVXKMq82MBSNoVGJWuLs8lKrMcHOB5Ecxy00jL5ATgAknrACK5efEESa8D/jPP4MzMrBwamYL+VmAHSeun689kNl+WV2BmeRmhFzCa5aqhWYMlHQxsB6xZuRM+Is7MMS6zltpxx6IjMOtujUz0+N8kj+w9haSb693AFjnHZdZSd95ZdARm3a2RAfh/iIj3A09HxBnAnsDW+YZlZmZl0kgyeTn9/aKkTYDXgI3zC8ksPx4vMctHI2Mmv5I0FvgacBvJ3fDfzzMos1byHexm+Rs0mUhaDZgTEcuAn0u6Cliz5oouMzMb4Qbt5oqIlcB3MuuvOJFYmWSnTXEXl1l+GhkzmSPpnyV3Flj5HHhg0RGYjQyNJJMPkjwQ61VJz0p6TtKzOcdlZmYl0sgd8Ou2IxCzVhvOs0vMbHgauWlRko6R9Ll0fbPsxI9mZmaNdHN9l+RGxaPT9efJDMqbdbqrry46ArPu18h9Jrunzy65HSAinpY0Jue4zJqS7eLy1PNm+WukZfKapFGkj+6VNAFYmWtUZmZWKo0kk3OAXwKvl3QW8DvgP3KNyqxFPPBu1h6NXM11saS5wD4kswYfGhELco/MbJh8R5RZ+60ymUg6B7gkIjzobmZmdTXSzTUX+KykeyV9XVJPsweV9G5J8yWtrN2fpNMkLZK0UNL+mfID0rJFkk7NlE+SdEtafqkvDhjZslduuYvLrH1WmUwi4sKIOAjYFVgIfEXSPU0edx5wOHBjtlDStsBRJE91PAD4rqRR6QUA3wEOBLYF3pvWBfgKcHZEbAU8DRzfZGxWYgcdVHQEZiNTIy2Tiq2AbUiesviXZg4aEQsiYmGdTVNJutReiYj7gUXAbunPooi4LyJeBS4Bpqbzhe0N/Cx9/YXAoc3EZmZmQ9fIHfBfTVsiZ5K0KHoi4l05xbMpsDizviQtG6h8Q2BZRCyvKa9L0nRJvZJ6ly5d2tLArbO4i8usvRq5afFeYM+IeGIoO5Y0G3hDnU2fiYgrhrKvVomIGcAMgJ6eHn/cdBlfxWVWnEYuDT5P0gbpfFxrZspvHORlRMSUYcTzELBZZn1iWsYA5U8CYyWNTlsn2fpmZtYmjXRznUAyUH4NcEb6+ws5xXMlcJSkNSRNAiYDfwRuBSanV26NIRmkvzIiAvgtcET6+mlAIa0e6xzu4jJrv0YG4D9CciXXgxHxTuAtwLJmDirpMElLSCaQnCnpGoCImA9cBtwF/AY4KSJWpK2Ok0kS2QLgsrQuwKeAj0taRDKGcn4zsVk5uYvLrFiKVXyNk3RrROwq6Q6SSR9fkTQ/IrZrS4Q56enpid7e3qLDsBbxs0vM2kPS3Ijod79hIwPwSySNBS4HZkl6GniwteGZtYYTiVkxGhmAPyxd/IKk3wLrk3RBmXUEd3GZFa+RlsnfRMQNeQViZmblNZQ74M062lVXFR2B2cjlZGKllu3iOvjg4uIwG+mcTMzMrGlOJlZaM2dWl30Vl1mxnEystA45pOgIzKzCycTMzJrmZGKltOee1WV3cZkVz8nESunmm4uOwMyynEzMzKxpTiZWau7iMusMTiZWOp6Ly6zzOJmYmVnTnEysIy1fnrRAJPj4x+vXcReXWedwMrGOtPrq1eWzz64mFndxmXUmJxMzM2uak8kQ+dtx/q64orocUb87681vbl88ZrZqTibDdNhhq65jw3Poof3LIvomkL/8pW3hmFkDhvSkRau6/PKiIxh5Kglk5cpi4zCz/twysY410NVaq/lfrVnH8X/LIfLlqPnyeJRZORWSTCS9W9J8SSsl9WTK95U0V9Kd6e+9M9t2ScsXSTpHSj52JI2TNEvSPenvDdp3Hu06Unfw5b1m3auolsk84HDgxpryJ4B3RcQOwDTgfzLbvgd8AJic/hyQlp8KzImIycCcdN1K7qMfLToCMxuKQpJJRCyIiIV1ym+PiIfT1fnAWpLWkLQxsF5E3BwRAVwEHJrWmwpcmC5fmCm3DlavdZItO/vs9sViZs3r5DGTfwZui4hXgE2BJZltS9IygI0i4pF0+VFgo4F2KGm6pF5JvUuXLh12YB43Gbqf/3zw7e76Miu33C4NljQbeEOdTZ+JiCvqlGdfux3wFWC/oRwzIkLSgB/1ETEDmAHQ09PTkpQgObk04ogj+pdV/naTJ/ct99/TrHxySyYRMWU4r5M0Efgl8P6IuDctfgiYmKk2MS0DeEzSxhHxSNod9vhwYy6LFStg1Kiio2iN2haJE4lZOXVUN5ekscBM4NSI+L9KedqN9aykPdKruN4PVFo3V5IM1pP+HrTV0yrbb1+/vHK1Up431o0eXT3O+efnd5w8DDQ9SmWbmZVTUZcGHyZpCbAnMFPSNemmk4GtgM9LuiP9eX267V+BHwCLgHuBq9PyLwP7SroHmJKu5+7OO6vL9fr782o51B7rhBM6f7xhVfEddJATiVnZKUbo/+Kenp7o7e1tah/ZD8mI9nTZDPTB3MlvY+3fyczKS9LciOipLe+obq6yyX4wDqd1sMsuQ3td7Ydys8dvt/Hji47AzPLiiR5zNNCVXrUf/LX1Xnut78Ohyix7rk1cjW1mHc4tkybVJotzzhm4biNTiUgwZkzy++ij+5bXO2YnJ50ytJbMrDWcTFrslFP6l73lLfU/WGu7qXbdte/2n/xk1Qno1Vf77qNT+JJfs5HFyaQFIpIP9coHZvaD89xz4Y47+tev9+HayPUAZfxQLmPMZjY0TiYtMlB3U7alcu65/T9Y633Q1ks2g92f0WldXb56y2zkcTJpo5NOql/+ve9Vl9ddt7pcSSCr+kDu1K4uMxs5nExy0kgLpOLEE+GMM5KruJ59Nt+42mmLLYqOwMzaxcmkDRrp6vn855NpUobrvPOqy2usMfz9NGuttarLDzxQWBhm1mZOJjlqpIuqVaZPry5nu73a7eWXizu2mRXHyaSLXHBBdbnI1gnAuHHFHt/M2svJpIscd1x1uYjWyYYbVpeffLL9xzez4jiZdJls62TGjPyPV5ngUoKnnsr/eGbWmZxMuky2dfLBD+Z/vNXq/AvqtPtezCx/TibWMh/6UHU2ADMbWTxrcBeq92yVPOy7b99jmtnI5ZZJl8szqcyend++zaxcnEzMzKxpTiZdql6309NPt647ypM5mlmWk8kIULl0d9y45Oqriy4qOiIz6zZOJiPQtGnw5je3Zl9TprRmP2ZWbk4mXeyHP+y7/va3V5fvvnv4g/PZ182aNbx9mFl3cTLpYsceW12OgBtv7D++4eefmFkrFJJMJL1b0nxJKyX11Nm+uaTnJX0iU3aApIWSFkk6NVM+SdItafmlksa06zzKYKCnNmY5oZhZs4pqmcwDDgduHGD7N4CrKyuSRgHfAQ4EtgXeK2nbdPNXgLMjYivgaeD4vILuJvUSyl57NbcPMxu5CkkmEbEgIhbW2ybpUOB+YH6meDdgUUTcFxGvApcAUyUJ2Bv4WVrvQuDQvOLuNrXJ4IYbVt1KcSvGzOrpqDETSesAnwLOqNm0KbA4s74kLdsQWBYRy2vKB9r/dEm9knqXLl3ausBLLAKOr2nLOWGY2VDllkwkzZY0r87P1EFe9gWSLqvn84gpImZERE9E9EyYMCGPQ5TSD37gLisza05uEz1GxHDuQNgdOELSV4GxwEpJLwNzgc0y9SYCDwFPAmMljU5bJ5VyG4bsBJHS4AnGycfMsjpq1uCI+NudEJK+ADwfEedKGg1MljSJJFkcBRwdESHpt8ARJOMo04Ar2h9591hnHXg+bRfWJhR3f5nZQIq6NPgwSUuAPYGZkq4ZrH7a6jgZuAZYAFwWEZUB+k8BH5e0iGQM5fz8Iu9+zz3Xd70yFYuZ2WAUI7S/oqenJ3p7e4sOo2MNlkBWX90PwDIbqSTNjYh+9wd21NVc1jnq3exY4URiZrU6aszEOs8Ibbia2RC5ZWJmZk1zMjEzs6Y5mZiZWdOcTMzMrGlOJmZm1jQnEzMza5qTiZmZNc3JxMzMmjZip1ORtBR4cJgvHw880cJwOonPrZy69dy69bygvOe2RUT0e4bHiE0mzZDUW29umm7gcyunbj23bj0v6L5zczeXmZk1zcnEzMya5mQyPDOKDiBHPrdy6tZz69bzgi47N4+ZmJlZ09wyMTOzpjmZmJlZ05xMhkjSAZIWSlok6dSi4xkuSZtJ+q2kuyTNl/SRtHycpFmS7kl/b1B0rMMlaZSk2yVdla5PknRL+t5dKmlM0TEOh6Sxkn4m6S+SFkjas1veN0kfS/89zpP0E0lrlvV9k3SBpMclzcuU1X2flDgnPcc/S9q5uMiHx8lkCCSNAr4DHAhsC7xX0rbFRjVsy4F/i4htgT2Ak9JzORWYExGTgTnpell9BFiQWf8KcHZEbAU8DRxfSFTN+xbwm4jYBvh7knMs/fsmaVPgw0BPRGwPjAKOorzv24+AA2rKBnqfDgQmpz/Tge+1KcaWcTIZmt2ARRFxX0S8ClwCTC04pmGJiEci4rZ0+TmSD6RNSc7nwrTahcChhQTYJEkTgYOBH6TrAvYGfpZWKeW5SVof+EfgfICIeDUiltEl7xvJo8TXkjQaWBt4hJK+bxFxI/BUTfFA79NU4KJI3AyMlbRxWwJtESeTodkUWJxZX5KWlZqkLYG3ALcAG0XEI+mmR4GNioqrSd8E/h1Yma5vCCyLiOXpelnfu0nAUuCHaRfeDyS9ji543yLiIeDrwF9JksgzwFy6432rGOh9Kv1ni5PJCCdpHeDnwEcj4tnstkiuGy/dteOSDgEej4i5RceSg9HAzsD3IuItwAvUdGmV+H3bgOQb+iRgE+B19O8m6hplfZ8G4mQyNA8Bm2XWJ6ZlpSRpdZJEcnFE/CItfqzSvE5/P15UfE14K/BPkh4g6Yrcm2ScYWzafQLlfe+WAEsi4pZ0/WckyaUb3rcpwP0RsTQiXgN+QfJedsP7VjHQ+1T6zxYnk6G5FZicXl0yhmRw8MqCYxqWdAzhfGBBRHwjs+lKYFq6PA24ot2xNSsiTouIiRGxJcl7dF1EvA/4LXBEWq2s5/YosFjSm9OifYC76IL3jaR7aw9Ja6f/PivnVvr3LWOg9+lK4P3pVV17AM9kusNKwXfAD5Gkg0j640cBF0TEWcVGNDyS3gbcBNxJdVzh0yTjJpcBm5NM0X9kRNQOIpaGpL2AT0TEIZLeSNJSGQfcDhwTEa8UGN6wSNqJ5MKCMcB9wHEkXwxL/75JOgN4D8nVhrcDJ5CMHZTufZP0E2AvkqnmHwNOBy6nzvuUJs9zSbr1XgSOi4jeAsIeNicTMzNrmru5zMysaU4mZmbWNCcTMzNrmpOJmZk1zcnEzMya5mRiVgBJZ0qa0oL9PN+KeMya5UuDzUpM0vMRsU7RcZi5ZWLWIpKOkfRHSXdIOi99nsrzks5On9ExR9KEtO6PJB2RLn85fa7MnyV9PS3bUtJ1adkcSZun5ZMk/UHSnZK+VHP8T0q6NX3NGe0+fxvZnEzMWkDS35Hcuf3WiNgJWAG8j2Sywt6I2A64geQu6OzrNgQOA7aLiB2BSoL4NnBhWnYxcE5a/i2SSR53IJlZt7Kf/UiehbEbsBOwi6R/bP2ZmtXnZGLWGvsAuwC3SrojXX8jyVQ1l6Z1/hd4W83rngFeBs6XdDjJVBoAewI/Tpf/J/O6twI/yZRX7Jf+3A7cBmxDklzM2mL0qquYWQNE0pI4rU+h9Lmaen0GKSNiuaTdSJLPEcDJJLMcD6beQKeA/4yI84YUtVmLuGVi1hpzgCMkvR7+9qzvLUj+j1VmvD0a+F32RenzZNaPiF8DHyN5DC/A70lmPIaku+ymdPn/asorrgH+Jd0fkjatxGLWDm6ZmLVARNwl6bPAtZJWA14DTiJ5eNVu6bbHScZVstYFrpC0Jknr4uNp+SkkT1P8JMmTFY9Lyz8C/FjSp8hMxR4R16bjNn9IJqDleeAYyvlcEyshXxpsliNfumsjhbu5zMysaW6ZmJlZ09wyMTOzpjmZmJlZ05xMzMysaU4mZmbWNCcTMzNr2v8H84PZz9XadZoAAAAASUVORK5CYII="
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "env.close()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}