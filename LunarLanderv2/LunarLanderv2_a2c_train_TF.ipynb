{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import gym\r\n",
    "import sys\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow.keras.layers import Dense\r\n",
    "from collections import deque\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class A2C(tf.keras.Model):\r\n",
    "    def __init__(self, state_size, action_size):\r\n",
    "        super(A2C, self).__init__()\r\n",
    "        self.fc1 = Dense(64, activation='relu')\r\n",
    "        self.fc2 = Dense(64, activation='relu')\r\n",
    "        self.actor  = Dense(action_size, activation='softmax',\r\n",
    "                                        kernel_initializer=tf.keras.initializers.RandomUniform(-1e-3,1e-3))\r\n",
    "        self.critic = Dense(1,\r\n",
    "                                        kernel_initializer=tf.keras.initializers.RandomUniform(-1e-3,1e-3))\r\n",
    "        \r\n",
    "    def call(self, x):\r\n",
    "        x      = self.fc1(x)\r\n",
    "        x      = self.fc2(x)\r\n",
    "        policy = self.actor(x)\r\n",
    "        value  = self.critic(x)\r\n",
    "        return policy, value"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class A2CAgent:\r\n",
    "    def __init__(self, state_size, action_size):\r\n",
    "        self.state_size = state_size\r\n",
    "        self.action_size= action_size\r\n",
    "        \r\n",
    "        # Hyper params for learning\r\n",
    "        self.discount_factor = 0.99\r\n",
    "        self.learning_rate = 0.001\r\n",
    "        \r\n",
    "        self.model = A2C(self.state_size,self.action_size)\r\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr=self.learning_rate)\r\n",
    "        \r\n",
    "    def get_action(self, state):\r\n",
    "        policy, _ = self.model(state)\r\n",
    "        policy = np.array(policy[0])\r\n",
    "        return np.random.choice(self.action_size, 1, p=policy)[0]\r\n",
    "        \r\n",
    "    def train_model(self, state, action, reward, next_state, done):\r\n",
    "        model_params = self.model.trainable_variables\r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            policy, value      = self.model(state)\r\n",
    "            _,      next_value = self.model(next_state)\r\n",
    "            target = reward + (1 - done) * self.discount_factor * next_value[0]\r\n",
    "            \r\n",
    "            # For policy network\r\n",
    "            one_hot_action = tf.one_hot([action], self.action_size)\r\n",
    "            action_prob = tf.reduce_sum(one_hot_action * policy, axis=1)\r\n",
    "            cross_entropy = - tf.math.log(action_prob + 1e-5)\r\n",
    "            advantage = tf.stop_gradient(target - value[0])\r\n",
    "            actor_loss = tf.reduce_mean(cross_entropy * advantage)\r\n",
    "            \r\n",
    "            # For value network\r\n",
    "            critic_loss = 0.5 * tf.square(tf.stop_gradient(target) - value[0])\r\n",
    "            critic_loss = tf.reduce_mean(critic_loss)\r\n",
    "            \r\n",
    "            # integrate losses\r\n",
    "            loss = 0.2 * actor_loss + critic_loss\r\n",
    "            \r\n",
    "        grads = tape.gradient(loss, model_params)\r\n",
    "        self.optimizer.apply_gradients(zip(grads, model_params))\r\n",
    "        return np.array(loss)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "%matplotlib tk\r\n",
    "\r\n",
    "ENV_NAME = 'LunarLander-v2'\r\n",
    "EPISODES = 4000\r\n",
    "END_SCORE = 200\r\n",
    "\r\n",
    "if __name__ == \"__main__\":\r\n",
    "    env = gym.make(ENV_NAME)\r\n",
    "    state_size = env.observation_space.shape[0]\r\n",
    "    action_size = env.action_space.n\r\n",
    "\r\n",
    "    agent = A2CAgent(state_size, action_size)\r\n",
    "    print('Env Name : ',ENV_NAME)\r\n",
    "    print('States {}, Actions {}'\r\n",
    "            .format(state_size, action_size))\r\n",
    "\r\n",
    "    scores, episodes, losses = [], [], []\r\n",
    "    score_avg = 0\r\n",
    "    \r\n",
    "    end = False\r\n",
    "    \r\n",
    "    for e in range(EPISODES):\r\n",
    "        # Episode initialization\r\n",
    "        done = False\r\n",
    "        score = 0\r\n",
    "        loss_list = []\r\n",
    "        \r\n",
    "        state = env.reset()\r\n",
    "        state = np.reshape(state, [1, state_size])\r\n",
    "        \r\n",
    "        while not done:\r\n",
    "            # env.render()\r\n",
    "\r\n",
    "            # Interact with env.\r\n",
    "            action = agent.get_action(state)\r\n",
    "            next_state, reward, done, info = env.step(action)\r\n",
    "            next_state = np.reshape(next_state, [1, state_size])\r\n",
    "            loss = agent.train_model(state, action, reward, next_state, done)\r\n",
    "            state = next_state\r\n",
    "\r\n",
    "            # \r\n",
    "            score += reward\r\n",
    "            loss_list.append(loss)\r\n",
    "            if done:\r\n",
    "                # agent.update_target_model()\r\n",
    "\r\n",
    "                score_avg = 0.9 * score_avg + 0.1 * score if score_avg != 0 else score\r\n",
    "                print('epi: {:3d} | score avg {:3.2f} | loss: {:.4f}'.format(e, score_avg, np.mean(loss_list)))\r\n",
    "\r\n",
    "                # Save data for plot\r\n",
    "                scores.append(score_avg)\r\n",
    "                episodes.append(e)\r\n",
    "                losses.append(np.mean(loss_list))\r\n",
    "                \r\n",
    "                # View data\r\n",
    "                plt.subplot(211)\r\n",
    "                plt.plot(episodes, scores, 'b')\r\n",
    "                plt.xlabel('episode')\r\n",
    "                plt.ylabel('average score')\r\n",
    "                plt.title('cartpole A2C')\r\n",
    "                plt.grid()\r\n",
    "                \r\n",
    "                plt.subplot(212)\r\n",
    "                plt.plot(episodes, losses, 'b')\r\n",
    "                plt.xlabel('episode')\r\n",
    "                plt.ylabel('loss')\r\n",
    "                plt.grid()\r\n",
    "                \r\n",
    "                plt.savefig('./save_model/LunarLanderv2_a2c_TF.png')\r\n",
    "\r\n",
    "                if score_avg > END_SCORE:\r\n",
    "                    agent.model.save_weights('./save_model/LunarLanderv2_a2c_TF', save_format='tf')\r\n",
    "                    end = True\r\n",
    "                    break\r\n",
    "        if end == True:\r\n",
    "            np.save('./save_model/LunarLanderv2_a2c_epi',episodes)\r\n",
    "            np.save('./save_model/LunarLanderv2_a2c_score',scores)\r\n",
    "            np.save('./save_model/LunarLanderv2_a2c_loss',losses)\r\n",
    "            env.close()\r\n",
    "            print(\"End\")\r\n",
    "            break"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Env Name :  LunarLander-v2\n",
      "States 8, Actions 4\n",
      "epi:   0 | score avg -350.88 | loss: 25.1242\n",
      "epi:   1 | score avg -348.28 | loss: 188.8681\n",
      "epi:   2 | score avg -333.36 | loss: 44.7586\n",
      "epi:   3 | score avg -350.35 | loss: 19.0472\n",
      "epi:   4 | score avg -370.28 | loss: 777.5601\n",
      "epi:   5 | score avg -421.80 | loss: 2756.9612\n",
      "epi:   6 | score avg -413.02 | loss: 675.2480\n",
      "epi:   7 | score avg -415.90 | loss: 1031.9916\n",
      "epi:   8 | score avg -427.39 | loss: 921.1139\n",
      "epi:   9 | score avg -458.58 | loss: 1535.3828\n",
      "epi:  10 | score avg -510.93 | loss: 1693.2179\n",
      "epi:  11 | score avg -519.83 | loss: 643.3398\n",
      "epi:  12 | score avg -568.33 | loss: 1594.1311\n",
      "epi:  13 | score avg -583.42 | loss: 976.9350\n",
      "epi:  14 | score avg -634.11 | loss: 948.9446\n",
      "epi:  15 | score avg -624.16 | loss: 630.1415\n",
      "epi:  16 | score avg -644.56 | loss: 603.0780\n",
      "epi:  17 | score avg -632.12 | loss: 420.9306\n",
      "epi:  18 | score avg -611.56 | loss: 210.6061\n",
      "epi:  19 | score avg -626.89 | loss: 195.8028\n",
      "epi:  20 | score avg -618.81 | loss: 95.6970\n",
      "epi:  21 | score avg -611.95 | loss: 372.3031\n",
      "epi:  22 | score avg -597.98 | loss: 331.1204\n",
      "epi:  23 | score avg -597.00 | loss: 51.7447\n",
      "epi:  24 | score avg -585.02 | loss: 218.5569\n",
      "epi:  25 | score avg -581.82 | loss: 70.1861\n",
      "epi:  26 | score avg -604.77 | loss: 60.0421\n",
      "epi:  27 | score avg -590.92 | loss: 23.8453\n",
      "epi:  28 | score avg -575.64 | loss: 277.7049\n",
      "epi:  29 | score avg -574.01 | loss: 40.9766\n",
      "epi:  30 | score avg -562.73 | loss: 18.8259\n",
      "epi:  31 | score avg -572.54 | loss: 110.1734\n",
      "epi:  32 | score avg -548.59 | loss: 16.7012\n",
      "epi:  33 | score avg -554.16 | loss: 39.1598\n",
      "epi:  34 | score avg -549.71 | loss: 35.5316\n",
      "epi:  35 | score avg -544.71 | loss: 18.7343\n",
      "epi:  36 | score avg -542.24 | loss: 33.8957\n",
      "epi:  37 | score avg -594.01 | loss: 228.7789\n",
      "epi:  38 | score avg -585.58 | loss: 286.9327\n",
      "epi:  39 | score avg -580.91 | loss: 36.1704\n",
      "epi:  40 | score avg -559.21 | loss: 9.1573\n",
      "epi:  41 | score avg -549.72 | loss: 260.7069\n",
      "epi:  42 | score avg -550.64 | loss: 44.0140\n",
      "epi:  43 | score avg -528.86 | loss: 5.9050\n",
      "epi:  44 | score avg -523.38 | loss: 5.8459\n",
      "epi:  45 | score avg -554.08 | loss: 75.4910\n",
      "epi:  46 | score avg -623.81 | loss: 159.7018\n",
      "epi:  47 | score avg -609.82 | loss: 47.6849\n",
      "epi:  48 | score avg -602.15 | loss: 52.4329\n",
      "epi:  49 | score avg -596.55 | loss: 161.5248\n",
      "epi:  50 | score avg -616.05 | loss: 38.1544\n",
      "epi:  51 | score avg -633.61 | loss: 35.4486\n",
      "epi:  52 | score avg -638.56 | loss: 33.4876\n",
      "epi:  53 | score avg -634.17 | loss: 10.2984\n",
      "epi:  54 | score avg -646.81 | loss: 106.5879\n",
      "epi:  55 | score avg -651.24 | loss: 8.9694\n",
      "epi:  56 | score avg -667.69 | loss: 45.4632\n",
      "epi:  57 | score avg -678.79 | loss: 38.2980\n",
      "epi:  58 | score avg -652.71 | loss: 4.8892\n",
      "epi:  59 | score avg -641.48 | loss: 1.4764\n",
      "epi:  60 | score avg -626.58 | loss: 223.8499\n",
      "epi:  61 | score avg -645.17 | loss: 48.7017\n",
      "epi:  62 | score avg -656.73 | loss: 257.7915\n",
      "epi:  63 | score avg -631.10 | loss: 30.8808\n",
      "epi:  64 | score avg -605.62 | loss: 14.5042\n",
      "epi:  65 | score avg -600.61 | loss: 213.0060\n",
      "epi:  66 | score avg -596.98 | loss: 36.7322\n",
      "epi:  67 | score avg -580.49 | loss: 8.1655\n",
      "epi:  68 | score avg -590.23 | loss: 81.7628\n",
      "epi:  69 | score avg -581.75 | loss: 47.6150\n",
      "epi:  70 | score avg -561.96 | loss: 43.3990\n",
      "epi:  71 | score avg -590.35 | loss: 204.0674\n",
      "epi:  72 | score avg -645.22 | loss: 457.3606\n",
      "epi:  73 | score avg -646.74 | loss: 208.3731\n",
      "epi:  74 | score avg -625.05 | loss: 49.9977\n",
      "epi:  75 | score avg -613.04 | loss: 67.9365\n",
      "epi:  76 | score avg -588.14 | loss: 29.1255\n",
      "epi:  77 | score avg -589.12 | loss: 59.9081\n",
      "epi:  78 | score avg -572.38 | loss: 30.6301\n",
      "epi:  79 | score avg -577.29 | loss: 149.0343\n",
      "epi:  80 | score avg -556.92 | loss: 24.8943\n",
      "epi:  81 | score avg -553.83 | loss: 43.1408\n",
      "epi:  82 | score avg -545.85 | loss: 24.7887\n",
      "epi:  83 | score avg -541.62 | loss: 29.6600\n",
      "epi:  84 | score avg -555.66 | loss: 76.7164\n",
      "epi:  85 | score avg -555.17 | loss: 27.9041\n",
      "epi:  86 | score avg -546.05 | loss: 71.1848\n",
      "epi:  87 | score avg -539.31 | loss: 13.6431\n",
      "epi:  88 | score avg -560.66 | loss: 83.0211\n",
      "epi:  89 | score avg -552.90 | loss: 51.1846\n",
      "epi:  90 | score avg -557.38 | loss: 242.2178\n",
      "epi:  91 | score avg -535.34 | loss: 12.1300\n",
      "epi:  92 | score avg -521.40 | loss: 3.3573\n",
      "epi:  93 | score avg -523.64 | loss: 26.5614\n",
      "epi:  94 | score avg -547.10 | loss: 184.3676\n",
      "epi:  95 | score avg -545.65 | loss: 23.2158\n",
      "epi:  96 | score avg -553.72 | loss: 24.2478\n",
      "epi:  97 | score avg -550.30 | loss: 36.7336\n",
      "epi:  98 | score avg -537.31 | loss: 247.4022\n",
      "epi:  99 | score avg -538.67 | loss: 5.2249\n",
      "epi: 100 | score avg -521.47 | loss: 10.2208\n",
      "epi: 101 | score avg -538.95 | loss: 24.5175\n",
      "epi: 102 | score avg -529.15 | loss: 3.6341\n",
      "epi: 103 | score avg -527.96 | loss: 5.2444\n",
      "epi: 104 | score avg -527.64 | loss: 114.3095\n",
      "epi: 105 | score avg -522.60 | loss: 114.1770\n",
      "epi: 106 | score avg -513.79 | loss: 2.7708\n",
      "epi: 107 | score avg -502.58 | loss: 1.9742\n",
      "epi: 108 | score avg -506.70 | loss: 6.4578\n",
      "epi: 109 | score avg -501.82 | loss: 155.2838\n",
      "epi: 110 | score avg -502.67 | loss: 1.9557\n",
      "epi: 111 | score avg -519.17 | loss: 21.4955\n",
      "epi: 112 | score avg -529.34 | loss: 4.0572\n",
      "epi: 113 | score avg -555.88 | loss: 46.0777\n",
      "epi: 114 | score avg -554.36 | loss: 96.8596\n",
      "epi: 115 | score avg -549.13 | loss: 8.3364\n",
      "epi: 116 | score avg -604.68 | loss: 28.1757\n",
      "epi: 117 | score avg -592.32 | loss: 83.0648\n",
      "epi: 118 | score avg -584.56 | loss: 2.8037\n",
      "epi: 119 | score avg -573.69 | loss: 2.7689\n",
      "epi: 120 | score avg -589.99 | loss: 39.9779\n",
      "epi: 121 | score avg -586.85 | loss: 4.0164\n",
      "epi: 122 | score avg -572.54 | loss: 6.7937\n",
      "epi: 123 | score avg -559.45 | loss: 3.0772\n",
      "epi: 124 | score avg -543.83 | loss: 1.4956\n",
      "epi: 125 | score avg -553.75 | loss: 25.8996\n",
      "epi: 126 | score avg -554.60 | loss: 11.1473\n",
      "epi: 127 | score avg -601.06 | loss: 205.8108\n",
      "epi: 128 | score avg -587.16 | loss: 19.3251\n",
      "epi: 129 | score avg -566.83 | loss: 5.7039\n",
      "epi: 130 | score avg -602.08 | loss: 24.3695\n",
      "epi: 131 | score avg -603.06 | loss: 69.1482\n",
      "epi: 132 | score avg -584.20 | loss: 227.7689\n",
      "epi: 133 | score avg -602.60 | loss: 14.6970\n",
      "epi: 134 | score avg -607.47 | loss: 36.7810\n",
      "epi: 135 | score avg -617.33 | loss: 32.9346\n",
      "epi: 136 | score avg -590.08 | loss: 14.0105\n",
      "epi: 137 | score avg -595.63 | loss: 65.3224\n",
      "epi: 138 | score avg -567.98 | loss: 53.4328\n",
      "epi: 139 | score avg -586.78 | loss: 38.2371\n",
      "epi: 140 | score avg -613.34 | loss: 20.0202\n",
      "epi: 141 | score avg -595.31 | loss: 10.0061\n",
      "epi: 142 | score avg -599.11 | loss: 14.9290\n",
      "epi: 143 | score avg -579.60 | loss: 2.9510\n",
      "epi: 144 | score avg -613.54 | loss: 169.2395\n",
      "epi: 145 | score avg -605.78 | loss: 21.9796\n",
      "epi: 146 | score avg -600.83 | loss: 14.2563\n",
      "epi: 147 | score avg -575.38 | loss: 8.3065\n",
      "epi: 148 | score avg -568.95 | loss: 9.8971\n",
      "epi: 149 | score avg -571.32 | loss: 5.5114\n",
      "epi: 150 | score avg -562.04 | loss: 114.5676\n",
      "epi: 151 | score avg -574.07 | loss: 19.6952\n",
      "epi: 152 | score avg -565.65 | loss: 185.7810\n",
      "epi: 153 | score avg -581.49 | loss: 22.0434\n",
      "epi: 154 | score avg -564.67 | loss: 185.2584\n",
      "epi: 155 | score avg -554.65 | loss: 17.2626\n",
      "epi: 156 | score avg -548.78 | loss: 20.8547\n",
      "epi: 157 | score avg -549.31 | loss: 33.7583\n",
      "epi: 158 | score avg -556.20 | loss: 24.5120\n",
      "epi: 159 | score avg -545.06 | loss: 199.1142\n",
      "epi: 160 | score avg -534.96 | loss: 14.2391\n",
      "epi: 161 | score avg -511.96 | loss: 97.3207\n",
      "epi: 162 | score avg -516.68 | loss: 7.6595\n",
      "epi: 163 | score avg -510.89 | loss: 2.4565\n",
      "epi: 164 | score avg -562.41 | loss: 128.6426\n",
      "epi: 165 | score avg -545.44 | loss: 16.3082\n",
      "epi: 166 | score avg -544.73 | loss: 3.6244\n",
      "epi: 167 | score avg -574.25 | loss: 29.8023\n",
      "epi: 168 | score avg -604.20 | loss: 119.6224\n",
      "epi: 169 | score avg -601.92 | loss: 34.0801\n",
      "epi: 170 | score avg -596.49 | loss: 3.9181\n",
      "epi: 171 | score avg -586.73 | loss: 1.4595\n",
      "epi: 172 | score avg -580.48 | loss: 156.6715\n",
      "epi: 173 | score avg -572.75 | loss: 166.7135\n",
      "epi: 174 | score avg -561.43 | loss: 1.5892\n",
      "epi: 175 | score avg -560.51 | loss: 1.7585\n",
      "epi: 176 | score avg -559.11 | loss: 126.4996\n",
      "epi: 177 | score avg -547.20 | loss: 13.8871\n",
      "epi: 178 | score avg -537.34 | loss: 15.8453\n",
      "epi: 179 | score avg -565.44 | loss: 25.7879\n",
      "epi: 180 | score avg -541.06 | loss: 80.5131\n",
      "epi: 181 | score avg -553.98 | loss: 29.8687\n",
      "epi: 182 | score avg -546.78 | loss: 4.2679\n",
      "epi: 183 | score avg -574.92 | loss: 39.4088\n",
      "epi: 184 | score avg -592.61 | loss: 33.4606\n",
      "epi: 185 | score avg -580.25 | loss: 5.2277\n",
      "epi: 186 | score avg -556.28 | loss: 25.3482\n",
      "epi: 187 | score avg -589.73 | loss: 24.4131\n",
      "epi: 188 | score avg -608.63 | loss: 50.1587\n",
      "epi: 189 | score avg -605.51 | loss: 6.7351\n",
      "epi: 190 | score avg -638.81 | loss: 16.0517\n",
      "epi: 191 | score avg -645.77 | loss: 53.1778\n",
      "epi: 192 | score avg -632.37 | loss: 55.5191\n",
      "epi: 193 | score avg -611.86 | loss: 6.8551\n",
      "epi: 194 | score avg -597.43 | loss: 151.4807\n",
      "epi: 195 | score avg -586.12 | loss: 21.5905\n",
      "epi: 196 | score avg -565.69 | loss: 203.2071\n",
      "epi: 197 | score avg -560.57 | loss: 28.4024\n",
      "epi: 198 | score avg -518.25 | loss: 93.9150\n",
      "epi: 199 | score avg -479.46 | loss: 90.7659\n",
      "epi: 200 | score avg -445.98 | loss: 78.5499\n",
      "epi: 201 | score avg -416.09 | loss: 79.8231\n",
      "epi: 202 | score avg -390.50 | loss: 62.6967\n",
      "epi: 203 | score avg -364.15 | loss: 98.5457\n",
      "epi: 204 | score avg -343.18 | loss: 80.2902\n",
      "epi: 205 | score avg -324.93 | loss: 85.8750\n",
      "epi: 206 | score avg -305.76 | loss: 69.6840\n",
      "epi: 207 | score avg -288.67 | loss: 74.4638\n",
      "epi: 208 | score avg -273.92 | loss: 60.5871\n",
      "epi: 209 | score avg -257.23 | loss: 60.9125\n",
      "epi: 210 | score avg -245.84 | loss: 71.1386\n",
      "epi: 211 | score avg -233.13 | loss: 83.6662\n",
      "epi: 212 | score avg -207.65 | loss: 172.9325\n",
      "epi: 213 | score avg -196.06 | loss: 91.3950\n",
      "epi: 214 | score avg -188.01 | loss: 93.8988\n",
      "epi: 215 | score avg -180.03 | loss: 62.3543\n",
      "epi: 216 | score avg -169.11 | loss: 121.9401\n",
      "epi: 217 | score avg -159.61 | loss: 99.2871\n",
      "epi: 218 | score avg -155.06 | loss: 74.0137\n",
      "epi: 219 | score avg -156.89 | loss: 58.6777\n",
      "epi: 220 | score avg -156.05 | loss: 94.9017\n",
      "epi: 221 | score avg -157.71 | loss: 64.6383\n",
      "epi: 222 | score avg -178.68 | loss: 176.2663\n",
      "epi: 223 | score avg -170.49 | loss: 87.3099\n",
      "epi: 224 | score avg -165.13 | loss: 73.7210\n",
      "epi: 225 | score avg -154.74 | loss: 96.4275\n",
      "epi: 226 | score avg -150.97 | loss: 69.5654\n",
      "epi: 227 | score avg -144.23 | loss: 65.2762\n",
      "epi: 228 | score avg -143.88 | loss: 66.9099\n",
      "epi: 229 | score avg -139.48 | loss: 81.7071\n",
      "epi: 230 | score avg -133.38 | loss: 56.7607\n",
      "epi: 231 | score avg -219.25 | loss: 95.5471\n",
      "epi: 232 | score avg -242.28 | loss: 18.1512\n",
      "epi: 233 | score avg -238.61 | loss: 68.5927\n",
      "epi: 234 | score avg -219.89 | loss: 94.6494\n",
      "epi: 235 | score avg -210.51 | loss: 115.3571\n",
      "epi: 236 | score avg -226.10 | loss: 69.5791\n",
      "epi: 237 | score avg -218.66 | loss: 48.1613\n",
      "epi: 238 | score avg -205.52 | loss: 60.0386\n",
      "epi: 239 | score avg -197.83 | loss: 61.8237\n",
      "epi: 240 | score avg -208.19 | loss: 30.8404\n",
      "epi: 241 | score avg -201.76 | loss: 59.8975\n",
      "epi: 242 | score avg -191.50 | loss: 58.4211\n",
      "epi: 243 | score avg -172.66 | loss: 79.7804\n",
      "epi: 244 | score avg -196.09 | loss: 45.3592\n",
      "epi: 245 | score avg -184.67 | loss: 63.4131\n",
      "epi: 246 | score avg -181.88 | loss: 32.2214\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbba320975a9114d2433fba427f26c389728c846a7c4900c481dce2a1a9f6231"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('tf240': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}